{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603660c",
   "metadata": {},
   "source": [
    "# 微调Qwen2.5-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af37cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:11,592 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:11,860 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "2025-06-01 20:37:13,079 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:13,329 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/data2/dzr/.cache\" \n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm  # 引入 tqdm 库\n",
    "import time  # 引入 time 模块\n",
    "import argparse  # 引入 argparse 模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from typing import Dict, List\n",
    "from modelscope import AutoTokenizer, AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "model_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764cb66",
   "metadata": {},
   "source": [
    "## 定义度量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ee1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#---------新增topkloss---------\n",
    "class NMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        mse = self.mse(output, target)\n",
    "        var = torch.var(target, unbiased=False)\n",
    "        # 防止除以零的情况\n",
    "        if var.item() == 0:\n",
    "            return torch.tensor(float('inf')).to(output.device)\n",
    "        return mse / var\n",
    "\n",
    "class TopkLoss(nn.Module):\n",
    "    def __init__(self, k=1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output : [B, T, C] 模型输出的logits（未归一化）\n",
    "            target : [B, T, C] one-hot编码 或 [B, T] 类别索引\n",
    "            B = Batch Size        批量大小（数据加载时设置的batch_size）\n",
    "            T = Sequence Length   输出序列的时间步数（output_length=3）\n",
    "            C = Num Classes       类别数量（64个离散目标类别）\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        output_flat = output.view(B*T, C)  # [B*T, C]\n",
    "        target_flat = target.contiguous().view(-1)  # [B*T]\n",
    "        \n",
    "        # 计算Top-k正确性\n",
    "        _, topk_indices = torch.topk(output_flat, self.k, dim=1)  # [B*T, k]\n",
    "        correct = topk_indices.eq(target_flat.unsqueeze(1)).any(dim=1)  # [B*T]\n",
    "        \n",
    "        # 计算损失（仅惩罚Top-k错误的样本）\n",
    "        loss = F.cross_entropy(output_flat, target_flat, reduction='none')  # [B*T]，表示每个样本的预测是否在 Top-K 中命中真实标签\n",
    "        masked_loss = loss * ~correct  # 仅保留错误样本的损失值，正确样本的损失被置零\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return masked_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return masked_loss.sum()\n",
    "        return masked_loss\n",
    "\n",
    "#------------新增HybridLoss--------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, k=3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 混合权重\n",
    "        self.k = k\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        output : [B, T, C]\n",
    "        target : [B, T]\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        \n",
    "        # 常规交叉熵损失（保持生成特性）\n",
    "        ce_loss = self.ce(output.view(-1, C), target.view(-1))  # [B*T]\n",
    "        \n",
    "        # Top-K增强损失\n",
    "        _, topk = output.topk(self.k, dim=-1)  # [B, T, k]\n",
    "        correct = topk.eq(target.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "        topk_loss = (1 - correct.float()).mean()  # 错误率\n",
    "        \n",
    "        # 时间依赖惩罚项\n",
    "        seq_penalty = self._sequence_consistency(output, target)  # [1]\n",
    "        \n",
    "        return self.alpha*ce_loss.mean() + (1-self.alpha)*topk_loss + seq_penalty\n",
    "        \n",
    "    def _sequence_consistency(self, output, target):\n",
    "        \"\"\"\n",
    "        惩罚相邻时间步预测不一致的情况\n",
    "        \"\"\"\n",
    "        preds = output.argmax(-1)  # [B, T]\n",
    "        diff = (preds[:, 1:] != preds[:, :-1]).float().mean()\n",
    "        return diff * 0.2  # 可调节系数\n",
    "    \n",
    "#-------------新增CrossEntropyloss-----------------\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')  # 始终返回非归约结果\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # 处理one-hot编码目标\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "\n",
    "        # 重塑维度\n",
    "        output = output.view(-1, output.size(-1))  # [B*T, C]\n",
    "        target = target.view(-1)                   # [B*T]\n",
    "\n",
    "        # 计算基础损失\n",
    "        ce_loss = self.ce(output, target)\n",
    "        \n",
    "        # 自定义归约方式\n",
    "        if self.reduction == 'mean':\n",
    "            return ce_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return ce_loss.sum()\n",
    "        return ce_loss  # 'none'模式返回原始形状\n",
    "\n",
    "def calculate_accuracy(output, target, k=3):\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        with torch.no_grad():\n",
    "            _, pred = output.topk(k, dim=-1)  # [B, T, k]\n",
    "            correct = pred.eq(target.unsqueeze(-1)).any(dim=-1)\n",
    "            return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7152d0",
   "metadata": {},
   "source": [
    "## Processer\n",
    "### 构建多模态提示词并提取视觉输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb251bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_inputs(sample: Dict, hist_steps: int = 5) -> Dict:\n",
    "    \"\"\"构建多模态提示词并提取视觉输入\n",
    "    Args:\n",
    "        sample: 包含多模态数据的样本\n",
    "        hist_steps: 使用历史时间步数（默认为5）\n",
    "    Returns:\n",
    "        包含处理后的提示词和视觉输入的字典\n",
    "    \"\"\"\n",
    "    # 提取并规范化路径\n",
    "    def normalize_paths(path_list: List[str]) -> List[str]:\n",
    "        return [os.path.normpath(p) for p in path_list]\n",
    "    # 处理所有路径\n",
    "    video_paths = normalize_paths(sample['video_paths'][:hist_steps])\n",
    "    heatmap_paths = normalize_paths(sample['heatmap_paths'][:hist_steps])\n",
    "    gps_data = sample['gps'][:hist_steps].tolist()\n",
    "    \n",
    "    # 构建时间序列提示词\n",
    "    prompt_parts = []\n",
    "    for step in range(hist_steps):\n",
    "        time_label = f\"t-{hist_steps-1-step}\" if step < hist_steps-1 else \"Current time (t)\"\n",
    "        \n",
    "        # GPS数据格式化（假设张量存储的是经度、纬度）\n",
    "        lon, lat = gps_data[step]\n",
    "        gps_str = f\"longitude:{lon:.6f},dimension:{lat:.6f}\"\n",
    "        \n",
    "        # 添加多模态信息块\n",
    "        prompt_part = (\n",
    "            f\"time:{time_label}\"\n",
    "            f\"gps:{gps_str}\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    # 组合完整提示词\n",
    "    full_prompt = (\"\".join(prompt_parts) )\n",
    "    \n",
    "    # 提取所有视觉路径（RGB + 热力图）\n",
    "    all_image_paths = [p for pair in zip(video_paths, heatmap_paths) for p in pair]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"image_paths\": all_image_paths,\n",
    "        \"target_mmwave\": sample['target_mmwave']\n",
    "    }\n",
    "\n",
    "# 示例使用 ---------------------------------------------------\n",
    "def process_sample(sample, processor):  # 添加processor参数\n",
    "    # Step 1: 构建提示词和获取图像路径\n",
    "    processed = build_prompt_and_inputs(sample)\n",
    "    \n",
    "    # Step 2: 构建messages结构\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\", \"image\": path} for path in processed[\"image_paths\"]] + \n",
    "                  [{\"type\": \"text\", \"text\": processed[\"prompt\"]}]\n",
    "    }]\n",
    "    \n",
    "    # Step 3: 使用传入的processor处理输入\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    return inputs, processed[\"target_mmwave\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048644",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "\n",
    "class QwenVisionDataset(Dataset):\n",
    "    def __init__(self, data_csv_paths, modal='mmwave_gps', input_length=8, output_length=3):\n",
    "        self.data_csv_paths = data_csv_paths\n",
    "        self.modal = modal\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        # 特征列映射\n",
    "        self.features_column = {\n",
    "            # 'rgbs': 'unit1_rgb',\n",
    "            'rgbs': 'unit1_camera_resized',\n",
    "            'u1_loc': 'unit1_loc',\n",
    "            'u2_loc': 'unit2_loc',\n",
    "            'mmwave': 'unit1_pwr_60ghz',\n",
    "            'heatmap': 'unit1_mmwave_heatmap'  # 新增热力图列\n",
    "        }\n",
    "        \n",
    "        # 初始化滑动窗口\n",
    "        self.window_samples = []\n",
    "        for seq_idx, data_csv_path in enumerate(self.data_csv_paths):\n",
    "            data_csv = pd.read_csv(data_csv_path)\n",
    "            for seq_id in data_csv['seq_index'].unique():\n",
    "                seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "                if len(seq_data) >= self.input_length:\n",
    "                    for start_idx in range(len(seq_data) - self.input_length + 1):\n",
    "                        self.window_samples.append((seq_idx, seq_id, start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, seq_id, start_idx = self.window_samples[idx]\n",
    "        base_path = os.path.dirname(self.data_csv_paths[seq_idx])\n",
    "        data_csv = pd.read_csv(self.data_csv_paths[seq_idx])\n",
    "        seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "        \n",
    "        # 获取原始路径数据\n",
    "        window_data = {\n",
    "            'video_paths': \n",
    "            seq_data[self.features_column['rgbs']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist(),\n",
    "            'heatmap_paths': \n",
    "            seq_data[self.features_column['heatmap']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist()\n",
    "        }\n",
    "\n",
    "        # 处理GPS数据\n",
    "        gps = []\n",
    "        for i in range(self.input_length):\n",
    "            u1_loc = os.path.join(base_path, seq_data[self.features_column['u1_loc']].iloc[start_idx+i])\n",
    "            u2_loc = os.path.join(base_path, seq_data[self.features_column['u2_loc']].iloc[start_idx+i])\n",
    "            \n",
    "            with open(u1_loc, 'r') as f:\n",
    "                lat1, lon1 = map(float, f.read().strip().split())\n",
    "            with open(u2_loc, 'r') as f:\n",
    "                lat2, lon2 = map(float, f.read().strip().split())\n",
    "                \n",
    "            gps.append(torch.tensor([lat2-lat1, lon2-lon1], dtype=torch.float32))\n",
    "        gps = torch.stack(gps)\n",
    "\n",
    "        # 处理mmWave数据\n",
    "        mmwave = []\n",
    "        for i in range(self.input_length):\n",
    "            mmwave_path = os.path.join(base_path, \n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                mmwave.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())), \n",
    "                    dtype=torch.float32))\n",
    "        mmwave = torch.stack(mmwave)\n",
    "\n",
    "        # 目标数据（最后output_length个时间步）\n",
    "        target = []\n",
    "        for i in range(self.input_length-self.output_length, self.input_length):\n",
    "            mmwave_path = os.path.join(base_path,\n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                target.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())),\n",
    "                    dtype=torch.float32))\n",
    "        target = torch.stack(target)\n",
    "\n",
    "        return {\n",
    "            'video_paths': [os.path.join(base_path, p) for p in window_data['video_paths']],\n",
    "            'heatmap_paths': [os.path.join(base_path, p) for p in window_data['heatmap_paths']],\n",
    "            'gps': gps,\n",
    "            'mmwave': mmwave,\n",
    "            'target_mmwave': target\n",
    "        }\n",
    "\n",
    "def qwen_collate_fn(batch):\n",
    "    collated = {\n",
    "        'video_paths': [item['video_paths'] for item in batch],\n",
    "        'heatmap_paths': [item['heatmap_paths'] for item in batch],\n",
    "        'gps': pad_sequence([item['gps'] for item in batch], batch_first=True),\n",
    "        'mmwave': pad_sequence([item['mmwave'] for item in batch], batch_first=True),\n",
    "        'target_mmwave': pad_sequence([item['target_mmwave'] for item in batch], batch_first=True)\n",
    "    }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d3200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_start_idx = 1\n",
    "dataset_end_idx = 9\n",
    "# 定义数据集路径\n",
    "dataset_path = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx, dataset_end_idx)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths = []\n",
    "for path in dataset_path:\n",
    "    data_csv_paths.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f7b41",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a605cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_554_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_555_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_556_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_557_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_558_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_559_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_560_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_561_00_42_26.jpg'],\n",
       " 'heatmap_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_98.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_99.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_100.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_101.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_102.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_103.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_104.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_105.png'],\n",
       " 'gps': tensor([[-3.8232e-05,  1.6488e-04],\n",
       "         [-4.2452e-05,  1.6538e-04],\n",
       "         [-4.6712e-05,  1.6588e-04],\n",
       "         [-5.1002e-05,  1.6648e-04],\n",
       "         [-5.5332e-05,  1.6698e-04],\n",
       "         [-5.9692e-05,  1.6758e-04],\n",
       "         [-6.4092e-05,  1.6808e-04],\n",
       "         [-6.8522e-05,  1.6868e-04]]),\n",
       " 'mmwave': tensor([[0.0161, 0.0191, 0.0196, 0.0225, 0.0221, 0.0225, 0.0188, 0.0182, 0.0185,\n",
       "          0.0197, 0.0216, 0.0221, 0.0227, 0.0217, 0.0207, 0.0240, 0.0220, 0.0195,\n",
       "          0.0190, 0.0193, 0.0198, 0.0215, 0.0220, 0.0213, 0.0218, 0.0293, 0.0311,\n",
       "          0.0283, 0.0238, 0.0248, 0.0221, 0.0265, 0.0279, 0.0298, 0.0317, 0.0337,\n",
       "          0.0375, 0.0431, 0.0388, 0.0267, 0.0263, 0.0483, 0.0858, 0.1219, 0.1708,\n",
       "          0.2159, 0.2279, 0.2392, 0.1746, 0.1013, 0.0824, 0.0617, 0.0341, 0.0275,\n",
       "          0.0269, 0.0302, 0.0418, 0.0415, 0.0604, 0.0828, 0.0699, 0.0492, 0.0235,\n",
       "          0.0221],\n",
       "         [0.0178, 0.0202, 0.0215, 0.0235, 0.0249, 0.0224, 0.0205, 0.0193, 0.0206,\n",
       "          0.0241, 0.0286, 0.0293, 0.0318, 0.0312, 0.0259, 0.0271, 0.0247, 0.0209,\n",
       "          0.0228, 0.0241, 0.0205, 0.0267, 0.0300, 0.0342, 0.0388, 0.0403, 0.0507,\n",
       "          0.0457, 0.0458, 0.0430, 0.0283, 0.0243, 0.0374, 0.0442, 0.0500, 0.0466,\n",
       "          0.0478, 0.0639, 0.0742, 0.0804, 0.0604, 0.0438, 0.0665, 0.1002, 0.1647,\n",
       "          0.2334, 0.2686, 0.2683, 0.2188, 0.1595, 0.1582, 0.1008, 0.0617, 0.0426,\n",
       "          0.0191, 0.0230, 0.0248, 0.0326, 0.0370, 0.0639, 0.0563, 0.0563, 0.0357,\n",
       "          0.0207],\n",
       "         [0.0273, 0.0287, 0.0209, 0.0228, 0.0253, 0.0249, 0.0261, 0.0273, 0.0238,\n",
       "          0.0211, 0.0256, 0.0324, 0.0432, 0.0382, 0.0269, 0.0259, 0.0222, 0.0218,\n",
       "          0.0301, 0.0275, 0.0190, 0.0246, 0.0283, 0.0345, 0.0387, 0.0342, 0.0269,\n",
       "          0.0289, 0.0390, 0.0464, 0.0297, 0.0213, 0.0210, 0.0247, 0.0298, 0.0312,\n",
       "          0.0340, 0.0380, 0.0481, 0.0557, 0.0582, 0.0420, 0.0263, 0.0242, 0.0366,\n",
       "          0.0830, 0.1062, 0.1214, 0.1225, 0.1121, 0.1136, 0.0988, 0.0688, 0.0505,\n",
       "          0.0272, 0.0216, 0.0219, 0.0229, 0.0247, 0.0273, 0.0313, 0.0296, 0.0251,\n",
       "          0.0181],\n",
       "         [0.0293, 0.0353, 0.0232, 0.0207, 0.0217, 0.0212, 0.0227, 0.0285, 0.0286,\n",
       "          0.0243, 0.0208, 0.0278, 0.0366, 0.0419, 0.0264, 0.0202, 0.0191, 0.0215,\n",
       "          0.0308, 0.0305, 0.0232, 0.0216, 0.0222, 0.0267, 0.0348, 0.0267, 0.0215,\n",
       "          0.0209, 0.0300, 0.0374, 0.0296, 0.0215, 0.0222, 0.0205, 0.0200, 0.0208,\n",
       "          0.0267, 0.0254, 0.0296, 0.0379, 0.0413, 0.0351, 0.0252, 0.0222, 0.0299,\n",
       "          0.0551, 0.0823, 0.1074, 0.1214, 0.1201, 0.1316, 0.1171, 0.0901, 0.0709,\n",
       "          0.0412, 0.0257, 0.0268, 0.0272, 0.0264, 0.0283, 0.0341, 0.0411, 0.0351,\n",
       "          0.0202],\n",
       "         [0.0250, 0.0314, 0.0248, 0.0234, 0.0201, 0.0187, 0.0196, 0.0252, 0.0240,\n",
       "          0.0252, 0.0226, 0.0207, 0.0276, 0.0305, 0.0274, 0.0214, 0.0207, 0.0208,\n",
       "          0.0234, 0.0207, 0.0228, 0.0245, 0.0202, 0.0270, 0.0263, 0.0276, 0.0224,\n",
       "          0.0246, 0.0292, 0.0343, 0.0364, 0.0244, 0.0281, 0.0216, 0.0217, 0.0269,\n",
       "          0.0322, 0.0330, 0.0283, 0.0312, 0.0456, 0.0497, 0.0419, 0.0361, 0.0284,\n",
       "          0.0318, 0.0568, 0.0885, 0.1042, 0.1229, 0.1430, 0.1293, 0.1158, 0.0900,\n",
       "          0.0638, 0.0391, 0.0279, 0.0242, 0.0218, 0.0211, 0.0254, 0.0279, 0.0306,\n",
       "          0.0196],\n",
       "         [0.0222, 0.0310, 0.0250, 0.0282, 0.0243, 0.0196, 0.0190, 0.0188, 0.0234,\n",
       "          0.0290, 0.0267, 0.0222, 0.0210, 0.0242, 0.0240, 0.0279, 0.0235, 0.0262,\n",
       "          0.0223, 0.0199, 0.0234, 0.0290, 0.0245, 0.0217, 0.0215, 0.0211, 0.0264,\n",
       "          0.0302, 0.0279, 0.0268, 0.0254, 0.0322, 0.0253, 0.0248, 0.0214, 0.0231,\n",
       "          0.0303, 0.0297, 0.0283, 0.0258, 0.0350, 0.0438, 0.0449, 0.0412, 0.0296,\n",
       "          0.0312, 0.0425, 0.0691, 0.1145, 0.1419, 0.1691, 0.1500, 0.1492, 0.1613,\n",
       "          0.1102, 0.0651, 0.0348, 0.0307, 0.0242, 0.0199, 0.0206, 0.0238, 0.0226,\n",
       "          0.0181],\n",
       "         [0.0227, 0.0251, 0.0253, 0.0282, 0.0259, 0.0234, 0.0207, 0.0199, 0.0210,\n",
       "          0.0249, 0.0255, 0.0225, 0.0203, 0.0215, 0.0228, 0.0242, 0.0310, 0.0269,\n",
       "          0.0215, 0.0189, 0.0224, 0.0265, 0.0225, 0.0227, 0.0194, 0.0222, 0.0258,\n",
       "          0.0310, 0.0257, 0.0265, 0.0272, 0.0331, 0.0281, 0.0280, 0.0241, 0.0246,\n",
       "          0.0287, 0.0363, 0.0292, 0.0266, 0.0312, 0.0415, 0.0511, 0.0518, 0.0355,\n",
       "          0.0282, 0.0307, 0.0593, 0.0986, 0.1469, 0.1782, 0.1723, 0.1655, 0.1622,\n",
       "          0.1471, 0.1089, 0.0653, 0.0426, 0.0329, 0.0240, 0.0204, 0.0202, 0.0211,\n",
       "          0.0196],\n",
       "         [0.0187, 0.0204, 0.0205, 0.0255, 0.0272, 0.0285, 0.0240, 0.0190, 0.0200,\n",
       "          0.0222, 0.0260, 0.0215, 0.0198, 0.0200, 0.0199, 0.0254, 0.0276, 0.0268,\n",
       "          0.0219, 0.0211, 0.0220, 0.0236, 0.0282, 0.0221, 0.0208, 0.0210, 0.0254,\n",
       "          0.0299, 0.0243, 0.0236, 0.0245, 0.0265, 0.0287, 0.0263, 0.0303, 0.0234,\n",
       "          0.0246, 0.0282, 0.0274, 0.0251, 0.0247, 0.0310, 0.0387, 0.0493, 0.0398,\n",
       "          0.0246, 0.0235, 0.0234, 0.0363, 0.0777, 0.0893, 0.1020, 0.1355, 0.1665,\n",
       "          0.1373, 0.0894, 0.0768, 0.0682, 0.0563, 0.0311, 0.0235, 0.0213, 0.0210,\n",
       "          0.0194]]),\n",
       " 'target_mmwave': tensor([[0.0222, 0.0310, 0.0250, 0.0282, 0.0243, 0.0196, 0.0190, 0.0188, 0.0234,\n",
       "          0.0290, 0.0267, 0.0222, 0.0210, 0.0242, 0.0240, 0.0279, 0.0235, 0.0262,\n",
       "          0.0223, 0.0199, 0.0234, 0.0290, 0.0245, 0.0217, 0.0215, 0.0211, 0.0264,\n",
       "          0.0302, 0.0279, 0.0268, 0.0254, 0.0322, 0.0253, 0.0248, 0.0214, 0.0231,\n",
       "          0.0303, 0.0297, 0.0283, 0.0258, 0.0350, 0.0438, 0.0449, 0.0412, 0.0296,\n",
       "          0.0312, 0.0425, 0.0691, 0.1145, 0.1419, 0.1691, 0.1500, 0.1492, 0.1613,\n",
       "          0.1102, 0.0651, 0.0348, 0.0307, 0.0242, 0.0199, 0.0206, 0.0238, 0.0226,\n",
       "          0.0181],\n",
       "         [0.0227, 0.0251, 0.0253, 0.0282, 0.0259, 0.0234, 0.0207, 0.0199, 0.0210,\n",
       "          0.0249, 0.0255, 0.0225, 0.0203, 0.0215, 0.0228, 0.0242, 0.0310, 0.0269,\n",
       "          0.0215, 0.0189, 0.0224, 0.0265, 0.0225, 0.0227, 0.0194, 0.0222, 0.0258,\n",
       "          0.0310, 0.0257, 0.0265, 0.0272, 0.0331, 0.0281, 0.0280, 0.0241, 0.0246,\n",
       "          0.0287, 0.0363, 0.0292, 0.0266, 0.0312, 0.0415, 0.0511, 0.0518, 0.0355,\n",
       "          0.0282, 0.0307, 0.0593, 0.0986, 0.1469, 0.1782, 0.1723, 0.1655, 0.1622,\n",
       "          0.1471, 0.1089, 0.0653, 0.0426, 0.0329, 0.0240, 0.0204, 0.0202, 0.0211,\n",
       "          0.0196],\n",
       "         [0.0187, 0.0204, 0.0205, 0.0255, 0.0272, 0.0285, 0.0240, 0.0190, 0.0200,\n",
       "          0.0222, 0.0260, 0.0215, 0.0198, 0.0200, 0.0199, 0.0254, 0.0276, 0.0268,\n",
       "          0.0219, 0.0211, 0.0220, 0.0236, 0.0282, 0.0221, 0.0208, 0.0210, 0.0254,\n",
       "          0.0299, 0.0243, 0.0236, 0.0245, 0.0265, 0.0287, 0.0263, 0.0303, 0.0234,\n",
       "          0.0246, 0.0282, 0.0274, 0.0251, 0.0247, 0.0310, 0.0387, 0.0493, 0.0398,\n",
       "          0.0246, 0.0235, 0.0234, 0.0363, 0.0777, 0.0893, 0.1020, 0.1355, 0.1665,\n",
       "          0.1373, 0.0894, 0.0768, 0.0682, 0.0563, 0.0311, 0.0235, 0.0213, 0.0210,\n",
       "          0.0194]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = QwenVisionDataset(\n",
    "    data_csv_paths,\n",
    "    input_length=8,\n",
    "    output_length=3\n",
    ")\n",
    "dataset[98]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4e860",
   "metadata": {},
   "source": [
    "### 划分数据集（抽出1600个样本微调）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# 固定随机种子确保每次结果一致（可选）\n",
    "random.seed(42)\n",
    "\n",
    "# 原始数据集有约 14400 个样本\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# 随机选出 12000 个样本的索引\n",
    "subset_indices = random.sample(range(total_samples), 12000)\n",
    "\n",
    "# 创建新的 dataset\n",
    "small_dataset = Subset(dataset, subset_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6813b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples: 9600\n",
      "Total Validation samples: 1200\n",
      "Total Testing samples: 1200\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(small_dataset))\n",
    "val_size = int(0.1 * len(small_dataset))\n",
    "test_size = len(small_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(small_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Total Training samples: {len(train_dataset)}\")\n",
    "print(f\"Total Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Total Testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    # 直接返回样本列表，不进行合并\n",
    "    return batch\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a0a4",
   "metadata": {},
   "source": [
    "## Model\n",
    "### 用Qwen构造带有64类分类头的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bc543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "class QwenTimeLLMDirectHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        qwen_model: nn.Module,\n",
    "        pred_len: int = 3,        # 未来要预测的时刻数 P（这里 3）\n",
    "        num_beams: int = 64,      # 每个时刻要分类的波束数 C（这里 64）\n",
    "        hidden_dim: int = 3584,   # Qwen 最后一层隐藏维度 D（Qwen2.5-VL 为 3584）\n",
    "        proj_hidden: int = 2048,  # MLP 第一层隐藏维度\n",
    "        dropout: float = 0.1,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.05\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - qwen_model: 预加载好的 Qwen2.5-VLForConditionalGeneration（多模态）  \n",
    "        - pred_len:   要预测的未来时刻数 P=3  \n",
    "        - num_beams:  每个时刻输出的波束索引类别数 C=64  \n",
    "        - hidden_dim: Qwen 骨干输出维度 D=3584  \n",
    "        - proj_hidden: MLP 第一层隐藏维度  \n",
    "        - dropout:    MLP 中用到的 Dropout 比例  \n",
    "        - lora_*:     LoRA 超参数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # ————————————————————————————————————————\n",
    "        # 1) 将 Qwen 骨干切换到 bfloat16，然后用 LoRA 微调\n",
    "        # ————————————————————————————————————————\n",
    "        # 原模型默认是 float32，我们这里转成 bf16\n",
    "        self.qwen = qwen_model.to(torch.bfloat16)\n",
    "\n",
    "        # LoRA 配置：只对 Qwen 内部的投影／线性层插入 LoRA adapter\n",
    "        # target_modules 里的名字要与你加载的 Qwen2.5-VLForConditionalGeneration 内部层名称一致。\n",
    "        # 下面示例里放了一些常见的线性层名称，比如 \"qkv_proj\", \"out_proj\", \"fc1\", \"fc2\" 等。\n",
    "        # 请根据实际用到的 Qwen 版本检查一下内部模块名字是否需要改动。\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,  # 训练模式：只训练 LoRA adapter，冻结原始权重\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\n",
    "                # 视觉分支\n",
    "                \"qkv\",         # Qwen2_5_VLVisionSdpaAttention.qkv\n",
    "                \"proj\",        # Qwen2_5_VLVisionSdpaAttention.proj\n",
    "                # 文本分支注意力\n",
    "                \"q_proj\",      # Qwen2_5_VLSdpaAttention.q_proj\n",
    "                \"k_proj\",      # Qwen2_5_VLSdpaAttention.k_proj\n",
    "                \"v_proj\",      # Qwen2_5_VLSdpaAttention.v_proj\n",
    "                \"o_proj\",      # Qwen2_5_VLSdpaAttention.o_proj\n",
    "                # 文本分支 MLP\n",
    "                \"gate_proj\",   # Qwen2_5_VLMLP.gate_proj\n",
    "                \"up_proj\",     # Qwen2_5_VLMLP.up_proj\n",
    "                \"down_proj\"    # Qwen2_5_VLMLP.down_proj\n",
    "            ]\n",
    "        )\n",
    "        # 用 PEFT 把 LoRA 加到基础模型上\n",
    "        self.qwen = get_peft_model(self.qwen, lora_config)\n",
    "\n",
    "        # 保证只有 LoRA adapter 参数可训练，其余骨干权重被冻结\n",
    "        # get_peft_model 默认会将原始权重 requires_grad=False\n",
    "\n",
    "        # 最后启用 gradient checkpointing 省显存\n",
    "        self.qwen.gradient_checkpointing_enable()\n",
    "\n",
    "        # ————————————————————————————————————————\n",
    "        # 2) 定义输出头：把 Qwen 最后一层“文本隐藏态”里最后一个 token 的向量当作上下文特征\n",
    "        #    然后用一个两层 MLP 预测 pred_len * num_beams 个 logits\n",
    "        # ————————————————————————————————————————\n",
    "        self.pred_len = pred_len\n",
    "        self.num_beams = num_beams\n",
    "        self.hidden_dim = hidden_dim  # 3584\n",
    "\n",
    "        # MLP：D -> proj_hidden -> ReLU -> Dropout -> (proj_hidden -> P*C)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, proj_hidden),   # 3584 -> 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(proj_hidden, self.pred_len * self.num_beams)  # 2048 -> 3*64=192\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,         # (B, L_text)\n",
    "        attention_mask: torch.LongTensor,    # (B, L_text)\n",
    "        pixel_values: torch.FloatTensor,     # (B, 3, H, W) or 按 Qwen 要求的多模态格式\n",
    "        image_grid_thw: torch.LongTensor      # (B, N_patches, 3)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - input_ids:      文本 token 序列 (B, L_text)，它已经把 5 个时间步的 GPS 串在一起共 482 个 token\n",
    "          - attention_mask: 对应的 mask (B, L_text)，这里应全为 1\n",
    "          - pixel_values:   10 张图合并后的一批图像 (B, 3, H, W)，供视觉编码器使用\n",
    "          - image_grid_thw: (B, N_patches, 3)：告诉视觉编码器如何切 patch，例如 shape=(1, 10, 18) 等\n",
    "        Returns:\n",
    "          - logits: (B, P, C) → (1, 3, 64)，预测未来 3 步每步 64 类的 logits\n",
    "        \"\"\"\n",
    "        B = input_ids.size(0)\n",
    "        device = input_ids.device\n",
    "\n",
    "        # —————————————————————————————————————————————\n",
    "        # A. 将文本和图像输入到 Qwen，多模态骨干输出\n",
    "        #    我们让 Qwen 以 bfloat16 运行，输入前先把 input_ids 转 long、转到 cuda/bf16\n",
    "        # —————————————————————————————————————————————\n",
    "        # 1) 文本 Embedding + Transformer：得到最后一层文本隐藏态 (B, L_text, D)\n",
    "        #    这里直接用 input_ids/attention_mask，Qwen 会内部调用 get_input_embeddings()\n",
    "        outputs = self.qwen(\n",
    "            input_ids=input_ids.to(torch.long),\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_grid_thw=image_grid_thw,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # hidden_states[-1] 的形状 = (B, L_text, D)\n",
    "        last_hidden_text = outputs.hidden_states[-1]  # bf16, (B, L_text, 3584)\n",
    "\n",
    "        # —————————————————————————————————————————————\n",
    "        # B. 我们只取“文本隐藏态序列”里最后一个 token 对应的向量，作为整条5步+10图的上下文特征\n",
    "        #    这个最后 token 通常对应拼接后 GPS 序列的末尾。\n",
    "        #    你也可以换成 mean pooling 或者 first token（[CLS]）向量，视任务需求而定。\n",
    "        # —————————————————————————————————————————————\n",
    "        # last_hidden_text[:, -1, :] 的形状 = (B, 3584)\n",
    "        context_vec = last_hidden_text[:, -1, :].to(torch.float32)  # 转回 float32 做后续 MLP 更稳定\n",
    "\n",
    "        # —————————————————————————————————————————————\n",
    "        # C. 分类头：context_vec -> MLP -> (B, P * C) -> reshape -> (B, P, C)\n",
    "        # —————————————————————————————————————————————\n",
    "        x = self.dropout(context_vec)                           # (B, 3584)\n",
    "        logits_flat = self.classifier(x)                         # (B, P * C) = (B, 3*64=192)\n",
    "        logits = logits_flat.view(B, self.pred_len, self.num_beams)  # (B, 3, 64)\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b7894",
   "metadata": {},
   "source": [
    "### 加载Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de76fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen-vl-utils[decord]==0.0.8\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30343410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:17,093 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 20:37:17,342 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ae2c891f87497593a09f735411e085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 15.49 GB\n"
     ]
    }
   ],
   "source": [
    "# 配置 bfloat16 精度\n",
    "finetune_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,    # 设置模型权重为 bfloat16\n",
    "    device_map=\"cuda\",              # 自动分配设备\n",
    "    trust_remote_code=True,         # 必须开启\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cd9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2_5_VLForConditionalGeneration(\n",
      "  (model): Qwen2_5_VLModel(\n",
      "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "      )\n",
      "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen2_5_VLMLP(\n",
      "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen2_5_VLPatchMerger(\n",
      "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen2_5_VLTextModel(\n",
      "      (embed_tokens): Embedding(152064, 3584)\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "          (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e69a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 15.62 GB\n",
      "QwenTimeLLMDirectHead(\n",
      "  (qwen): PeftModelForCausalLM(\n",
      "    (base_model): LoraModel(\n",
      "      (model): Qwen2_5_VLForConditionalGeneration(\n",
      "        (model): Qwen2_5_VLModel(\n",
      "          (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "            (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "              (proj): lora.Conv3d(\n",
      "                (base_layer): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Conv3d(3, 8, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Conv3d(8, 1280, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "            (blocks): ModuleList(\n",
      "              (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "                (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "                (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "                (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "                  (qkv): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3840, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (mlp): Qwen2_5_VLMLP(\n",
      "                  (gate_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3420, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (up_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3420, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (down_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3420, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (merger): Qwen2_5_VLPatchMerger(\n",
      "              (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "              (mlp): Sequential(\n",
      "                (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "                (1): GELU(approximate='none')\n",
      "                (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (language_model): Qwen2_5_VLTextModel(\n",
      "            (embed_tokens): Embedding(152064, 3584)\n",
      "            (layers): ModuleList(\n",
      "              (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "                (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "                  (q_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (v_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): Qwen2MLP(\n",
      "                  (gate_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=18944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (up_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=18944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (down_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=18944, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "                (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              )\n",
      "            )\n",
      "            (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "          )\n",
      "        )\n",
      "        (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "finetuner = QwenTimeLLMDirectHead(\n",
    "    qwen_model=finetune_model,  # 你加载好的 Qwen2.5-VLForConditionalGeneration\n",
    "    pred_len=3,\n",
    "    num_beams=64,\n",
    "    hidden_dim=3584,\n",
    "    proj_hidden=2048,\n",
    "    dropout=0.1,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ").to(device)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")\n",
    "print(finetuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5a305",
   "metadata": {},
   "source": [
    "### 训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f6fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 33515904\n"
     ]
    }
   ],
   "source": [
    "# 忽略可能的警告\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "total_trainable = sum(p.numel() for p in finetuner.parameters() if p.requires_grad)\n",
    "print(\"Trainable params:\", total_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f8aeb",
   "metadata": {},
   "source": [
    "### 检查输入样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fc16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'time:t-4gps:longitude:0.000087,dimension:0.000158time:t-3gps:longitude:0.000084,dimension:0.000159time:t-2gps:longitude:0.000081,dimension:0.000159time:t-1gps:longitude:0.000078,dimension:0.000159time:Current time (t)gps:longitude:0.000075,dimension:0.000159', 'image_paths': ['/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_976_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_123.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_977_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_124.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_978_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_125.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_979_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_126.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_980_02_12_22.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_127.png'], 'target_mmwave': tensor([[0.0778, 0.1865, 0.1825, 0.1027, 0.0815, 0.0884, 0.1438, 0.2466, 0.3730,\n",
      "         0.4503, 0.5026, 0.5143, 0.4780, 0.4624, 0.3400, 0.2270, 0.1590, 0.0933,\n",
      "         0.0871, 0.0929, 0.1002, 0.0913, 0.1182, 0.1346, 0.1863, 0.1726, 0.1324,\n",
      "         0.0892, 0.0785, 0.0791, 0.0952, 0.0975, 0.0934, 0.1030, 0.1114, 0.0994,\n",
      "         0.0887, 0.0862, 0.0865, 0.0838, 0.0737, 0.0849, 0.0736, 0.0811, 0.0834,\n",
      "         0.0932, 0.0893, 0.0802, 0.0787, 0.0791, 0.0946, 0.1072, 0.1015, 0.0791,\n",
      "         0.0751, 0.0793, 0.0795, 0.0822, 0.0977, 0.0914, 0.0857, 0.0874, 0.0756,\n",
      "         0.0614],\n",
      "        [0.1002, 0.1837, 0.2138, 0.1128, 0.0987, 0.0933, 0.0952, 0.1936, 0.2908,\n",
      "         0.4070, 0.4543, 0.4580, 0.4948, 0.4694, 0.3498, 0.2551, 0.2063, 0.1130,\n",
      "         0.0881, 0.0809, 0.0876, 0.0780, 0.1017, 0.1337, 0.1671, 0.1830, 0.1427,\n",
      "         0.1074, 0.0838, 0.0787, 0.0839, 0.0986, 0.0971, 0.1164, 0.1056, 0.1063,\n",
      "         0.0890, 0.0791, 0.0801, 0.0885, 0.0769, 0.0822, 0.0709, 0.0798, 0.0789,\n",
      "         0.0904, 0.1004, 0.0845, 0.0882, 0.0758, 0.0913, 0.0941, 0.1040, 0.0861,\n",
      "         0.0782, 0.0748, 0.0761, 0.0727, 0.0978, 0.0940, 0.0966, 0.0926, 0.0765,\n",
      "         0.0607],\n",
      "        [0.0667, 0.1895, 0.2176, 0.1707, 0.1318, 0.1359, 0.0890, 0.1056, 0.1989,\n",
      "         0.3248, 0.3945, 0.4246, 0.4703, 0.4939, 0.3897, 0.3163, 0.2616, 0.1739,\n",
      "         0.1020, 0.0747, 0.0825, 0.0759, 0.0901, 0.1179, 0.1620, 0.1808, 0.1672,\n",
      "         0.1444, 0.1161, 0.0904, 0.0875, 0.0955, 0.1032, 0.1408, 0.1110, 0.1191,\n",
      "         0.0936, 0.0787, 0.0805, 0.0821, 0.0740, 0.0809, 0.0697, 0.0764, 0.0765,\n",
      "         0.0853, 0.0898, 0.1068, 0.0938, 0.0799, 0.0835, 0.0888, 0.0945, 0.1001,\n",
      "         0.0973, 0.0784, 0.0703, 0.0721, 0.0846, 0.0923, 0.1015, 0.0991, 0.0856,\n",
      "         0.0633]])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151653,   1678,  60777,     12,     19,  74511,     25,\n",
      "          25446,     25,     15,     13,     15,     15,     15,     15,     23,\n",
      "             22,  11991,  18161,     25,     15,     13,     15,     15,     15,\n",
      "             16,     20,     23,   1678,  60777,     12,     18,  74511,     25,\n",
      "          25446,     25,     15,     13,     15,     15,     15,     15,     23,\n",
      "             19,  11991,  18161,     25,     15,     13,     15,     15,     15,\n",
      "             16,     20,     24,   1678,  60777,     12,     17,  74511,     25,\n",
      "          25446,     25,     15,     13,     15,     15,     15,     15,     23,\n",
      "             16,  11991,  18161,     25,     15,     13,     15,     15,     15,\n",
      "             16,     20,     24,   1678,  60777,     12,     16,  74511,     25,\n",
      "          25446,     25,     15,     13,     15,     15,     15,     15,     22,\n",
      "             23,  11991,  18161,     25,     15,     13,     15,     15,     15,\n",
      "             16,     20,     24,   1678,     25,   5405,    882,    320,     83,\n",
      "              8,  74511,     25,  25446,     25,     15,     13,     15,     15,\n",
      "             15,     15,     22,     20,  11991,  18161,     25,     15,     13,\n",
      "             15,     15,     15,     16,     20,     24, 151645,    198, 151644,\n",
      "          77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'pixel_values': tensor([[-1.3543, -1.3397, -1.2959,  ..., -0.5844, -0.9114, -1.3807],\n",
      "        [-1.3251, -1.4565, -1.4127,  ...,  0.0413, -1.1958, -1.4376],\n",
      "        [-1.3689, -0.7558,  0.3099,  ..., -1.4802, -1.4802, -1.4802],\n",
      "        ...,\n",
      "        [-0.7558, -0.7558, -0.7558,  ..., -0.0724, -0.0724, -0.0724],\n",
      "        [-0.7558, -0.7558, -0.7558,  ..., -0.0582, -0.0582, -0.0724],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 10, 12],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 12],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 12],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 12],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 12],\n",
      "        [ 1,  8,  8]], device='cuda:0')}, tensor([[0.0778, 0.1865, 0.1825, 0.1027, 0.0815, 0.0884, 0.1438, 0.2466, 0.3730,\n",
      "         0.4503, 0.5026, 0.5143, 0.4780, 0.4624, 0.3400, 0.2270, 0.1590, 0.0933,\n",
      "         0.0871, 0.0929, 0.1002, 0.0913, 0.1182, 0.1346, 0.1863, 0.1726, 0.1324,\n",
      "         0.0892, 0.0785, 0.0791, 0.0952, 0.0975, 0.0934, 0.1030, 0.1114, 0.0994,\n",
      "         0.0887, 0.0862, 0.0865, 0.0838, 0.0737, 0.0849, 0.0736, 0.0811, 0.0834,\n",
      "         0.0932, 0.0893, 0.0802, 0.0787, 0.0791, 0.0946, 0.1072, 0.1015, 0.0791,\n",
      "         0.0751, 0.0793, 0.0795, 0.0822, 0.0977, 0.0914, 0.0857, 0.0874, 0.0756,\n",
      "         0.0614],\n",
      "        [0.1002, 0.1837, 0.2138, 0.1128, 0.0987, 0.0933, 0.0952, 0.1936, 0.2908,\n",
      "         0.4070, 0.4543, 0.4580, 0.4948, 0.4694, 0.3498, 0.2551, 0.2063, 0.1130,\n",
      "         0.0881, 0.0809, 0.0876, 0.0780, 0.1017, 0.1337, 0.1671, 0.1830, 0.1427,\n",
      "         0.1074, 0.0838, 0.0787, 0.0839, 0.0986, 0.0971, 0.1164, 0.1056, 0.1063,\n",
      "         0.0890, 0.0791, 0.0801, 0.0885, 0.0769, 0.0822, 0.0709, 0.0798, 0.0789,\n",
      "         0.0904, 0.1004, 0.0845, 0.0882, 0.0758, 0.0913, 0.0941, 0.1040, 0.0861,\n",
      "         0.0782, 0.0748, 0.0761, 0.0727, 0.0978, 0.0940, 0.0966, 0.0926, 0.0765,\n",
      "         0.0607],\n",
      "        [0.0667, 0.1895, 0.2176, 0.1707, 0.1318, 0.1359, 0.0890, 0.1056, 0.1989,\n",
      "         0.3248, 0.3945, 0.4246, 0.4703, 0.4939, 0.3897, 0.3163, 0.2616, 0.1739,\n",
      "         0.1020, 0.0747, 0.0825, 0.0759, 0.0901, 0.1179, 0.1620, 0.1808, 0.1672,\n",
      "         0.1444, 0.1161, 0.0904, 0.0875, 0.0955, 0.1032, 0.1408, 0.1110, 0.1191,\n",
      "         0.0936, 0.0787, 0.0805, 0.0821, 0.0740, 0.0809, 0.0697, 0.0764, 0.0765,\n",
      "         0.0853, 0.0898, 0.1068, 0.0938, 0.0799, 0.0835, 0.0888, 0.0945, 0.1001,\n",
      "         0.0973, 0.0784, 0.0703, 0.0721, 0.0846, 0.0923, 0.1015, 0.0991, 0.0856,\n",
      "         0.0633]]))\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2324]\n",
    "print(build_prompt_and_inputs(sample))\n",
    "print(process_sample(sample,processor=processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e7e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本 token 数: 407\n",
      "图像张数: 10\n",
      "  第 1 张图 → 120 个 patch token\n",
      "  第 2 张图 → 64 个 patch token\n",
      "  第 3 张图 → 120 个 patch token\n",
      "  第 4 张图 → 64 个 patch token\n",
      "  第 5 张图 → 120 个 patch token\n",
      "  第 6 张图 → 64 个 patch token\n",
      "  第 7 张图 → 120 个 patch token\n",
      "  第 8 张图 → 64 个 patch token\n",
      "  第 9 张图 → 120 个 patch token\n",
      "  第 10 张图 → 64 个 patch token\n",
      "图像总 patch token 数: 920\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经有 process_sample 函数和 processor\n",
    "sample = train_dataset[0]     # 或者任何一个样本\n",
    "\n",
    "# 1) 只做一次前处理（在 CPU 上）\n",
    "inputs, _ = process_sample(sample, processor)\n",
    "\n",
    "# 2) 文本 token 数\n",
    "#    inputs[\"input_ids\"] 的形状是 (1, seq_len)\n",
    "text_token_count = inputs[\"input_ids\"].shape[1]\n",
    "print(f\"文本 token 数: {text_token_count}\")\n",
    "\n",
    "# 3) 图像 token 数\n",
    "#    inputs[\"image_grid_thw\"] 的形状是 (n_images, 3)\n",
    "#      每行 = [T, H, W]，对于静态图片 T=1，token = H*W\n",
    "grid = inputs[\"image_grid_thw\"].cpu().long()  # (n_images, 3)\n",
    "T, H, W = grid.unbind(dim=1)                 # 拆成三个向量\n",
    "tokens_per_image = (H * W).tolist()           # list 长度 = n_images\n",
    "total_image_tokens = sum(tokens_per_image)\n",
    "\n",
    "print(f\"图像张数: {grid.shape[0]}\")\n",
    "for i, nt in enumerate(tokens_per_image):\n",
    "    print(f\"  第 {i+1} 张图 → {nt} 个 patch token\")\n",
    "print(f\"图像总 patch token 数: {total_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, processor, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct_1 = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "        batch_labels = []\n",
    "\n",
    "        for sample in batch:\n",
    "            inputs, target = process_sample(sample, processor)  # 变量名改为target\n",
    "            batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "            batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "            batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "            batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "            batch_labels.append(target)  # 接收target_mmwave数据\n",
    "\n",
    "        # 修改维度处理\n",
    "        batch_inputs = {\n",
    "            k: torch.cat(v, dim=0).to(device)\n",
    "            for k, v in batch_inputs.items()\n",
    "        }\n",
    "        # 改为stack处理三维目标数据 (batch_size, seq_len, num_classes)\n",
    "        batch_labels = torch.stack(batch_labels).to(device)  # [B, T, C]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(**batch_inputs)  # [B, T, C]\n",
    "        # 展平时间步维度计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), \n",
    "                        batch_labels.view(-1, batch_labels.size(-1)))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        total_correct_1 += calculate_accuracy(logits, batch_labels, k=1)\n",
    "    train_acc1 = total_correct_1 / len(train_loader)\n",
    "    return total_loss / len(train_loader) , train_acc1\n",
    "\n",
    "def evaluate(model, processor, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = defaultdict(int)  # 存储不同k值的正确数\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"evaluating\"):\n",
    "            batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "            batch_labels = []\n",
    "\n",
    "            for sample in batch:\n",
    "                inputs, target = process_sample(sample, processor)  # 变量名改为target\n",
    "                batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "                batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "                batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "                batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "                batch_labels.append(target)\n",
    "\n",
    "            batch_inputs = {k: torch.cat(v, dim=0).to(device) for k, v in batch_inputs.items()}\n",
    "            batch_labels = torch.stack(batch_labels).to(device)  # [B, T, C]\n",
    "\n",
    "            with autocast():\n",
    "                logits = model(**batch_inputs)  # [B, T, C]\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)),\n",
    "                                batch_labels.view(-1, batch_labels.size(-1)))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "            # 修改准确率计算逻辑\n",
    "            for k in [1, 3, 5]:\n",
    "                _, preds = logits.topk(k, dim=-1)  # [B, T, k]\n",
    "                # 将target转换为类别索引（假设target是one-hot编码）\n",
    "                targets = torch.argmax(batch_labels, dim=-1)  # [B, T]\n",
    "                correct = preds.eq(targets.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "                total_correct[k] += correct.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracies = {k: total_correct[k]/(total_samples * batch_labels.size(1)) for k in [1,3,5]}\n",
    "    \n",
    "    return avg_loss, accuracies[1], accuracies[3], accuracies[5]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e7f54",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaeb6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "patience  = 5\n",
    "checkpoint_dir = \"/data2/dzr/finetune/finetunning_1_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d85a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76182/3751787654.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "model = finetuner.to(device)\n",
    "# 模型里用了 model.qwen.gradient_checkpointing_enable()\n",
    "scaler    = GradScaler()\n",
    "criterion = HybridLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9cb3c",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217077e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test ,train_test_acc = train_epoch(model,processor,train_loader,criterion,optimizer,scaler,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3147f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_inline import backend_inline\n",
    "from IPython import display\n",
    "# 定义 use_svg_display 函数\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "# 定义 set_axes 函数\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置 Matplotlib 的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    " \n",
    "\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 3.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()   \n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "\n",
    "    def show(self):\n",
    "        display.display(self.fig)# 输出图像\n",
    "        display.clear_output(wait=True)# 不输出新图像，而是覆盖之前的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0ee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    }
   ],
   "source": [
    "# 初始化画图\n",
    "animator_loss = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0, 10],\n",
    "                            legend=['train_loss','val_loss'])\n",
    "animator_acc = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0, 1],\n",
    "                            legend=['train_acc1', 'val_acc1'])\n",
    "\n",
    "def format_time(seconds):\n",
    "    mins, sec = divmod(seconds, 60)\n",
    "    hrs, mins = divmod(mins, 60)\n",
    "    return f\"{int(hrs)}h {int(mins)}m {int(sec)}s\"\n",
    "\n",
    "num_epochs = epochs\n",
    "best_val_loss = float('inf') # 初始化为“正无限大”（infinity）\n",
    "\n",
    "# 确保保存模型的目录存在\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 记录训练开始时间\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.2e}\")\n",
    "    # 训练\n",
    "    train_loss, train_acc1 = train_epoch(model,processor,train_loader,criterion,optimizer,scaler,device)\n",
    "\n",
    "    # 验证\n",
    "    val_loss ,acc_1 ,acc_3 ,acc_5 = evaluate(model,processor,val_loader,criterion,device)\n",
    "    # 绘图\n",
    "    animator_loss.add(epoch + 1, [\n",
    "    train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss,\n",
    "    val_loss.item() if isinstance(val_loss, torch.Tensor) else val_loss\n",
    "    ])\n",
    "    animator_acc.add(epoch + 1, [\n",
    "    train_acc1.item() if isinstance(train_acc1, torch.Tensor) else train_acc1,\n",
    "    acc_1.item() if isinstance(acc_1, torch.Tensor) else acc_1\n",
    "    ])\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # 计算剩余时间\n",
    "    elapsed_time = epoch_end_time - training_start_time\n",
    "    avg_epoch_time = elapsed_time / (epoch + 1)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * remaining_epochs\n",
    "\n",
    "    # 转换为更易读的格式\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f},Train Accuracy@1:{train_acc1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f},Val Accuracy@1: {acc_1:.4f},Val Accuracy@3: {acc_3:.4f},Val Accuracy@5: {acc_5:.4f}\")\n",
    "    print(f\"Epoch Duration: {format_time(epoch_duration)}, Estimated Remaining Time: {format_time(remaining_time)}\")\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1} to {best_model_path}\")\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "    else:\n",
    "        early_stop_counter += 1  # 增加计数器\n",
    "\n",
    "    # 如果验证损失连续多个 epoch 没有改善，则停止训练\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break  # 提前停止训练\n",
    "\n",
    "    # 每隔若干个 epoch 保存模型\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'multimodal_encoder_decoder_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model at epoch {epoch+1} to {checkpoint_path}\")\n",
    "animator_loss.show()\n",
    "animator_acc.show()\n",
    "# 7. 测试评估\n",
    "\n",
    "# 加载最佳模型\n",
    "best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Loaded best model for testing.\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}. Skipping test evaluation.\")\n",
    "\n",
    "# 定义测试评估函数（可以与验证相同）\n",
    "\n",
    "\n",
    "test_loss ,test_acc1 ,test_acc3 ,test_acc5 = evaluate(model,processor,test_loader,criterion,device)\n",
    "print(f\"Test Loss : {test_loss:.4f};Test Accuracy@3 : {test_acc3:.4f}\")\n",
    "print(f\"Test Accuracy@1 : {test_acc1:.4f};Test Accuracy@5 : {test_acc5:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
