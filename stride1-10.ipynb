{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603660c",
   "metadata": {},
   "source": [
    "# 微调Qwen2.5-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af37cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:13:47,793 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:13:48,030 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "2025-06-03 23:13:49,309 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:13:49,683 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/data2/dzr/.cache\" \n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm  # 引入 tqdm 库\n",
    "import time  # 引入 time 模块\n",
    "import argparse  # 引入 argparse 模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from typing import Dict, List\n",
    "from modelscope import AutoTokenizer, AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "model_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "from functools import partial\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8eadf",
   "metadata": {},
   "source": [
    "## 指定设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af77c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen-vl-utils[decord]==0.0.8\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d506bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output_length = 3\n",
    "checkpoint_dir = \"/data2/dzr/finetune/train_outputprojection_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7152d0",
   "metadata": {},
   "source": [
    "## Processer\n",
    "### 构建多模态提示词并提取视觉输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb251bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_inputs(sample: Dict, hist_steps: int = 5) -> Dict:\n",
    "    \"\"\"构建多模态提示词并提取视觉输入\n",
    "    Args:\n",
    "        sample: 包含多模态数据的样本\n",
    "        hist_steps: 使用历史时间步数（默认为5）\n",
    "    Returns:\n",
    "        包含处理后的提示词和视觉输入的字典\n",
    "    \"\"\"\n",
    "    # 提取并规范化路径\n",
    "    def normalize_paths(path_list: List[str]) -> List[str]:\n",
    "        return [os.path.normpath(p) for p in path_list]\n",
    "    # 处理所有路径\n",
    "    video_paths = normalize_paths(sample['video_paths'][:hist_steps])\n",
    "    heatmap_paths = normalize_paths(sample['heatmap_paths'][:hist_steps])\n",
    "    gps_data = sample['gps'][:hist_steps].tolist()\n",
    "    \n",
    "    # 构建时间序列提示词\n",
    "    prompt_parts = []\n",
    "    for step in range(hist_steps):\n",
    "        time_label = f\"t-{hist_steps-1-step}\" if step < hist_steps-1 else \"Current time (t)\"\n",
    "        \n",
    "        # GPS数据格式化（假设张量存储的是经度、纬度）\n",
    "        lon, lat = gps_data[step]\n",
    "        gps_str = f\"longitude:{lon:.6f},dimension:{lat:.6f}\"\n",
    "        \n",
    "        # 添加多模态信息块\n",
    "        prompt_part = (\n",
    "            f\"time:{time_label}\"\n",
    "            f\"gps:{gps_str}\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    # 组合完整提示词\n",
    "    full_prompt = (\"\".join(prompt_parts))\n",
    "\n",
    "    # 提取所有视觉路径（RGB + 热力图）\n",
    "    all_image_paths = [p for pair in zip(video_paths, heatmap_paths) for p in pair]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"image_paths\": all_image_paths,\n",
    "        \"labels\": sample['target_mmwave'].argmax(dim=-1).tolist()  # 假设索引是最大值位置\n",
    "    }\n",
    "\n",
    "# 示例使用 ---------------------------------------------------\n",
    "def process_sample(sample, processor):  # 添加processor参数\n",
    "    # Step 1: 构建提示词和获取图像路径\n",
    "    processed = build_prompt_and_inputs(sample)\n",
    "    \n",
    "    # Step 2: 构建messages结构\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\", \"image\": path} for path in processed[\"image_paths\"]] + \n",
    "                  [{\"type\": \"text\", \"text\": processed[\"prompt\"]}]\n",
    "    }]\n",
    "    \n",
    "    # Step 3: 使用传入的processor处理输入\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return inputs, processed[\"labels\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048644",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ffa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedVisionDataset(Dataset):\n",
    "    def __init__(self, original_dataset, processor):\n",
    "        self.cache = []\n",
    "        self.original_dataset = original_dataset  # 保存原始数据集引用\n",
    "        \n",
    "        # 预加载所有样本\n",
    "        for i in tqdm(range(len(original_dataset)), desc=\"Caching dataset\"):\n",
    "            sample = original_dataset[i]\n",
    "            try:\n",
    "                inputs, labels = process_sample(sample, processor)\n",
    "                # 将处理后的数据转移到CPU（避免占用GPU内存）\n",
    "                inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "                self.cache.append((inputs, labels))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                # 可以选择跳过错误样本或添加占位符\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小 - 这是必须实现的方法\"\"\"\n",
    "        return len(self.original_dataset)  # 或者 len(self.cache)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"直接返回缓存的处理结果\"\"\"\n",
    "        return self.cache[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "\n",
    "class QwenVisionDataset(Dataset):\n",
    "    def __init__(self, data_csv_paths, modal='mmwave_gps', input_length=8, output_length=3):\n",
    "        self.data_csv_paths = data_csv_paths\n",
    "        self.modal = modal\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        # 特征列映射\n",
    "        self.features_column = {\n",
    "            # 'rgbs': 'unit1_rgb',\n",
    "            'rgbs': 'unit1_camera_resized',\n",
    "            'u1_loc': 'unit1_loc',\n",
    "            'u2_loc': 'unit2_loc',\n",
    "            'mmwave': 'unit1_pwr_60ghz',\n",
    "            'heatmap': 'unit1_mmwave_heatmap'  # 新增热力图列\n",
    "        }\n",
    "        \n",
    "        # 初始化滑动窗口\n",
    "        self.window_samples = []\n",
    "        for seq_idx, data_csv_path in enumerate(self.data_csv_paths):\n",
    "            data_csv = pd.read_csv(data_csv_path)\n",
    "            for seq_id in data_csv['seq_index'].unique():\n",
    "                seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "                if len(seq_data) >= self.input_length:\n",
    "                    for start_idx in range(len(seq_data) - self.input_length + 1):\n",
    "                        self.window_samples.append((seq_idx, seq_id, start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, seq_id, start_idx = self.window_samples[idx]\n",
    "        base_path = os.path.dirname(self.data_csv_paths[seq_idx])\n",
    "        data_csv = pd.read_csv(self.data_csv_paths[seq_idx])\n",
    "        seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "        \n",
    "        # 获取原始路径数据\n",
    "        window_data = {\n",
    "            'video_paths': \n",
    "            seq_data[self.features_column['rgbs']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist(),\n",
    "            'heatmap_paths': \n",
    "            seq_data[self.features_column['heatmap']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist()\n",
    "        }\n",
    "\n",
    "        # 处理GPS数据\n",
    "        gps = []\n",
    "        for i in range(self.input_length):\n",
    "            u1_loc = os.path.join(base_path, seq_data[self.features_column['u1_loc']].iloc[start_idx+i])\n",
    "            u2_loc = os.path.join(base_path, seq_data[self.features_column['u2_loc']].iloc[start_idx+i])\n",
    "            \n",
    "            with open(u1_loc, 'r') as f:\n",
    "                lat1, lon1 = map(float, f.read().strip().split())\n",
    "            with open(u2_loc, 'r') as f:\n",
    "                lat2, lon2 = map(float, f.read().strip().split())\n",
    "                \n",
    "            gps.append(torch.tensor([lat2-lat1, lon2-lon1], dtype=torch.float32))\n",
    "        gps = torch.stack(gps)\n",
    "\n",
    "        # 处理mmWave数据\n",
    "        mmwave = []\n",
    "        for i in range(self.input_length):\n",
    "            mmwave_path = os.path.join(base_path, \n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                mmwave.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())), \n",
    "                    dtype=torch.float32))\n",
    "        mmwave = torch.stack(mmwave)\n",
    "\n",
    "        # 目标数据（最后output_length个时间步）\n",
    "        target = []\n",
    "        for i in range(self.input_length-self.output_length, self.input_length):\n",
    "            mmwave_path = os.path.join(base_path,\n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                target.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())),\n",
    "                    dtype=torch.float32))\n",
    "        target = torch.stack(target)\n",
    "\n",
    "        return {\n",
    "            'video_paths': [os.path.join(base_path, p) for p in window_data['video_paths']],\n",
    "            'heatmap_paths': [os.path.join(base_path, p) for p in window_data['heatmap_paths']],\n",
    "            'gps': gps,\n",
    "            'mmwave': mmwave,\n",
    "            'target_mmwave': target\n",
    "        }\n",
    "\n",
    "def qwen_collate_fn(batch):\n",
    "    collated = {\n",
    "        'video_paths': [item['video_paths'] for item in batch],\n",
    "        'heatmap_paths': [item['heatmap_paths'] for item in batch],\n",
    "        'gps': pad_sequence([item['gps'] for item in batch], batch_first=True),\n",
    "        'mmwave': pad_sequence([item['mmwave'] for item in batch], batch_first=True),\n",
    "        'target_mmwave': pad_sequence([item['target_mmwave'] for item in batch], batch_first=True)\n",
    "    }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808148aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, device):\n",
    "    \"\"\"处理缓存数据的批处理\"\"\"\n",
    "    batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "    batch_labels = []\n",
    "    \n",
    "    for (inputs, labels) in batch:\n",
    "        batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "        batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "        batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "        batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "        batch_labels.append(labels)\n",
    "    \n",
    "    # 拼接张量（保持在CPU）\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": torch.cat(batch_inputs[\"input_ids\"], dim=0),\n",
    "        \"attention_mask\": torch.cat(batch_inputs[\"attention_mask\"], dim=0),\n",
    "        \"pixel_values\": torch.cat(batch_inputs[\"pixel_values\"], dim=0),\n",
    "        \"image_grid_thw\": torch.cat(batch_inputs[\"image_grid_thw\"], dim=0)\n",
    "    }\n",
    "    batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "    \n",
    "    return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a0a4",
   "metadata": {},
   "source": [
    "## Model\n",
    "### 用Qwen构造带有输出投影模块的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bc543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenReprogPatchHeadLight(nn.Module):\n",
    "    def __init__(self,\n",
    "                 qwen_model: nn.Module,\n",
    "                 pred_len: int = 3,       # 未来预测步数 P\n",
    "                 num_beams: int = 64,     # 类别数 C\n",
    "                 hidden_dim: int = 3584,  # Qwen 隐藏维度 D\n",
    "                 mha_heads: int = 8,      # Multi-Head Attention 的头数\n",
    "                 proj_hidden: int = 2048, # 投影层中间隐藏维度\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.qwen = qwen_model\n",
    "        self.P = pred_len\n",
    "        self.C = num_beams\n",
    "        self.D = hidden_dim\n",
    "\n",
    "        # 冻结主干\n",
    "        for p in self.qwen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # 可训练 patch\n",
    "        self.patch_init = nn.Parameter(torch.randn(self.P, self.D))\n",
    "\n",
    "        # Patch reprogramming\n",
    "        self.reprog_mha = nn.MultiheadAttention(\n",
    "            embed_dim=self.D,\n",
    "            num_heads=mha_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 更轻量的投影头：10752 -> 2048 -> 192\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.P * self.D, proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(proj_hidden, self.P * self.C),\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, image_grid_thw):\n",
    "        B = input_ids.size(0)\n",
    "\n",
    "        # 冻结主干前向\n",
    "        with torch.no_grad():\n",
    "            outputs = self.qwen(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                image_grid_thw=image_grid_thw,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        history_hidden = outputs.hidden_states[-1]  # [B, L, D]\n",
    "\n",
    "        # patch initialization\n",
    "        patch = self.patch_init.unsqueeze(0).expand(B, -1, -1)  # [B, P, D]\n",
    "\n",
    "        # reprogram\n",
    "        reprog_patch, _ = self.reprog_mha(\n",
    "            query=patch,\n",
    "            key=history_hidden,\n",
    "            value=history_hidden,\n",
    "        )  # [B, P, D]\n",
    "\n",
    "        # flatten + light projection\n",
    "        flat = reprog_patch.contiguous().view(B, self.P * self.D)  # [B, 3*3584]\n",
    "        logits = self.classifier(flat).view(B, self.P, self.C)     # [B, 3, 64]\n",
    "\n",
    "        return logits\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b7894",
   "metadata": {},
   "source": [
    "### 加载Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30343410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:13:52,510 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:13:52,775 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e6ef6a83b546288077f1542d7c8bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 配置 bfloat16 精度\n",
    "qwenbf16_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,    # 设置模型权重为 bfloat16\n",
    "    trust_remote_code=True,         # 必须开启\n",
    "    return_dict=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5841febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QwenReprogPatchHeadLight(\n",
    "    qwen_model=qwenbf16_model,\n",
    "    pred_len=output_length,     # 预测未来 3 步\n",
    "    num_beams=64,       # 类别数 64\n",
    "    hidden_dim=3584,    # Qwen2.5-VL 的隐藏维度\n",
    "    mha_heads=8,\n",
    "    proj_hidden=2048,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651b3e4",
   "metadata": {},
   "source": [
    "## 计算预测步长和时间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60bd339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_start_idx_zero = 9\n",
    "dataset_end_idx_zero = 10\n",
    "# 定义数据集路径\n",
    "dataset_path_zero = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx_zero, dataset_end_idx_zero)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths_zero = []\n",
    "for path in dataset_path_zero:\n",
    "    data_csv_paths_zero.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths_zero)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5740bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_dataset = QwenVisionDataset(\n",
    "    data_csv_paths_zero,\n",
    "    input_length=8,\n",
    "    output_length=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "239a9d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching dataset: 100%|██████████| 5012/5012 [05:28<00:00, 15.26it/s]\n"
     ]
    }
   ],
   "source": [
    "cached_zeroshot = CachedVisionDataset(zeroshot_dataset, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c3245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择100个样本的索引\n",
    "total_size = len(cached_zeroshot)\n",
    "random.seed(42)  # 设置随机种子确保可复现\n",
    "indices = list(range(total_size))\n",
    "selected_indices = random.sample(indices, 640)  \n",
    "\n",
    "# 创建子集\n",
    "subset = Subset(cached_zeroshot, selected_indices)\n",
    "\n",
    "zeroshot_dataloader = DataLoader(\n",
    "    subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(collate_fn, device=\"cpu\"),  # 绑定设备参数\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e936be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_forward_full_dataset(fn_forward, model, data_loader, device,\n",
    "                                num_warmup: int = 1, num_runs: int = 5):\n",
    "    \"\"\"\n",
    "    对“遍历整个 data_loader，对每个 batch 都 run 一次 model(inputs, tgt_seq)”进行\n",
    "    预热 + N 次正式测时。返回 mean_ms, std_ms，单位是毫秒。\n",
    "\n",
    "    - fn_forward:       你包装好的 forward(model, data_loader, device) 函数\n",
    "    - model:            已加载到 device，并且 eval() 的 nn.Module\n",
    "    - data_loader:      DataLoader，batch_size 已设置好\n",
    "    - device:           torch.device(\"cuda\") 或 \"cpu\"\n",
    "    - num_warmup:       预热遍历数据集的次数，默认 1 次\n",
    "    - num_runs:         正式测时次数，默认 5 次（因为跑完整个数据集通常比较慢）\n",
    "\n",
    "    返回：\n",
    "    (mean_latency_ms, std_latency_ms)\n",
    "    \"\"\"\n",
    "    # —— 在这里打印当前 model.output_length —— #\n",
    "    print(f\"当前 qwen.pred_len = {model.pred_len}\")\n",
    "\n",
    "    # —— Warm-up —— #\n",
    "    for _ in range(num_warmup):\n",
    "        fn_forward(model, data_loader, device)\n",
    "\n",
    "    # —— 正式测时 —— #\n",
    "    latencies = []\n",
    "    for _ in range(num_runs):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "\n",
    "        fn_forward(model, data_loader, device)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        latencies.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(latencies, dtype=np.float32)\n",
    "    # 这里 len(data_loader) 就是“DataLoader 里 batch 的数量”\n",
    "    num_batches = len(data_loader)\n",
    "    mean_full_dataset = float(arr.mean())    # 整个数据集一次过的均耗时\n",
    "    std_full_dataset  = float(arr.std())\n",
    "\n",
    "    # 换算成“单个 batch 的平均耗时”：\n",
    "    mean_per_batch = mean_full_dataset / num_batches\n",
    "    std_per_batch  = std_full_dataset  / num_batches\n",
    "\n",
    "    return mean_per_batch, std_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8777eb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zeroshot_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb6547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "def forward(model, data_loader,  device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs,label in tqdm(data_loader, desc=\"forward\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                _ = model(**inputs)\n",
    "    return \n",
    "          \n",
    "results = {\n",
    "    \"Qwen2.5-VL-7B\": {\"mean\": [], \"std\": []},  \n",
    "}\n",
    "\n",
    "\n",
    "num_warmup_runs = 1   # 预热遍历数据集的次数\n",
    "num_timed_runs   = 5  # 正式测时遍历数据集的次数（可以适当增大，但会更耗时）\n",
    "\n",
    "# 确保保存模型的目录存在\n",
    "os.makedirs(checkpoint_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0e6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for testing.\n"
     ]
    }
   ],
   "source": [
    "# 加载最佳模型\n",
    "best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Loaded best model for testing.\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}. Skipping test evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b519480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 qwen.pred_len = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_492851/4187172484.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "forward: 100%|██████████| 10/10 [08:24<00:00, 50.40s/it]\n",
      "forward: 100%|██████████| 10/10 [10:48<00:00, 64.86s/it]\n",
      "forward: 100%|██████████| 10/10 [10:45<00:00, 64.58s/it]\n",
      "forward: 100%|██████████| 10/10 [10:45<00:00, 64.59s/it]\n",
      "forward: 100%|██████████| 10/10 [10:44<00:00, 64.49s/it]\n",
      "forward: 100%|██████████| 10/10 [10:43<00:00, 64.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 | Qwen Model: 65406.0±167.2 ms \n",
      "当前 qwen.pred_len = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:42<00:00, 64.29s/it]\n",
      "forward: 100%|██████████| 10/10 [10:43<00:00, 64.32s/it]\n",
      "forward: 100%|██████████| 10/10 [10:43<00:00, 64.35s/it]\n",
      "forward: 100%|██████████| 10/10 [10:41<00:00, 64.15s/it]\n",
      "forward: 100%|██████████| 10/10 [10:40<00:00, 64.09s/it]\n",
      "forward: 100%|██████████| 10/10 [10:38<00:00, 63.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 2 | Qwen Model: 64987.2±198.7 ms \n",
      "当前 qwen.pred_len = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:27<00:00, 62.79s/it]\n",
      "forward: 100%|██████████| 10/10 [10:25<00:00, 62.55s/it]\n",
      "forward: 100%|██████████| 10/10 [10:21<00:00, 62.16s/it]\n",
      "forward: 100%|██████████| 10/10 [10:21<00:00, 62.16s/it]\n",
      "forward: 100%|██████████| 10/10 [10:21<00:00, 62.13s/it]\n",
      "forward: 100%|██████████| 10/10 [10:20<00:00, 62.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 3 | Qwen Model: 62999.8±202.4 ms \n",
      "当前 qwen.pred_len = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.88s/it]\n",
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.81s/it]\n",
      "forward: 100%|██████████| 10/10 [10:16<00:00, 61.69s/it]\n",
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.89s/it]\n",
      "forward: 100%|██████████| 10/10 [10:16<00:00, 61.64s/it]\n",
      "forward: 100%|██████████| 10/10 [10:16<00:00, 61.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 4 | Qwen Model: 62503.1±111.1 ms \n",
      "当前 qwen.pred_len = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:21<00:00, 62.12s/it]\n",
      "forward: 100%|██████████| 10/10 [10:19<00:00, 61.93s/it]\n",
      "forward: 100%|██████████| 10/10 [10:17<00:00, 61.75s/it]\n",
      "forward: 100%|██████████| 10/10 [10:16<00:00, 61.64s/it]\n",
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.86s/it]\n",
      "forward: 100%|██████████| 10/10 [10:15<00:00, 61.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 5 | Qwen Model: 62546.9±161.5 ms \n",
      "当前 qwen.pred_len = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:13<00:00, 61.30s/it]\n",
      "forward: 100%|██████████| 10/10 [10:12<00:00, 61.26s/it]\n",
      "forward: 100%|██████████| 10/10 [10:11<00:00, 61.13s/it]\n",
      "forward: 100%|██████████| 10/10 [10:12<00:00, 61.26s/it]\n",
      "forward: 100%|██████████| 10/10 [10:13<00:00, 61.30s/it]\n",
      "forward: 100%|██████████| 10/10 [10:15<00:00, 61.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 6 | Qwen Model: 62106.2±144.3 ms \n",
      "当前 qwen.pred_len = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:19<00:00, 61.93s/it]\n",
      "forward: 100%|██████████| 10/10 [10:21<00:00, 62.19s/it]\n",
      "forward: 100%|██████████| 10/10 [10:20<00:00, 62.05s/it]\n",
      "forward: 100%|██████████| 10/10 [10:24<00:00, 62.44s/it]\n",
      "forward: 100%|██████████| 10/10 [10:20<00:00, 62.03s/it]\n",
      "forward: 100%|██████████| 10/10 [10:25<00:00, 62.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 7 | Qwen Model: 63054.2±209.8 ms \n",
      "当前 qwen.pred_len = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:25<00:00, 62.57s/it]\n",
      "forward: 100%|██████████| 10/10 [10:26<00:00, 62.61s/it]\n",
      "forward: 100%|██████████| 10/10 [10:24<00:00, 62.46s/it]\n",
      "forward: 100%|██████████| 10/10 [10:25<00:00, 62.51s/it]\n",
      "forward: 100%|██████████| 10/10 [10:22<00:00, 62.23s/it]\n",
      "forward: 100%|██████████| 10/10 [10:26<00:00, 62.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 8 | Qwen Model: 63259.0±144.7 ms \n",
      "当前 qwen.pred_len = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:22<00:00, 62.20s/it]\n",
      "forward: 100%|██████████| 10/10 [10:19<00:00, 61.92s/it]\n",
      "forward: 100%|██████████| 10/10 [10:19<00:00, 62.00s/it]\n",
      "forward: 100%|██████████| 10/10 [10:16<00:00, 61.64s/it]\n",
      "forward: 100%|██████████| 10/10 [10:17<00:00, 61.75s/it]\n",
      "forward: 100%|██████████| 10/10 [10:39<00:00, 63.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 9 | Qwen Model: 63095.3±866.3 ms \n",
      "当前 qwen.pred_len = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.88s/it]\n",
      "forward: 100%|██████████| 10/10 [10:12<00:00, 61.26s/it]\n",
      "forward: 100%|██████████| 10/10 [10:19<00:00, 61.99s/it]\n",
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.85s/it]\n",
      "forward: 100%|██████████| 10/10 [10:18<00:00, 61.80s/it]\n",
      "forward: 100%|██████████| 10/10 [10:23<00:00, 62.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10 | Qwen Model: 62607.1±323.9 ms \n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 11):\n",
    "    # 1) 动态修改全局 output_length\n",
    "    output_length = k\n",
    "\n",
    "    # 2) 把 k 传给两个模型，让它们内部知道“预测 k 步”\n",
    "    #    - Model A 例子：QwenReprogPatchHeadLight 在 __init__ 里存了 output_length，\n",
    "    #      并且 forward 里会读取它；所以这里改 output_length，。\n",
    "    model.pred_len = k\n",
    "    \n",
    "    # —— 测 Model A 遍历整个数据集所用时间 —— #\n",
    "    mean_qwen, std_qwen = measure_forward_full_dataset(\n",
    "        fn_forward   = forward,\n",
    "        model        = model,\n",
    "        data_loader  = zeroshot_dataloader,\n",
    "        device       = device,\n",
    "        num_warmup   = num_warmup_runs,\n",
    "        num_runs     = num_timed_runs\n",
    "    )\n",
    "    results[\"Qwen2.5-VL-7B\"][\"mean\"].append(mean_qwen)\n",
    "    results[\"Qwen2.5-VL-7B\"][\"std\"].append(std_qwen)\n",
    "    print(\n",
    "        f\"k={k:2d} | \"\n",
    "        f\"Qwen Model: {mean_qwen:.1f}±{std_qwen:.1f} ms \"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341f73c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有 k=1..10 的遍历数据集推理时延已保存到 inference_latency_full_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"inference_latency_full_dataset.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"k\", \"qwen_mean_ms\", \"qwen_std_ms\"])\n",
    "    for idx, k in enumerate(range(1, 11)):\n",
    "        writer.writerow([\n",
    "            k,\n",
    "            results[\"Qwen2.5-VL-7B\"][\"mean\"][idx],\n",
    "            results[\"Qwen2.5-VL-7B\"][\"std\"][idx]\n",
    "        ])\n",
    "\n",
    "print(\"所有 k=1..10 的遍历数据集推理时延已保存到 inference_latency_full_dataset.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
