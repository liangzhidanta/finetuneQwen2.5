{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603660c",
   "metadata": {},
   "source": [
    "# 微调Qwen2.5-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af37cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:28:17,256 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:28:17,503 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "2025-05-30 15:28:18,715 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:28:18,953 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/data2/dzr/.cache\" \n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm  # 引入 tqdm 库\n",
    "import time  # 引入 time 模块\n",
    "import argparse  # 引入 argparse 模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from typing import Dict, List\n",
    "from modelscope import AutoTokenizer, AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "model_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8eadf",
   "metadata": {},
   "source": [
    "## 指定设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af77c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen-vl-utils[decord]==0.0.8\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764cb66",
   "metadata": {},
   "source": [
    "## 定义度量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ee1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#---------新增topkloss---------\n",
    "class TopkLoss(nn.Module):\n",
    "    def __init__(self, k=1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output : [B, T, C] 模型输出的logits（未归一化）\n",
    "            target : [B, T, C] one-hot编码 或 [B, T] 类别索引\n",
    "            B = Batch Size        批量大小（数据加载时设置的batch_size）\n",
    "            T = Sequence Length   输出序列的时间步数（output_length=3）\n",
    "            C = Num Classes       类别数量（64个离散目标类别）\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        output_flat = output.view(B*T, C)  # [B*T, C]\n",
    "        target_flat = target.contiguous().view(-1)  # [B*T]\n",
    "        \n",
    "        # 计算Top-k正确性\n",
    "        _, topk_indices = torch.topk(output_flat, self.k, dim=1)  # [B*T, k]\n",
    "        correct = topk_indices.eq(target_flat.unsqueeze(1)).any(dim=1)  # [B*T]\n",
    "        \n",
    "        # 计算损失（仅惩罚Top-k错误的样本）\n",
    "        loss = F.cross_entropy(output_flat, target_flat, reduction='none')  # [B*T]，表示每个样本的预测是否在 Top-K 中命中真实标签\n",
    "        masked_loss = loss * ~correct  # 仅保留错误样本的损失值，正确样本的损失被置零\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return masked_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return masked_loss.sum()\n",
    "        return masked_loss\n",
    "\n",
    "#------------新增HybridLoss--------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, k=3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 混合权重\n",
    "        self.k = k\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        output : [B, T, C]\n",
    "        target : [B, T]\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        \n",
    "        # 常规交叉熵损失（保持生成特性）\n",
    "        ce_loss = self.ce(output.view(-1, C), target.view(-1))  # [B*T]\n",
    "        \n",
    "        # Top-K增强损失\n",
    "        _, topk = output.topk(self.k, dim=-1)  # [B, T, k]\n",
    "        correct = topk.eq(target.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "        topk_loss = (1 - correct.float()).mean()  # 错误率\n",
    "        \n",
    "        # 时间依赖惩罚项\n",
    "        seq_penalty = self._sequence_consistency(output, target)  # [1]\n",
    "        \n",
    "        return self.alpha*ce_loss.mean() + (1-self.alpha)*topk_loss + seq_penalty\n",
    "        \n",
    "    def _sequence_consistency(self, output, target):\n",
    "        \"\"\"\n",
    "        惩罚相邻时间步预测不一致的情况\n",
    "        \"\"\"\n",
    "        preds = output.argmax(-1)  # [B, T]\n",
    "        diff = (preds[:, 1:] != preds[:, :-1]).float().mean()\n",
    "        return diff * 0.2  # 可调节系数\n",
    "    \n",
    "#-------------新增CrossEntropyloss-----------------\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')  # 始终返回非归约结果\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # 处理one-hot编码目标\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "\n",
    "        # 重塑维度\n",
    "        output = output.view(-1, output.size(-1))  # [B*T, C]\n",
    "        target = target.view(-1)                   # [B*T]\n",
    "\n",
    "        # 计算基础损失\n",
    "        ce_loss = self.ce(output, target)\n",
    "        \n",
    "        # 自定义归约方式\n",
    "        if self.reduction == 'mean':\n",
    "            return ce_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return ce_loss.sum()\n",
    "        return ce_loss  # 'none'模式返回原始形状\n",
    "\n",
    "def calculate_accuracy(output, target, k=3):\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        with torch.no_grad():\n",
    "            _, pred = output.topk(k, dim=-1)  # [B, T, k]\n",
    "            correct = pred.eq(target.unsqueeze(-1)).any(dim=-1)\n",
    "            return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7152d0",
   "metadata": {},
   "source": [
    "## Processer\n",
    "### 构建多模态提示词并提取视觉输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb251bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_inputs(sample: Dict, hist_steps: int = 5) -> Dict:\n",
    "    \"\"\"构建多模态提示词并提取视觉输入\n",
    "    Args:\n",
    "        sample: 包含多模态数据的样本\n",
    "        hist_steps: 使用历史时间步数（默认为5）\n",
    "    Returns:\n",
    "        包含处理后的提示词和视觉输入的字典\n",
    "    \"\"\"\n",
    "    # 提取并规范化路径\n",
    "    def normalize_paths(path_list: List[str]) -> List[str]:\n",
    "        return [os.path.normpath(p) for p in path_list]\n",
    "    # 处理所有路径\n",
    "    video_paths = normalize_paths(sample['video_paths'][:hist_steps])\n",
    "    heatmap_paths = normalize_paths(sample['heatmap_paths'][:hist_steps])\n",
    "    gps_data = sample['gps'][:hist_steps].tolist()\n",
    "    \n",
    "    # 构建时间序列提示词\n",
    "    prompt_parts = []\n",
    "    for step in range(hist_steps):\n",
    "        time_label = f\"t-{hist_steps-1-step}\" if step < hist_steps-1 else \"Current time (t)\"\n",
    "        \n",
    "        # GPS数据格式化（假设张量存储的是经度、纬度）\n",
    "        lon, lat = gps_data[step]\n",
    "        gps_str = f\"longitude:{lon:.6f}, dimension:{lat:.6f}\"\n",
    "        \n",
    "        # 添加多模态信息块\n",
    "        prompt_part = (\n",
    "            f\"-time:{time_label}\"\n",
    "            f\"-gps:{gps_str}\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    # 组合完整提示词\n",
    "    full_prompt = (\"\".join(prompt_parts))\n",
    "\n",
    "    # 提取所有视觉路径（RGB + 热力图）\n",
    "    all_image_paths = [p for pair in zip(video_paths, heatmap_paths) for p in pair]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"image_paths\": all_image_paths,\n",
    "        \"labels\": sample['target_mmwave'].argmax(dim=-1).tolist()  # 假设索引是最大值位置\n",
    "    }\n",
    "\n",
    "# 示例使用 ---------------------------------------------------\n",
    "def process_sample(sample, processor):  # 添加processor参数\n",
    "    # Step 1: 构建提示词和获取图像路径\n",
    "    processed = build_prompt_and_inputs(sample)\n",
    "    \n",
    "    # Step 2: 构建messages结构\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\", \"image\": path} for path in processed[\"image_paths\"]] + \n",
    "                  [{\"type\": \"text\", \"text\": processed[\"prompt\"]}]\n",
    "    }]\n",
    "    \n",
    "    # Step 3: 使用传入的processor处理输入\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return inputs, processed[\"labels\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048644",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ffa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedVisionDataset(Dataset):\n",
    "    def __init__(self, original_dataset, processor):\n",
    "        self.cache = []\n",
    "        self.original_dataset = original_dataset  # 保存原始数据集引用\n",
    "        \n",
    "        # 预加载所有样本\n",
    "        for i in tqdm(range(len(original_dataset)), desc=\"Caching dataset\"):\n",
    "            sample = original_dataset[i]\n",
    "            try:\n",
    "                inputs, labels = process_sample(sample, processor)\n",
    "                # 将处理后的数据转移到CPU（避免占用GPU内存）\n",
    "                inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "                self.cache.append((inputs, labels))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {e}\")\n",
    "                # 可以选择跳过错误样本或添加占位符\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小 - 这是必须实现的方法\"\"\"\n",
    "        return len(self.original_dataset)  # 或者 len(self.cache)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"直接返回缓存的处理结果\"\"\"\n",
    "        return self.cache[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "\n",
    "class QwenVisionDataset(Dataset):\n",
    "    def __init__(self, data_csv_paths, modal='mmwave_gps', input_length=8, output_length=3):\n",
    "        self.data_csv_paths = data_csv_paths\n",
    "        self.modal = modal\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        # 特征列映射\n",
    "        self.features_column = {\n",
    "            # 'rgbs': 'unit1_rgb',\n",
    "            'rgbs': 'unit1_camera_resized',\n",
    "            'u1_loc': 'unit1_loc',\n",
    "            'u2_loc': 'unit2_loc',\n",
    "            'mmwave': 'unit1_pwr_60ghz',\n",
    "            'heatmap': 'unit1_mmwave_heatmap'  # 新增热力图列\n",
    "        }\n",
    "        \n",
    "        # 初始化滑动窗口\n",
    "        self.window_samples = []\n",
    "        for seq_idx, data_csv_path in enumerate(self.data_csv_paths):\n",
    "            data_csv = pd.read_csv(data_csv_path)\n",
    "            for seq_id in data_csv['seq_index'].unique():\n",
    "                seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "                if len(seq_data) >= self.input_length:\n",
    "                    for start_idx in range(len(seq_data) - self.input_length + 1):\n",
    "                        self.window_samples.append((seq_idx, seq_id, start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, seq_id, start_idx = self.window_samples[idx]\n",
    "        base_path = os.path.dirname(self.data_csv_paths[seq_idx])\n",
    "        data_csv = pd.read_csv(self.data_csv_paths[seq_idx])\n",
    "        seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "        \n",
    "        # 获取原始路径数据\n",
    "        window_data = {\n",
    "            'video_paths': \n",
    "            seq_data[self.features_column['rgbs']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist(),\n",
    "            'heatmap_paths': \n",
    "            seq_data[self.features_column['heatmap']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist()\n",
    "        }\n",
    "\n",
    "        # 处理GPS数据\n",
    "        gps = []\n",
    "        for i in range(self.input_length):\n",
    "            u1_loc = os.path.join(base_path, seq_data[self.features_column['u1_loc']].iloc[start_idx+i])\n",
    "            u2_loc = os.path.join(base_path, seq_data[self.features_column['u2_loc']].iloc[start_idx+i])\n",
    "            \n",
    "            with open(u1_loc, 'r') as f:\n",
    "                lat1, lon1 = map(float, f.read().strip().split())\n",
    "            with open(u2_loc, 'r') as f:\n",
    "                lat2, lon2 = map(float, f.read().strip().split())\n",
    "                \n",
    "            gps.append(torch.tensor([lat2-lat1, lon2-lon1], dtype=torch.float32))\n",
    "        gps = torch.stack(gps)\n",
    "\n",
    "        # 处理mmWave数据\n",
    "        mmwave = []\n",
    "        for i in range(self.input_length):\n",
    "            mmwave_path = os.path.join(base_path, \n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                mmwave.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())), \n",
    "                    dtype=torch.float32))\n",
    "        mmwave = torch.stack(mmwave)\n",
    "\n",
    "        # 目标数据（最后output_length个时间步）\n",
    "        target = []\n",
    "        for i in range(self.input_length-self.output_length, self.input_length):\n",
    "            mmwave_path = os.path.join(base_path,\n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                target.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())),\n",
    "                    dtype=torch.float32))\n",
    "        target = torch.stack(target)\n",
    "\n",
    "        return {\n",
    "            'video_paths': [os.path.join(base_path, p) for p in window_data['video_paths']],\n",
    "            'heatmap_paths': [os.path.join(base_path, p) for p in window_data['heatmap_paths']],\n",
    "            'gps': gps,\n",
    "            'mmwave': mmwave,\n",
    "            'target_mmwave': target\n",
    "        }\n",
    "\n",
    "def qwen_collate_fn(batch):\n",
    "    collated = {\n",
    "        'video_paths': [item['video_paths'] for item in batch],\n",
    "        'heatmap_paths': [item['heatmap_paths'] for item in batch],\n",
    "        'gps': pad_sequence([item['gps'] for item in batch], batch_first=True),\n",
    "        'mmwave': pad_sequence([item['mmwave'] for item in batch], batch_first=True),\n",
    "        'target_mmwave': pad_sequence([item['target_mmwave'] for item in batch], batch_first=True)\n",
    "    }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d3200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_start_idx = 1\n",
    "dataset_end_idx = 9\n",
    "# 定义数据集路径\n",
    "dataset_path = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx, dataset_end_idx)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths = []\n",
    "for path in dataset_path:\n",
    "    data_csv_paths.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbd2179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 每个 CSV 的行数 vs 窗口样本数 ===\n",
      "scenario1/scenario1.csv: rows = 2411, windows = 2208\n",
      "scenario2/scenario2.csv: rows = 2974, windows = 2736\n",
      "scenario3/scenario3.csv: rows = 1487, windows = 1123\n",
      "scenario4/scenario4.csv: rows = 1867, windows = 1363\n",
      "scenario5/scenario5.csv: rows = 2300, windows = 2097\n",
      "scenario6/scenario6.csv: rows = 915, windows = 831\n",
      "scenario7/scenario7.csv: rows = 854, windows = 430\n",
      "scenario8/scenario8.csv: rows = 4043, windows = 3462\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 正则提取场景和序列索引\n",
    "pattern_scen = re.compile(r'scenario(\\d+)')\n",
    "input_length = 8\n",
    "\n",
    "print(\"=== 每个 CSV 的行数 vs 窗口样本数 ===\")\n",
    "for csv_path in data_csv_paths:\n",
    "    # 1) 读表\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # 2) 按 seq_index 统计窗口数\n",
    "    win_count = 0\n",
    "    for seq_id, grp in df.groupby('seq_index'):\n",
    "        L = len(grp)\n",
    "        if L >= input_length:\n",
    "            win_count += (L - input_length + 1)\n",
    "    \n",
    "    # 3) 输出\n",
    "    scen_match = pattern_scen.search(csv_path)\n",
    "    scen = scen_match.group(0) if scen_match else \"unknown\"\n",
    "    fname = os.path.basename(csv_path)\n",
    "    print(f\"{scen}/{fname}: rows = {total_rows}, windows = {win_count}\")\n",
    "print(\"====================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f7b41",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a605cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5376_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5377_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5378_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5379_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5380_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5381_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5382_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_5383_00_52_36.jpg'],\n",
       " 'heatmap_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1075.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1076.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1077.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1078.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1079.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1080.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1081.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1082.png'],\n",
       " 'gps': tensor([[2.3318e-05, 2.2738e-04],\n",
       "         [2.7988e-05, 2.2698e-04],\n",
       "         [3.2698e-05, 2.2658e-04],\n",
       "         [3.7458e-05, 2.2618e-04],\n",
       "         [4.2258e-05, 2.2588e-04],\n",
       "         [4.7098e-05, 2.2558e-04],\n",
       "         [5.1988e-05, 2.2518e-04],\n",
       "         [5.6918e-05, 2.2488e-04]]),\n",
       " 'mmwave': tensor([[0.0169, 0.0203, 0.0211, 0.0198, 0.0211, 0.0201, 0.0187, 0.0196, 0.0210,\n",
       "          0.0223, 0.0211, 0.0221, 0.0206, 0.0206, 0.0195, 0.0222, 0.0212, 0.0206,\n",
       "          0.0195, 0.0212, 0.0226, 0.0238, 0.0233, 0.0206, 0.0186, 0.0193, 0.0241,\n",
       "          0.0338, 0.0400, 0.0487, 0.0505, 0.0473, 0.0453, 0.0347, 0.0272, 0.0239,\n",
       "          0.0217, 0.0186, 0.0190, 0.0197, 0.0215, 0.0233, 0.0239, 0.0260, 0.0256,\n",
       "          0.0212, 0.0199, 0.0196, 0.0191, 0.0191, 0.0199, 0.0193, 0.0192, 0.0186,\n",
       "          0.0192, 0.0192, 0.0183, 0.0183, 0.0184, 0.0190, 0.0184, 0.0182, 0.0171,\n",
       "          0.0159],\n",
       "         [0.0174, 0.0222, 0.0234, 0.0222, 0.0207, 0.0207, 0.0210, 0.0219, 0.0258,\n",
       "          0.0254, 0.0245, 0.0235, 0.0219, 0.0225, 0.0219, 0.0272, 0.0271, 0.0217,\n",
       "          0.0239, 0.0263, 0.0375, 0.0365, 0.0293, 0.0235, 0.0219, 0.0240, 0.0423,\n",
       "          0.0815, 0.1052, 0.1008, 0.1143, 0.1066, 0.0822, 0.0552, 0.0324, 0.0234,\n",
       "          0.0199, 0.0204, 0.0236, 0.0273, 0.0257, 0.0286, 0.0346, 0.0332, 0.0322,\n",
       "          0.0234, 0.0208, 0.0206, 0.0223, 0.0239, 0.0257, 0.0225, 0.0222, 0.0208,\n",
       "          0.0217, 0.0227, 0.0217, 0.0199, 0.0191, 0.0224, 0.0216, 0.0199, 0.0185,\n",
       "          0.0163],\n",
       "         [0.0186, 0.0225, 0.0244, 0.0212, 0.0214, 0.0213, 0.0245, 0.0251, 0.0272,\n",
       "          0.0253, 0.0239, 0.0214, 0.0184, 0.0213, 0.0226, 0.0284, 0.0267, 0.0227,\n",
       "          0.0232, 0.0318, 0.0365, 0.0320, 0.0259, 0.0198, 0.0204, 0.0359, 0.0623,\n",
       "          0.1012, 0.1065, 0.0929, 0.0903, 0.0835, 0.0531, 0.0334, 0.0219, 0.0197,\n",
       "          0.0209, 0.0220, 0.0237, 0.0263, 0.0290, 0.0303, 0.0308, 0.0288, 0.0268,\n",
       "          0.0207, 0.0185, 0.0188, 0.0205, 0.0240, 0.0241, 0.0213, 0.0209, 0.0204,\n",
       "          0.0192, 0.0207, 0.0202, 0.0202, 0.0206, 0.0221, 0.0209, 0.0190, 0.0187,\n",
       "          0.0172],\n",
       "         [0.0184, 0.0218, 0.0214, 0.0197, 0.0225, 0.0263, 0.0285, 0.0309, 0.0290,\n",
       "          0.0241, 0.0215, 0.0195, 0.0185, 0.0244, 0.0266, 0.0298, 0.0326, 0.0228,\n",
       "          0.0275, 0.0320, 0.0305, 0.0257, 0.0222, 0.0200, 0.0270, 0.0518, 0.0799,\n",
       "          0.1019, 0.0933, 0.0828, 0.0704, 0.0567, 0.0351, 0.0226, 0.0194, 0.0188,\n",
       "          0.0228, 0.0268, 0.0269, 0.0296, 0.0321, 0.0330, 0.0290, 0.0249, 0.0204,\n",
       "          0.0191, 0.0193, 0.0190, 0.0207, 0.0223, 0.0220, 0.0203, 0.0211, 0.0201,\n",
       "          0.0192, 0.0189, 0.0181, 0.0193, 0.0196, 0.0205, 0.0204, 0.0193, 0.0178,\n",
       "          0.0169],\n",
       "         [0.0199, 0.0212, 0.0240, 0.0281, 0.0319, 0.0373, 0.0405, 0.0387, 0.0312,\n",
       "          0.0220, 0.0185, 0.0214, 0.0259, 0.0320, 0.0423, 0.0427, 0.0340, 0.0379,\n",
       "          0.0461, 0.0604, 0.0454, 0.0259, 0.0245, 0.0357, 0.0627, 0.1006, 0.1222,\n",
       "          0.1290, 0.1155, 0.0817, 0.0676, 0.0495, 0.0306, 0.0206, 0.0206, 0.0200,\n",
       "          0.0239, 0.0297, 0.0308, 0.0351, 0.0396, 0.0350, 0.0280, 0.0236, 0.0206,\n",
       "          0.0197, 0.0223, 0.0219, 0.0210, 0.0241, 0.0223, 0.0218, 0.0224, 0.0220,\n",
       "          0.0200, 0.0186, 0.0187, 0.0198, 0.0198, 0.0208, 0.0207, 0.0198, 0.0183,\n",
       "          0.0171],\n",
       "         [0.0203, 0.0224, 0.0240, 0.0318, 0.0294, 0.0274, 0.0281, 0.0237, 0.0215,\n",
       "          0.0191, 0.0185, 0.0190, 0.0210, 0.0222, 0.0276, 0.0288, 0.0263, 0.0256,\n",
       "          0.0293, 0.0349, 0.0236, 0.0219, 0.0314, 0.0448, 0.0755, 0.0939, 0.1082,\n",
       "          0.1029, 0.0792, 0.0563, 0.0406, 0.0329, 0.0243, 0.0190, 0.0209, 0.0220,\n",
       "          0.0253, 0.0331, 0.0358, 0.0373, 0.0343, 0.0313, 0.0238, 0.0202, 0.0203,\n",
       "          0.0202, 0.0231, 0.0212, 0.0206, 0.0227, 0.0209, 0.0205, 0.0216, 0.0221,\n",
       "          0.0206, 0.0191, 0.0191, 0.0193, 0.0194, 0.0188, 0.0192, 0.0220, 0.0199,\n",
       "          0.0172],\n",
       "         [0.0214, 0.0263, 0.0292, 0.0378, 0.0287, 0.0271, 0.0238, 0.0210, 0.0196,\n",
       "          0.0192, 0.0205, 0.0200, 0.0232, 0.0256, 0.0295, 0.0349, 0.0354, 0.0302,\n",
       "          0.0321, 0.0257, 0.0225, 0.0270, 0.0390, 0.0473, 0.0642, 0.0705, 0.0747,\n",
       "          0.0773, 0.0669, 0.0450, 0.0292, 0.0242, 0.0235, 0.0217, 0.0219, 0.0223,\n",
       "          0.0302, 0.0410, 0.0395, 0.0360, 0.0320, 0.0267, 0.0228, 0.0229, 0.0246,\n",
       "          0.0234, 0.0234, 0.0247, 0.0253, 0.0240, 0.0201, 0.0203, 0.0215, 0.0244,\n",
       "          0.0257, 0.0229, 0.0207, 0.0203, 0.0202, 0.0198, 0.0202, 0.0272, 0.0261,\n",
       "          0.0179],\n",
       "         [0.0187, 0.0213, 0.0252, 0.0245, 0.0210, 0.0208, 0.0181, 0.0176, 0.0189,\n",
       "          0.0221, 0.0206, 0.0197, 0.0229, 0.0271, 0.0302, 0.0290, 0.0282, 0.0239,\n",
       "          0.0192, 0.0184, 0.0286, 0.0496, 0.0654, 0.0738, 0.0865, 0.0931, 0.0866,\n",
       "          0.0636, 0.0463, 0.0302, 0.0224, 0.0211, 0.0243, 0.0270, 0.0373, 0.0356,\n",
       "          0.0529, 0.0608, 0.0455, 0.0315, 0.0258, 0.0227, 0.0208, 0.0240, 0.0285,\n",
       "          0.0224, 0.0211, 0.0230, 0.0223, 0.0202, 0.0187, 0.0184, 0.0189, 0.0205,\n",
       "          0.0219, 0.0215, 0.0191, 0.0192, 0.0198, 0.0219, 0.0216, 0.0232, 0.0223,\n",
       "          0.0169]]),\n",
       " 'target_mmwave': tensor([[0.0203, 0.0224, 0.0240, 0.0318, 0.0294, 0.0274, 0.0281, 0.0237, 0.0215,\n",
       "          0.0191, 0.0185, 0.0190, 0.0210, 0.0222, 0.0276, 0.0288, 0.0263, 0.0256,\n",
       "          0.0293, 0.0349, 0.0236, 0.0219, 0.0314, 0.0448, 0.0755, 0.0939, 0.1082,\n",
       "          0.1029, 0.0792, 0.0563, 0.0406, 0.0329, 0.0243, 0.0190, 0.0209, 0.0220,\n",
       "          0.0253, 0.0331, 0.0358, 0.0373, 0.0343, 0.0313, 0.0238, 0.0202, 0.0203,\n",
       "          0.0202, 0.0231, 0.0212, 0.0206, 0.0227, 0.0209, 0.0205, 0.0216, 0.0221,\n",
       "          0.0206, 0.0191, 0.0191, 0.0193, 0.0194, 0.0188, 0.0192, 0.0220, 0.0199,\n",
       "          0.0172],\n",
       "         [0.0214, 0.0263, 0.0292, 0.0378, 0.0287, 0.0271, 0.0238, 0.0210, 0.0196,\n",
       "          0.0192, 0.0205, 0.0200, 0.0232, 0.0256, 0.0295, 0.0349, 0.0354, 0.0302,\n",
       "          0.0321, 0.0257, 0.0225, 0.0270, 0.0390, 0.0473, 0.0642, 0.0705, 0.0747,\n",
       "          0.0773, 0.0669, 0.0450, 0.0292, 0.0242, 0.0235, 0.0217, 0.0219, 0.0223,\n",
       "          0.0302, 0.0410, 0.0395, 0.0360, 0.0320, 0.0267, 0.0228, 0.0229, 0.0246,\n",
       "          0.0234, 0.0234, 0.0247, 0.0253, 0.0240, 0.0201, 0.0203, 0.0215, 0.0244,\n",
       "          0.0257, 0.0229, 0.0207, 0.0203, 0.0202, 0.0198, 0.0202, 0.0272, 0.0261,\n",
       "          0.0179],\n",
       "         [0.0187, 0.0213, 0.0252, 0.0245, 0.0210, 0.0208, 0.0181, 0.0176, 0.0189,\n",
       "          0.0221, 0.0206, 0.0197, 0.0229, 0.0271, 0.0302, 0.0290, 0.0282, 0.0239,\n",
       "          0.0192, 0.0184, 0.0286, 0.0496, 0.0654, 0.0738, 0.0865, 0.0931, 0.0866,\n",
       "          0.0636, 0.0463, 0.0302, 0.0224, 0.0211, 0.0243, 0.0270, 0.0373, 0.0356,\n",
       "          0.0529, 0.0608, 0.0455, 0.0315, 0.0258, 0.0227, 0.0208, 0.0240, 0.0285,\n",
       "          0.0224, 0.0211, 0.0230, 0.0223, 0.0202, 0.0187, 0.0184, 0.0189, 0.0205,\n",
       "          0.0219, 0.0215, 0.0191, 0.0192, 0.0198, 0.0219, 0.0216, 0.0232, 0.0223,\n",
       "          0.0169]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset = QwenVisionDataset(\n",
    "    data_csv_paths,\n",
    "    input_length=8,\n",
    "    output_length=3\n",
    ")\n",
    "original_dataset[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fdb023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各场景最少窗口数 = 430\n",
      "每个场景实际采样: 20\n",
      "Balanced dataset sizes:\n",
      "  train: 128\n",
      "  val:   16\n",
      "  test:  16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 1) 从 window_samples 推场景标签\n",
    "pattern = re.compile(r'scenario(\\d+)')\n",
    "scenario_of_sample = []\n",
    "for seq_idx, seq_id, start_idx in original_dataset.window_samples:\n",
    "    csv_path = original_dataset.data_csv_paths[seq_idx]\n",
    "    match = pattern.search(csv_path)\n",
    "    scen = f\"scenario{match.group(1)}\" if match else \"unknown\"\n",
    "    scenario_of_sample.append(scen)\n",
    "\n",
    "# 2) 按场景分组\n",
    "scenario_to_indices = defaultdict(list)\n",
    "for idx, scen in enumerate(scenario_of_sample):\n",
    "    scenario_to_indices[scen].append(idx)\n",
    "\n",
    "# 3) 取各场景最小样本数\n",
    "min_count = min(len(v) for v in scenario_to_indices.values())\n",
    "print(f\"各场景最少窗口数 = {min_count}\")\n",
    "\n",
    "# —— 在这里，自行设置每个场景的采样数 —— #\n",
    "# 比如你想每个场景都采 200 个样本，就写：\n",
    "samples_per_scenario = 20\n",
    "# 如果你希望用不超过 min_count 的最大值，可以加一行：\n",
    "samples_per_scenario = min(samples_per_scenario, min_count)\n",
    "print(f\"每个场景实际采样: {samples_per_scenario}\")\n",
    "\n",
    "# —— 之后在第 4 步用 samples_per_scenario 替换 min_count —— #\n",
    "random.seed(42)\n",
    "balanced_indices = []\n",
    "for scen, indices in scenario_to_indices.items():\n",
    "    # 如果某个场景样本少于 samples_per_scenario，会报错，必要时先检查：\n",
    "    assert len(indices) >= samples_per_scenario, \\\n",
    "        f\"{scen} 只有 {len(indices)} 个样本，少于 {samples_per_scenario}\"\n",
    "    chosen = random.sample(indices, samples_per_scenario)\n",
    "    balanced_indices.extend(chosen)\n",
    "\n",
    "# 5) 每个场景内部再按 80/10/10 拆 train/val/test\n",
    "train_indices, val_indices, test_indices = [], [], []\n",
    "for scen in scenario_to_indices:\n",
    "    inds = [i for i in balanced_indices if scenario_of_sample[i] == scen]\n",
    "    random.shuffle(inds)\n",
    "    n = len(inds)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    train_indices += inds[:n_train]\n",
    "    val_indices   += inds[n_train:n_train+n_val]\n",
    "    test_indices  += inds[n_train+n_val:]\n",
    "\n",
    "# 6) 构造最终 Dataset\n",
    "train_dataset = Subset(original_dataset, train_indices)\n",
    "val_dataset   = Subset(original_dataset, val_indices)\n",
    "test_dataset  = Subset(original_dataset, test_indices)\n",
    "\n",
    "print(\"Balanced dataset sizes:\")\n",
    "print(f\"  train: {len(train_dataset)}\")\n",
    "print(f\"  val:   {len(val_dataset)}\")\n",
    "print(f\"  test:  {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a298b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train 集合场景分布 ---\n",
      "scenario1: 16\n",
      "scenario2: 16\n",
      "scenario3: 16\n",
      "scenario4: 16\n",
      "scenario5: 16\n",
      "scenario6: 16\n",
      "scenario7: 16\n",
      "scenario8: 16\n",
      "\n",
      "--- Val 集合场景分布 ---\n",
      "scenario1: 2\n",
      "scenario2: 2\n",
      "scenario3: 2\n",
      "scenario4: 2\n",
      "scenario5: 2\n",
      "scenario6: 2\n",
      "scenario7: 2\n",
      "scenario8: 2\n",
      "\n",
      "--- Test 集合场景分布 ---\n",
      "scenario1: 2\n",
      "scenario2: 2\n",
      "scenario3: 2\n",
      "scenario4: 2\n",
      "scenario5: 2\n",
      "scenario6: 2\n",
      "scenario7: 2\n",
      "scenario8: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 先重现一份 scenario_of_sample 的映射（同采样脚本中做法）\n",
    "pattern = re.compile(r'scenario(\\d+)')\n",
    "scenario_of_sample = []\n",
    "for seq_idx, seq_id, start_idx in original_dataset.window_samples:\n",
    "    csv_path = original_dataset.data_csv_paths[seq_idx]\n",
    "    m = pattern.search(csv_path)\n",
    "    scenario_of_sample.append(f\"scenario{m.group(1)}\" if m else \"unknown\")\n",
    "\n",
    "# 定义一个函数统计\n",
    "def print_distribution(indices, name):\n",
    "    cnt = Counter(scenario_of_sample[i] for i in indices)\n",
    "    print(f\"--- {name} 集合场景分布 ---\")\n",
    "    for scen in sorted(cnt.keys(), key=lambda x: int(x.replace('scenario',''))):\n",
    "        print(f\"{scen}: {cnt[scen]}\")\n",
    "    print()\n",
    "\n",
    "# 拿到各子集对应的原始索引\n",
    "train_idx = train_dataset.indices if hasattr(train_dataset, 'indices') else train_dataset.dataset_indices\n",
    "val_idx   = val_dataset.indices   if hasattr(val_dataset,   'indices') else val_dataset.dataset_indices\n",
    "test_idx  = test_dataset.indices  if hasattr(test_dataset,  'indices') else test_dataset.dataset_indices\n",
    "\n",
    "# 打印分布\n",
    "print_distribution(train_idx, 'Train')\n",
    "print_distribution(val_idx,   'Val')\n",
    "print_distribution(test_idx,  'Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4e860",
   "metadata": {},
   "source": [
    "### 划分数据集（抽出1600个样本微调）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febef180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torch.utils.data import Subset\\nimport random\\n\\n# 固定随机种子确保每次结果一致（可选）\\nrandom.seed(42)\\n\\n# 原始数据集有约 14400 个样本\\ntotal_samples = len(original_dataset)\\n\\n# 随机选出 1600 个样本的索引\\nsubset_indices = random.sample(range(total_samples), 160)\\n\\n# 创建新的 dataset\\nsmall_dataset = Subset(original_dataset, subset_indices)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# 固定随机种子确保每次结果一致（可选）\n",
    "random.seed(42)\n",
    "\n",
    "# 原始数据集有约 14400 个样本\n",
    "total_samples = len(original_dataset)\n",
    "\n",
    "# 随机选出 1600 个样本的索引\n",
    "subset_indices = random.sample(range(total_samples), 160)\n",
    "\n",
    "# 创建新的 dataset\n",
    "small_dataset = Subset(original_dataset, subset_indices)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6813b67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_size = int(0.8 * len(small_dataset))\\nval_size = int(0.1 * len(small_dataset))\\ntest_size = len(small_dataset) - train_size - val_size\\ntrain_dataset, val_dataset, test_dataset = random_split(small_dataset, [train_size, val_size, test_size])\\n\\nprint(f\"Total Training samples: {len(train_dataset)}\")\\nprint(f\"Total Validation samples: {len(val_dataset)}\")\\nprint(f\"Total Testing samples: {len(test_dataset)}\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_size = int(0.8 * len(small_dataset))\n",
    "val_size = int(0.1 * len(small_dataset))\n",
    "test_size = len(small_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(small_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Total Training samples: {len(train_dataset)}\")\n",
    "print(f\"Total Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Total Testing samples: {len(test_dataset)}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae7f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching dataset: 100%|██████████| 128/128 [00:13<00:00,  9.71it/s]\n",
      "Caching dataset: 100%|██████████| 16/16 [00:01<00:00,  9.78it/s]\n",
      "Caching dataset: 100%|██████████| 16/16 [00:01<00:00,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 创建缓存数据集（在训练前一次性处理）\n",
    "cached_train = CachedVisionDataset(train_dataset, processor)\n",
    "cached_val = CachedVisionDataset(val_dataset, processor)\n",
    "cached_test = CachedVisionDataset(test_dataset, processor)\n",
    "# 修改DataLoader使用新collate函数\n",
    "def collate_fn(batch, device):\n",
    "    \"\"\"处理缓存数据的批处理\"\"\"\n",
    "    batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "    batch_labels = []\n",
    "    \n",
    "    for (inputs, labels) in batch:\n",
    "        batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "        batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "        batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "        batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "        batch_labels.append(labels)\n",
    "    \n",
    "    # 拼接张量（保持在CPU）\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": torch.cat(batch_inputs[\"input_ids\"], dim=0),\n",
    "        \"attention_mask\": torch.cat(batch_inputs[\"attention_mask\"], dim=0),\n",
    "        \"pixel_values\": torch.cat(batch_inputs[\"pixel_values\"], dim=0),\n",
    "        \"image_grid_thw\": torch.cat(batch_inputs[\"image_grid_thw\"], dim=0)\n",
    "    }\n",
    "    batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "    \n",
    "    return batch_inputs, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8309968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 0.00 GB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |  21521 KiB |   3362 MiB |   3362 MiB |\n",
      "|       from large pool |      0 B   |  21499 KiB |   3359 MiB |   3359 MiB |\n",
      "|       from small pool |      0 B   |     22 KiB |      3 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |  21521 KiB |   3362 MiB |   3362 MiB |\n",
      "|       from large pool |      0 B   |  21499 KiB |   3359 MiB |   3359 MiB |\n",
      "|       from small pool |      0 B   |     22 KiB |      3 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |  21520 KiB |   3362 MiB |   3362 MiB |\n",
      "|       from large pool |      0 B   |  21498 KiB |   3359 MiB |   3359 MiB |\n",
      "|       from small pool |      0 B   |     21 KiB |      3 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  24576 KiB |  24576 KiB |  24576 KiB |      0 B   |\n",
      "|       from large pool |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |   3076 KiB | 494080 KiB | 494080 KiB |\n",
      "|       from large pool |      0 B   |   1029 KiB | 164640 KiB | 164640 KiB |\n",
      "|       from small pool |      0 B   |   2047 KiB | 329440 KiB | 329440 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       4    |     640    |     640    |\n",
      "|       from large pool |       0    |       1    |     160    |     160    |\n",
      "|       from small pool |       0    |       3    |     480    |     480    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       4    |     640    |     640    |\n",
      "|       from large pool |       0    |       1    |     160    |     160    |\n",
      "|       from small pool |       0    |       3    |     480    |     480    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       3    |     480    |     480    |\n",
      "|       from large pool |       0    |       1    |     160    |     160    |\n",
      "|       from small pool |       0    |       2    |     320    |     320    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    cached_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_fn, device=\"cpu\"),  # 绑定设备参数\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    cached_val, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=partial(collate_fn, device=\"cpu\"),\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    cached_test, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=partial(collate_fn, device=\"cpu\"),\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")\n",
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d90d4c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 输入数据 ===\n",
      "Input IDs 形状: torch.Size([1, 1369])\n",
      "Attention Mask 形状: torch.Size([1, 1369])\n",
      "Pixel Values 形状: torch.Size([4680, 1176])\n",
      "Image Grid THW: tensor([[ 1, 20, 34],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 20, 34],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 20, 34],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 20, 34],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 20, 34],\n",
      "        [ 1, 16, 16]])\n",
      "\n",
      "=== 标签数据 ===\n",
      "标签: [9, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "# 查看第一个训练样本\n",
    "sample_idx = 0\n",
    "inputs, labels = cached_train[sample_idx]\n",
    "\n",
    "print(\"=== 输入数据 ===\")\n",
    "print(f\"Input IDs 形状: {inputs['input_ids'].shape}\")  # (1, 序列长度)\n",
    "print(f\"Attention Mask 形状: {inputs['attention_mask'].shape}\")\n",
    "print(f\"Pixel Values 形状: {inputs['pixel_values'].shape}\")  # (1, 通道, 高, 宽)\n",
    "print(f\"Image Grid THW: {inputs['image_grid_thw']}\")\n",
    "\n",
    "print(\"\\n=== 标签数据 ===\")\n",
    "print(f\"标签: {labels}\")  # 例如 [23, 15, 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a0a4",
   "metadata": {},
   "source": [
    "## Model\n",
    "### 用Qwen构造带有输出投影模块的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2bc543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenReprogPatchHeadLight(nn.Module):\n",
    "    def __init__(self,\n",
    "                 qwen_model: nn.Module,\n",
    "                 pred_len: int = 3,       # 未来预测步数 P\n",
    "                 num_beams: int = 64,     # 类别数 C\n",
    "                 hidden_dim: int = 3584,  # Qwen 隐藏维度 D\n",
    "                 mha_heads: int = 8,      # Multi-Head Attention 的头数\n",
    "                 proj_hidden: int = 2048, # 投影层中间隐藏维度\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.qwen = qwen_model\n",
    "        self.P = pred_len\n",
    "        self.C = num_beams\n",
    "        self.D = hidden_dim\n",
    "\n",
    "        # 冻结主干\n",
    "        for p in self.qwen.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # 可训练 patch\n",
    "        self.patch_init = nn.Parameter(torch.randn(self.P, self.D))\n",
    "\n",
    "        # Patch reprogramming\n",
    "        self.reprog_mha = nn.MultiheadAttention(\n",
    "            embed_dim=self.D,\n",
    "            num_heads=mha_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 更轻量的投影头：10752 -> 2048 -> 192\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.P * self.D, proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(proj_hidden, self.P * self.C),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, image_grid_thw):\n",
    "        B = input_ids.size(0)\n",
    "\n",
    "        # 冻结主干前向\n",
    "        with torch.no_grad():\n",
    "            outputs = self.qwen(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                image_grid_thw=image_grid_thw,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        history_hidden = outputs.hidden_states[-1]  # [B, L, D]\n",
    "\n",
    "        # patch initialization\n",
    "        patch = self.patch_init.unsqueeze(0).expand(B, -1, -1)  # [B, P, D]\n",
    "\n",
    "        # reprogram\n",
    "        reprog_patch, _ = self.reprog_mha(\n",
    "            query=patch,\n",
    "            key=history_hidden,\n",
    "            value=history_hidden,\n",
    "        )  # [B, P, D]\n",
    "\n",
    "        # flatten + light projection\n",
    "        flat = reprog_patch.contiguous().view(B, self.P * self.D)  # [B, 3*3584]\n",
    "        logits = self.classifier(flat).view(B, self.P, self.C)     # [B, 3, 64]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b7894",
   "metadata": {},
   "source": [
    "### 加载Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30343410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:28:39,806 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:28:40,044 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadbe96ddf3f4e7bba4c283f693324f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Memory usage: 15.49 GB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  15865 MiB |  15865 MiB |  19228 MiB |   3362 MiB |\n",
      "|       from large pool |  15863 MiB |  15863 MiB |  19223 MiB |   3359 MiB |\n",
      "|       from small pool |      1 MiB |      1 MiB |      5 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  15865 MiB |  15865 MiB |  19228 MiB |   3362 MiB |\n",
      "|       from large pool |  15863 MiB |  15863 MiB |  19223 MiB |   3359 MiB |\n",
      "|       from small pool |      1 MiB |      1 MiB |      5 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  15816 MiB |  15816 MiB |  19178 MiB |   3362 MiB |\n",
      "|       from large pool |  15814 MiB |  15814 MiB |  19173 MiB |   3359 MiB |\n",
      "|       from small pool |      1 MiB |      1 MiB |      5 MiB |      3 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  16066 MiB |  16066 MiB |  16066 MiB |      0 B   |\n",
      "|       from large pool |  16064 MiB |  16064 MiB |  16064 MiB |      0 B   |\n",
      "|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 205274 KiB | 207346 KiB |   1514 MiB |   1314 MiB |\n",
      "|       from large pool | 204920 KiB | 206968 KiB |   1191 MiB |    990 MiB |\n",
      "|       from small pool |    354 KiB |   2047 KiB |    323 MiB |    323 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     759    |     759    |    1399    |     640    |\n",
      "|       from large pool |     361    |     361    |     521    |     160    |\n",
      "|       from small pool |     398    |     398    |     878    |     480    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     759    |     759    |    1399    |     640    |\n",
      "|       from large pool |     361    |     361    |     521    |     160    |\n",
      "|       from small pool |     398    |     398    |     878    |     480    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     221    |     221    |     221    |       0    |\n",
      "|       from large pool |     220    |     220    |     220    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     102    |     102    |     613    |     511    |\n",
      "|       from large pool |     101    |     101    |     292    |     191    |\n",
      "|       from small pool |       1    |       2    |     321    |     320    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 配置 bfloat16 精度\n",
    "qwenbf16_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,    # 设置模型权重为 bfloat16\n",
    "    trust_remote_code=True,         # 必须开启\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "print(device)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")\n",
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad003faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwenbf16_model.eval()\n",
    "for param in qwenbf16_model.parameters():\n",
    "    param.requires_grad_(False)  # 冻结所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22cd9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2_5_VLForConditionalGeneration(\n",
      "  (model): Qwen2_5_VLModel(\n",
      "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "      )\n",
      "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen2_5_VLMLP(\n",
      "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen2_5_VLPatchMerger(\n",
      "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen2_5_VLTextModel(\n",
      "      (embed_tokens): Embedding(152064, 3584)\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "          (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(qwenbf16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5841febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QwenReprogPatchHeadLight(\n",
    "    qwen_model=qwenbf16_model,\n",
    "    pred_len=3,         # 预测未来 3 步\n",
    "    num_beams=64,       # 类别数 64\n",
    "    hidden_dim=3584,    # Qwen2.5-VL 的隐藏维度\n",
    "    mha_heads=8,\n",
    "    proj_hidden=2048,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct_1 = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        # inputs already dict of input_ids, attention_mask, pixel_values, image_grid_thw\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)                     # [B, P]\n",
    "\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "            logits = model(**inputs)                   # [B, 3, 64]\n",
    "            loss   = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct_1 += calculate_accuracy(logits, labels, k=1)\n",
    "\n",
    "    train_acc1 = total_correct_1 / len(train_loader)\n",
    "    avg_loss = total_loss / (len(train_loader) / accumulation_steps)\n",
    "    return avg_loss, train_acc1\n",
    "\n",
    "# 3. 验证函数同理\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = total_correct_1 = total_correct_3 = total_correct_5 = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            with autocast(dtype=torch.bfloat16):\n",
    "                logits = model(**inputs)\n",
    "                loss   = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct_1 += calculate_accuracy(logits, labels, k=1)\n",
    "            total_correct_3 += calculate_accuracy(logits, labels, k=3)\n",
    "            total_correct_5 += calculate_accuracy(logits, labels, k=5)\n",
    "\n",
    "    n = len(val_loader)\n",
    "    return (total_loss / n,\n",
    "            total_correct_1 / n,\n",
    "            total_correct_3 / n,\n",
    "            total_correct_5 / n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e7f54",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaeb6c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1833652/567263588.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "patience  = 5\n",
    "checkpoint_dir = \"/data2/dzr/finetune/train_outputprojection_checkpoints\"\n",
    "accumulation_steps = 2\n",
    "scaler    = GradScaler()\n",
    "criterion = HybridLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4, weight_decay=1e-2\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5, \n",
    "    patience=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c512dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 15.77 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9cb3c",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217077e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1833652/3863311555.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n",
      "Training:   0%|          | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 41.78 GiB. GPU 0 has a total capacity of 79.15 GiB of which 37.05 GiB is free. Process 1796618 has 438.00 MiB memory in use. Including non-PyTorch memory, this process has 41.66 GiB memory in use. Of the allocated memory 40.60 GiB is allocated by PyTorch, and 571.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_test_loss ,train_test_acc1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Acc1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_test_acc1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, scaler, device, accumulation_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)                     \u001b[38;5;66;03m# [B, P]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m                   \u001b[38;5;66;03m# [B, 3, 64]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m     17\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss \u001b[38;5;241m/\u001b[39m accumulation_steps)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m, in \u001b[0;36mQwenReprogPatchHeadLight.forward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 冻结主干前向\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqwen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m history_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# patch initialization\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1908\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1903\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1904\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1905\u001b[0m )\n\u001b[1;32m   1906\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1908\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1927\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1661\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1659\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1661\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m     n_image_tokens \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1663\u001b[0m     n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1614\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.get_image_features\u001b[0;34m(self, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03mEncodes images into continuous embeddings that can be forwarded to the language model.\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;124;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1614\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_embeds\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:530\u001b[0m, in \u001b[0;36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m    526\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    527\u001b[0m             blk\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cu_seqlens_now, \u001b[38;5;28;01mNone\u001b[39;00m, position_embeddings\n\u001b[1;32m    528\u001b[0m         )\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(hidden_states)\n\u001b[1;32m    533\u001b[0m reverse_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(window_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:341\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 341\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:308\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionSdpaAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    306\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    307\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 308\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    312\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 41.78 GiB. GPU 0 has a total capacity of 79.15 GiB of which 37.05 GiB is free. Process 1796618 has 438.00 MiB memory in use. Including non-PyTorch memory, this process has 41.66 GiB memory in use. Of the allocated memory 40.60 GiB is allocated by PyTorch, and 571.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_test_loss ,train_test_acc1 = train_epoch(\n",
    "    model,  \n",
    "    train_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scaler, \n",
    "    device, \n",
    "    accumulation_steps\n",
    ")\n",
    "print(f\"Train Loss: {train_test_loss:.4f}\")\n",
    "print(f\"Train Acc1: {train_test_acc1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630fb03",
   "metadata": {},
   "source": [
    "### 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_inline import backend_inline\n",
    "from IPython import display\n",
    "# 定义 use_svg_display 函数\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "# 定义 set_axes 函数\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置 Matplotlib 的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    " \n",
    "\n",
    "class Animator:  #@save\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 3.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()   \n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "\n",
    "    def show(self):\n",
    "        display.display(self.fig)# 输出图像\n",
    "        display.clear_output(wait=True)# 不输出新图像，而是覆盖之前的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化画图\n",
    "animator_loss = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0, 10],\n",
    "                            legend=['train_loss','val_loss'])\n",
    "animator_acc = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0, 1],\n",
    "                            legend=['train_acc1', 'val_acc1'])\n",
    "def format_time(seconds):\n",
    "    mins, sec = divmod(seconds, 60)\n",
    "    hrs, mins = divmod(mins, 60)\n",
    "    return f\"{int(hrs)}h {int(mins)}m {int(sec)}s\"\n",
    "\n",
    "num_epochs = epochs\n",
    "best_val_loss = float('inf') # 初始化为“正无限大”（infinity）\n",
    "\n",
    "# 确保保存模型的目录存在\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 记录训练开始时间\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.2e}\")\n",
    "\n",
    "    # 训练\n",
    "    train_loss ,train_acc1 = train_epoch(model, train_loader, criterion, optimizer, scaler, device, accumulation_steps)\n",
    "\n",
    "    # 验证\n",
    "    val_loss ,acc_1 ,acc_3 ,acc_5 = evaluate(model,val_loader,criterion,device)\n",
    "    # 绘图\n",
    "    animator_loss.add(epoch + 1, [\n",
    "    train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss,\n",
    "    val_loss.item() if isinstance(val_loss, torch.Tensor) else val_loss\n",
    "    ])\n",
    "    animator_acc.add(epoch + 1, [\n",
    "    train_acc1.item() if isinstance(train_acc1, torch.Tensor) else train_acc1,\n",
    "    acc_1.item() if isinstance(acc_1, torch.Tensor) else acc_1\n",
    "    ])\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # 计算剩余时间\n",
    "    elapsed_time = epoch_end_time - training_start_time\n",
    "    avg_epoch_time = elapsed_time / (epoch + 1)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * remaining_epochs\n",
    "\n",
    "    # 转换为更易读的格式\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f},Train Accuracy@1:{train_acc1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f},Val Accuracy@1: {acc_1.item():.4f},Val Accuracy@3: {acc_3.item():.4f},Val Accuracy@5: {acc_5.item():.4f}\")\n",
    "    print(f\"Epoch Duration: {format_time(epoch_duration)}, Estimated Remaining Time: {format_time(remaining_time)}\")\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1} to {best_model_path}\")\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "    else:\n",
    "        early_stop_counter += 1  # 增加计数器\n",
    "\n",
    "    # 如果验证损失连续多个 epoch 没有改善，则停止训练\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break  # 提前停止训练\n",
    "\n",
    "    # 每隔若干个 epoch 保存模型\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'multimodal_encoder_decoder_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model at epoch {epoch+1} to {checkpoint_path}\")\n",
    "animator_loss.show()\n",
    "animator_acc.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceea436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 测试评估\n",
    "\n",
    "# 加载最佳模型\n",
    "best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Loaded best model for testing.\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}. Skipping test evaluation.\")\n",
    "\n",
    "# 定义测试评估函数（可以与验证相同）\n",
    "\n",
    "\n",
    "test_loss ,test_acc1 ,test_acc3 ,test_acc5 = evaluate(model,processor,test_loader,criterion,device)\n",
    "print(f\"Test Loss : {test_loss:.4f};Test Accuracy@3 : {test_acc3:.4f}\")\n",
    "print(f\"Test Accuracy@1 : {test_acc1:.4f};Test Accuracy@5 : {test_acc5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651b3e4",
   "metadata": {},
   "source": [
    "## 评估泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_start_idx_zero = 9\n",
    "dataset_end_idx_zero = 10\n",
    "# 定义数据集路径\n",
    "dataset_path_zero = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx_zero, dataset_end_idx_zero)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths_zero = []\n",
    "for path in dataset_path_zero:\n",
    "    data_csv_paths_zero.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths_zero)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5740bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_dataset = QwenVisionDataset(\n",
    "    data_csv_paths,\n",
    "    input_length=8,\n",
    "    output_length=3\n",
    ")\n",
    "zeroshot_dataset[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_zeroshot = CachedVisionDataset(zeroshot_dataset, processor)\n",
    "zeroshot_dataloader = DataLoader(\n",
    "    cached_zeroshot,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(collate_fn, device=\"cpu\"),  # 绑定设备参数\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_loss ,zeroshot_acc_1 ,zeroshot_acc_3 ,zeroshot_acc_5 = evaluate(model,processor,val_loader,criterion,device)\n",
    "print(f\"Zeroshot loss: {zeroshot_loss:.4f}\")\n",
    "print(f\"Zeroshot acc@1: {zeroshot_acc_1:.4f}\")\n",
    "print(f\"Zeroshot acc@3: {zeroshot_acc_3:.4f}\")\n",
    "print(f\"Zeroshot acc@5: {zeroshot_acc_5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
