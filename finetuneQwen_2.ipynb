{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2603660c",
   "metadata": {},
   "source": [
    "# 微调Qwen2.5-7B：边微调边处理输入；选择最后一个隐藏状态向量给输出头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af37cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 00:34:01,942 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 00:34:02,186 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 00:34:03,532 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "2025-05-31 00:34:03,766 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/data2/dzr/.cache\" \n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm  # 引入 tqdm 库\n",
    "import time  # 引入 time 模块\n",
    "import argparse  # 引入 argparse 模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from typing import Dict, List\n",
    "from modelscope import AutoTokenizer, AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "model_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8764cb66",
   "metadata": {},
   "source": [
    "## 定义度量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ee1888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#---------新增topkloss---------\n",
    "class NMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        mse = self.mse(output, target)\n",
    "        var = torch.var(target, unbiased=False)\n",
    "        # 防止除以零的情况\n",
    "        if var.item() == 0:\n",
    "            return torch.tensor(float('inf')).to(output.device)\n",
    "        return mse / var\n",
    "\n",
    "class TopkLoss(nn.Module):\n",
    "    def __init__(self, k=1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output : [B, T, C] 模型输出的logits（未归一化）\n",
    "            target : [B, T, C] one-hot编码 或 [B, T] 类别索引\n",
    "            B = Batch Size        批量大小（数据加载时设置的batch_size）\n",
    "            T = Sequence Length   输出序列的时间步数（output_length=3）\n",
    "            C = Num Classes       类别数量（64个离散目标类别）\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        output_flat = output.view(B*T, C)  # [B*T, C]\n",
    "        target_flat = target.contiguous().view(-1)  # [B*T]\n",
    "        \n",
    "        # 计算Top-k正确性\n",
    "        _, topk_indices = torch.topk(output_flat, self.k, dim=1)  # [B*T, k]\n",
    "        correct = topk_indices.eq(target_flat.unsqueeze(1)).any(dim=1)  # [B*T]\n",
    "        \n",
    "        # 计算损失（仅惩罚Top-k错误的样本）\n",
    "        loss = F.cross_entropy(output_flat, target_flat, reduction='none')  # [B*T]，表示每个样本的预测是否在 Top-K 中命中真实标签\n",
    "        masked_loss = loss * ~correct  # 仅保留错误样本的损失值，正确样本的损失被置零\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return masked_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return masked_loss.sum()\n",
    "        return masked_loss\n",
    "\n",
    "#------------新增HybridLoss--------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, k=3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 混合权重\n",
    "        self.k = k\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        output : [B, T, C]\n",
    "        target : [B, T]\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        \n",
    "        # 常规交叉熵损失（保持生成特性）\n",
    "        ce_loss = self.ce(output.view(-1, C), target.view(-1))  # [B*T]\n",
    "        \n",
    "        # Top-K增强损失\n",
    "        _, topk = output.topk(self.k, dim=-1)  # [B, T, k]\n",
    "        correct = topk.eq(target.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "        topk_loss = (1 - correct.float()).mean()  # 错误率\n",
    "        \n",
    "        # 时间依赖惩罚项\n",
    "        seq_penalty = self._sequence_consistency(output, target)  # [1]\n",
    "        \n",
    "        return self.alpha*ce_loss.mean() + (1-self.alpha)*topk_loss + seq_penalty\n",
    "        \n",
    "    def _sequence_consistency(self, output, target):\n",
    "        \"\"\"\n",
    "        惩罚相邻时间步预测不一致的情况\n",
    "        \"\"\"\n",
    "        preds = output.argmax(-1)  # [B, T]\n",
    "        diff = (preds[:, 1:] != preds[:, :-1]).float().mean()\n",
    "        return diff * 0.2  # 可调节系数\n",
    "    \n",
    "#-------------新增CrossEntropyloss-----------------\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')  # 始终返回非归约结果\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # 处理one-hot编码目标\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "\n",
    "        # 重塑维度\n",
    "        output = output.view(-1, output.size(-1))  # [B*T, C]\n",
    "        target = target.view(-1)                   # [B*T]\n",
    "\n",
    "        # 计算基础损失\n",
    "        ce_loss = self.ce(output, target)\n",
    "        \n",
    "        # 自定义归约方式\n",
    "        if self.reduction == 'mean':\n",
    "            return ce_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return ce_loss.sum()\n",
    "        return ce_loss  # 'none'模式返回原始形状\n",
    "\n",
    "def calculate_accuracy(output, target, k=3):\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        with torch.no_grad():\n",
    "            _, pred = output.topk(k, dim=-1)  # [B, T, k]\n",
    "            correct = pred.eq(target.unsqueeze(-1)).any(dim=-1)\n",
    "            return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7152d0",
   "metadata": {},
   "source": [
    "## Processer\n",
    "### 构建多模态提示词并提取视觉输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb251bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_inputs(sample: Dict, hist_steps: int = 5) -> Dict:\n",
    "    \"\"\"构建多模态提示词并提取视觉输入\n",
    "    Args:\n",
    "        sample: 包含多模态数据的样本\n",
    "        hist_steps: 使用历史时间步数（默认为5）\n",
    "    Returns:\n",
    "        包含处理后的提示词和视觉输入的字典\n",
    "    \"\"\"\n",
    "    # 提取并规范化路径\n",
    "    def normalize_paths(path_list: List[str]) -> List[str]:\n",
    "        return [os.path.normpath(p) for p in path_list]\n",
    "    # 处理所有路径\n",
    "    video_paths = normalize_paths(sample['video_paths'][:hist_steps])\n",
    "    heatmap_paths = normalize_paths(sample['heatmap_paths'][:hist_steps])\n",
    "    gps_data = sample['gps'][:hist_steps].tolist()\n",
    "    \n",
    "    # 构建时间序列提示词\n",
    "    prompt_parts = []\n",
    "    for step in range(hist_steps):\n",
    "        time_label = f\"t-{hist_steps-1-step}\" if step < hist_steps-1 else \"Current time (t)\"\n",
    "        \n",
    "        # GPS数据格式化（假设张量存储的是经度、纬度）\n",
    "        lon, lat = gps_data[step]\n",
    "        gps_str = f\"longitude:{lon:.6f},dimension:{lat:.6f}\"\n",
    "        \n",
    "        # 添加多模态信息块\n",
    "        prompt_part = (\n",
    "            f\"time:{time_label}\"\n",
    "            f\"gps:{gps_str}\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    # 组合完整提示词\n",
    "    full_prompt = (\"\".join(prompt_parts) )\n",
    "    \n",
    "    # 提取所有视觉路径（RGB + 热力图）\n",
    "    all_image_paths = [p for pair in zip(video_paths, heatmap_paths) for p in pair]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"image_paths\": all_image_paths,\n",
    "        \"target_mmwave\": sample['target_mmwave']\n",
    "    }\n",
    "\n",
    "# 示例使用 ---------------------------------------------------\n",
    "def process_sample(sample, processor):  # 添加processor参数\n",
    "    # Step 1: 构建提示词和获取图像路径\n",
    "    processed = build_prompt_and_inputs(sample)\n",
    "    \n",
    "    # Step 2: 构建messages结构\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\", \"image\": path} for path in processed[\"image_paths\"]] + \n",
    "                  [{\"type\": \"text\", \"text\": processed[\"prompt\"]}]\n",
    "    }]\n",
    "    \n",
    "    # Step 3: 使用传入的processor处理输入\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    return inputs, processed[\"target_mmwave\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048644",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "\n",
    "class QwenVisionDataset(Dataset):\n",
    "    def __init__(self, data_csv_paths, modal='mmwave_gps', input_length=8, output_length=3):\n",
    "        self.data_csv_paths = data_csv_paths\n",
    "        self.modal = modal\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        # 特征列映射\n",
    "        self.features_column = {\n",
    "            # 'rgbs': 'unit1_rgb',\n",
    "            'rgbs': 'unit1_camera_resized',\n",
    "            'u1_loc': 'unit1_loc',\n",
    "            'u2_loc': 'unit2_loc',\n",
    "            'mmwave': 'unit1_pwr_60ghz',\n",
    "            'heatmap': 'unit1_mmwave_heatmap'  # 新增热力图列\n",
    "        }\n",
    "        \n",
    "        # 初始化滑动窗口\n",
    "        self.window_samples = []\n",
    "        for seq_idx, data_csv_path in enumerate(self.data_csv_paths):\n",
    "            data_csv = pd.read_csv(data_csv_path)\n",
    "            for seq_id in data_csv['seq_index'].unique():\n",
    "                seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "                if len(seq_data) >= self.input_length:\n",
    "                    for start_idx in range(len(seq_data) - self.input_length + 1):\n",
    "                        self.window_samples.append((seq_idx, seq_id, start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, seq_id, start_idx = self.window_samples[idx]\n",
    "        base_path = os.path.dirname(self.data_csv_paths[seq_idx])\n",
    "        data_csv = pd.read_csv(self.data_csv_paths[seq_idx])\n",
    "        seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "        \n",
    "        # 获取原始路径数据\n",
    "        window_data = {\n",
    "            'video_paths': \n",
    "            seq_data[self.features_column['rgbs']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist(),\n",
    "            'heatmap_paths': \n",
    "            seq_data[self.features_column['heatmap']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist()\n",
    "        }\n",
    "\n",
    "        # 处理GPS数据\n",
    "        gps = []\n",
    "        for i in range(self.input_length):\n",
    "            u1_loc = os.path.join(base_path, seq_data[self.features_column['u1_loc']].iloc[start_idx+i])\n",
    "            u2_loc = os.path.join(base_path, seq_data[self.features_column['u2_loc']].iloc[start_idx+i])\n",
    "            \n",
    "            with open(u1_loc, 'r') as f:\n",
    "                lat1, lon1 = map(float, f.read().strip().split())\n",
    "            with open(u2_loc, 'r') as f:\n",
    "                lat2, lon2 = map(float, f.read().strip().split())\n",
    "                \n",
    "            gps.append(torch.tensor([lat2-lat1, lon2-lon1], dtype=torch.float32))\n",
    "        gps = torch.stack(gps)\n",
    "\n",
    "        # 处理mmWave数据\n",
    "        mmwave = []\n",
    "        for i in range(self.input_length):\n",
    "            mmwave_path = os.path.join(base_path, \n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                mmwave.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())), \n",
    "                    dtype=torch.float32))\n",
    "        mmwave = torch.stack(mmwave)\n",
    "\n",
    "        # 目标数据（最后output_length个时间步）\n",
    "        target = []\n",
    "        for i in range(self.input_length-self.output_length, self.input_length):\n",
    "            mmwave_path = os.path.join(base_path,\n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                target.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())),\n",
    "                    dtype=torch.float32))\n",
    "        target = torch.stack(target)\n",
    "\n",
    "        return {\n",
    "            'video_paths': [os.path.join(base_path, p) for p in window_data['video_paths']],\n",
    "            'heatmap_paths': [os.path.join(base_path, p) for p in window_data['heatmap_paths']],\n",
    "            'gps': gps,\n",
    "            'mmwave': mmwave,\n",
    "            'target_mmwave': target\n",
    "        }\n",
    "\n",
    "def qwen_collate_fn(batch):\n",
    "    collated = {\n",
    "        'video_paths': [item['video_paths'] for item in batch],\n",
    "        'heatmap_paths': [item['heatmap_paths'] for item in batch],\n",
    "        'gps': pad_sequence([item['gps'] for item in batch], batch_first=True),\n",
    "        'mmwave': pad_sequence([item['mmwave'] for item in batch], batch_first=True),\n",
    "        'target_mmwave': pad_sequence([item['target_mmwave'] for item in batch], batch_first=True)\n",
    "    }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d3200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_start_idx = 1\n",
    "dataset_end_idx = 9\n",
    "# 定义数据集路径\n",
    "dataset_path = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx, dataset_end_idx)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths = []\n",
    "for path in dataset_path:\n",
    "    data_csv_paths.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f7b41",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a605cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_554_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_555_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_556_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_557_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_558_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_559_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_560_00_42_26.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_resized/image_BS1_561_00_42_26.jpg'],\n",
       " 'heatmap_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_98.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_99.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_100.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_101.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_102.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_103.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_104.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_105.png'],\n",
       " 'gps': tensor([[-3.8232e-05,  1.6488e-04],\n",
       "         [-4.2452e-05,  1.6538e-04],\n",
       "         [-4.6712e-05,  1.6588e-04],\n",
       "         [-5.1002e-05,  1.6648e-04],\n",
       "         [-5.5332e-05,  1.6698e-04],\n",
       "         [-5.9692e-05,  1.6758e-04],\n",
       "         [-6.4092e-05,  1.6808e-04],\n",
       "         [-6.8522e-05,  1.6868e-04]]),\n",
       " 'mmwave': tensor([[0.0161, 0.0191, 0.0196, 0.0225, 0.0221, 0.0225, 0.0188, 0.0182, 0.0185,\n",
       "          0.0197, 0.0216, 0.0221, 0.0227, 0.0217, 0.0207, 0.0240, 0.0220, 0.0195,\n",
       "          0.0190, 0.0193, 0.0198, 0.0215, 0.0220, 0.0213, 0.0218, 0.0293, 0.0311,\n",
       "          0.0283, 0.0238, 0.0248, 0.0221, 0.0265, 0.0279, 0.0298, 0.0317, 0.0337,\n",
       "          0.0375, 0.0431, 0.0388, 0.0267, 0.0263, 0.0483, 0.0858, 0.1219, 0.1708,\n",
       "          0.2159, 0.2279, 0.2392, 0.1746, 0.1013, 0.0824, 0.0617, 0.0341, 0.0275,\n",
       "          0.0269, 0.0302, 0.0418, 0.0415, 0.0604, 0.0828, 0.0699, 0.0492, 0.0235,\n",
       "          0.0221],\n",
       "         [0.0178, 0.0202, 0.0215, 0.0235, 0.0249, 0.0224, 0.0205, 0.0193, 0.0206,\n",
       "          0.0241, 0.0286, 0.0293, 0.0318, 0.0312, 0.0259, 0.0271, 0.0247, 0.0209,\n",
       "          0.0228, 0.0241, 0.0205, 0.0267, 0.0300, 0.0342, 0.0388, 0.0403, 0.0507,\n",
       "          0.0457, 0.0458, 0.0430, 0.0283, 0.0243, 0.0374, 0.0442, 0.0500, 0.0466,\n",
       "          0.0478, 0.0639, 0.0742, 0.0804, 0.0604, 0.0438, 0.0665, 0.1002, 0.1647,\n",
       "          0.2334, 0.2686, 0.2683, 0.2188, 0.1595, 0.1582, 0.1008, 0.0617, 0.0426,\n",
       "          0.0191, 0.0230, 0.0248, 0.0326, 0.0370, 0.0639, 0.0563, 0.0563, 0.0357,\n",
       "          0.0207],\n",
       "         [0.0273, 0.0287, 0.0209, 0.0228, 0.0253, 0.0249, 0.0261, 0.0273, 0.0238,\n",
       "          0.0211, 0.0256, 0.0324, 0.0432, 0.0382, 0.0269, 0.0259, 0.0222, 0.0218,\n",
       "          0.0301, 0.0275, 0.0190, 0.0246, 0.0283, 0.0345, 0.0387, 0.0342, 0.0269,\n",
       "          0.0289, 0.0390, 0.0464, 0.0297, 0.0213, 0.0210, 0.0247, 0.0298, 0.0312,\n",
       "          0.0340, 0.0380, 0.0481, 0.0557, 0.0582, 0.0420, 0.0263, 0.0242, 0.0366,\n",
       "          0.0830, 0.1062, 0.1214, 0.1225, 0.1121, 0.1136, 0.0988, 0.0688, 0.0505,\n",
       "          0.0272, 0.0216, 0.0219, 0.0229, 0.0247, 0.0273, 0.0313, 0.0296, 0.0251,\n",
       "          0.0181],\n",
       "         [0.0293, 0.0353, 0.0232, 0.0207, 0.0217, 0.0212, 0.0227, 0.0285, 0.0286,\n",
       "          0.0243, 0.0208, 0.0278, 0.0366, 0.0419, 0.0264, 0.0202, 0.0191, 0.0215,\n",
       "          0.0308, 0.0305, 0.0232, 0.0216, 0.0222, 0.0267, 0.0348, 0.0267, 0.0215,\n",
       "          0.0209, 0.0300, 0.0374, 0.0296, 0.0215, 0.0222, 0.0205, 0.0200, 0.0208,\n",
       "          0.0267, 0.0254, 0.0296, 0.0379, 0.0413, 0.0351, 0.0252, 0.0222, 0.0299,\n",
       "          0.0551, 0.0823, 0.1074, 0.1214, 0.1201, 0.1316, 0.1171, 0.0901, 0.0709,\n",
       "          0.0412, 0.0257, 0.0268, 0.0272, 0.0264, 0.0283, 0.0341, 0.0411, 0.0351,\n",
       "          0.0202],\n",
       "         [0.0250, 0.0314, 0.0248, 0.0234, 0.0201, 0.0187, 0.0196, 0.0252, 0.0240,\n",
       "          0.0252, 0.0226, 0.0207, 0.0276, 0.0305, 0.0274, 0.0214, 0.0207, 0.0208,\n",
       "          0.0234, 0.0207, 0.0228, 0.0245, 0.0202, 0.0270, 0.0263, 0.0276, 0.0224,\n",
       "          0.0246, 0.0292, 0.0343, 0.0364, 0.0244, 0.0281, 0.0216, 0.0217, 0.0269,\n",
       "          0.0322, 0.0330, 0.0283, 0.0312, 0.0456, 0.0497, 0.0419, 0.0361, 0.0284,\n",
       "          0.0318, 0.0568, 0.0885, 0.1042, 0.1229, 0.1430, 0.1293, 0.1158, 0.0900,\n",
       "          0.0638, 0.0391, 0.0279, 0.0242, 0.0218, 0.0211, 0.0254, 0.0279, 0.0306,\n",
       "          0.0196],\n",
       "         [0.0222, 0.0310, 0.0250, 0.0282, 0.0243, 0.0196, 0.0190, 0.0188, 0.0234,\n",
       "          0.0290, 0.0267, 0.0222, 0.0210, 0.0242, 0.0240, 0.0279, 0.0235, 0.0262,\n",
       "          0.0223, 0.0199, 0.0234, 0.0290, 0.0245, 0.0217, 0.0215, 0.0211, 0.0264,\n",
       "          0.0302, 0.0279, 0.0268, 0.0254, 0.0322, 0.0253, 0.0248, 0.0214, 0.0231,\n",
       "          0.0303, 0.0297, 0.0283, 0.0258, 0.0350, 0.0438, 0.0449, 0.0412, 0.0296,\n",
       "          0.0312, 0.0425, 0.0691, 0.1145, 0.1419, 0.1691, 0.1500, 0.1492, 0.1613,\n",
       "          0.1102, 0.0651, 0.0348, 0.0307, 0.0242, 0.0199, 0.0206, 0.0238, 0.0226,\n",
       "          0.0181],\n",
       "         [0.0227, 0.0251, 0.0253, 0.0282, 0.0259, 0.0234, 0.0207, 0.0199, 0.0210,\n",
       "          0.0249, 0.0255, 0.0225, 0.0203, 0.0215, 0.0228, 0.0242, 0.0310, 0.0269,\n",
       "          0.0215, 0.0189, 0.0224, 0.0265, 0.0225, 0.0227, 0.0194, 0.0222, 0.0258,\n",
       "          0.0310, 0.0257, 0.0265, 0.0272, 0.0331, 0.0281, 0.0280, 0.0241, 0.0246,\n",
       "          0.0287, 0.0363, 0.0292, 0.0266, 0.0312, 0.0415, 0.0511, 0.0518, 0.0355,\n",
       "          0.0282, 0.0307, 0.0593, 0.0986, 0.1469, 0.1782, 0.1723, 0.1655, 0.1622,\n",
       "          0.1471, 0.1089, 0.0653, 0.0426, 0.0329, 0.0240, 0.0204, 0.0202, 0.0211,\n",
       "          0.0196],\n",
       "         [0.0187, 0.0204, 0.0205, 0.0255, 0.0272, 0.0285, 0.0240, 0.0190, 0.0200,\n",
       "          0.0222, 0.0260, 0.0215, 0.0198, 0.0200, 0.0199, 0.0254, 0.0276, 0.0268,\n",
       "          0.0219, 0.0211, 0.0220, 0.0236, 0.0282, 0.0221, 0.0208, 0.0210, 0.0254,\n",
       "          0.0299, 0.0243, 0.0236, 0.0245, 0.0265, 0.0287, 0.0263, 0.0303, 0.0234,\n",
       "          0.0246, 0.0282, 0.0274, 0.0251, 0.0247, 0.0310, 0.0387, 0.0493, 0.0398,\n",
       "          0.0246, 0.0235, 0.0234, 0.0363, 0.0777, 0.0893, 0.1020, 0.1355, 0.1665,\n",
       "          0.1373, 0.0894, 0.0768, 0.0682, 0.0563, 0.0311, 0.0235, 0.0213, 0.0210,\n",
       "          0.0194]]),\n",
       " 'target_mmwave': tensor([[0.0222, 0.0310, 0.0250, 0.0282, 0.0243, 0.0196, 0.0190, 0.0188, 0.0234,\n",
       "          0.0290, 0.0267, 0.0222, 0.0210, 0.0242, 0.0240, 0.0279, 0.0235, 0.0262,\n",
       "          0.0223, 0.0199, 0.0234, 0.0290, 0.0245, 0.0217, 0.0215, 0.0211, 0.0264,\n",
       "          0.0302, 0.0279, 0.0268, 0.0254, 0.0322, 0.0253, 0.0248, 0.0214, 0.0231,\n",
       "          0.0303, 0.0297, 0.0283, 0.0258, 0.0350, 0.0438, 0.0449, 0.0412, 0.0296,\n",
       "          0.0312, 0.0425, 0.0691, 0.1145, 0.1419, 0.1691, 0.1500, 0.1492, 0.1613,\n",
       "          0.1102, 0.0651, 0.0348, 0.0307, 0.0242, 0.0199, 0.0206, 0.0238, 0.0226,\n",
       "          0.0181],\n",
       "         [0.0227, 0.0251, 0.0253, 0.0282, 0.0259, 0.0234, 0.0207, 0.0199, 0.0210,\n",
       "          0.0249, 0.0255, 0.0225, 0.0203, 0.0215, 0.0228, 0.0242, 0.0310, 0.0269,\n",
       "          0.0215, 0.0189, 0.0224, 0.0265, 0.0225, 0.0227, 0.0194, 0.0222, 0.0258,\n",
       "          0.0310, 0.0257, 0.0265, 0.0272, 0.0331, 0.0281, 0.0280, 0.0241, 0.0246,\n",
       "          0.0287, 0.0363, 0.0292, 0.0266, 0.0312, 0.0415, 0.0511, 0.0518, 0.0355,\n",
       "          0.0282, 0.0307, 0.0593, 0.0986, 0.1469, 0.1782, 0.1723, 0.1655, 0.1622,\n",
       "          0.1471, 0.1089, 0.0653, 0.0426, 0.0329, 0.0240, 0.0204, 0.0202, 0.0211,\n",
       "          0.0196],\n",
       "         [0.0187, 0.0204, 0.0205, 0.0255, 0.0272, 0.0285, 0.0240, 0.0190, 0.0200,\n",
       "          0.0222, 0.0260, 0.0215, 0.0198, 0.0200, 0.0199, 0.0254, 0.0276, 0.0268,\n",
       "          0.0219, 0.0211, 0.0220, 0.0236, 0.0282, 0.0221, 0.0208, 0.0210, 0.0254,\n",
       "          0.0299, 0.0243, 0.0236, 0.0245, 0.0265, 0.0287, 0.0263, 0.0303, 0.0234,\n",
       "          0.0246, 0.0282, 0.0274, 0.0251, 0.0247, 0.0310, 0.0387, 0.0493, 0.0398,\n",
       "          0.0246, 0.0235, 0.0234, 0.0363, 0.0777, 0.0893, 0.1020, 0.1355, 0.1665,\n",
       "          0.1373, 0.0894, 0.0768, 0.0682, 0.0563, 0.0311, 0.0235, 0.0213, 0.0210,\n",
       "          0.0194]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = QwenVisionDataset(\n",
    "    data_csv_paths,\n",
    "    input_length=8,\n",
    "    output_length=3\n",
    ")\n",
    "dataset[98]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4e860",
   "metadata": {},
   "source": [
    "### 划分数据集（抽出1600个样本微调）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# 固定随机种子确保每次结果一致（可选）\n",
    "random.seed(42)\n",
    "\n",
    "# 原始数据集有约 14400 个样本\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# 随机选出 1600 个样本的索引\n",
    "subset_indices = random.sample(range(total_samples), 320)\n",
    "\n",
    "# 创建新的 dataset\n",
    "small_dataset = Subset(dataset, subset_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6813b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples: 128\n",
      "Total Validation samples: 16\n",
      "Total Testing samples: 16\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(small_dataset))\n",
    "val_size = int(0.1 * len(small_dataset))\n",
    "test_size = len(small_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(small_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Total Training samples: {len(train_dataset)}\")\n",
    "print(f\"Total Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Total Testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    # 直接返回样本列表，不进行合并\n",
    "    return batch\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  \n",
    "    collate_fn=custom_collate,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a0a4",
   "metadata": {},
   "source": [
    "## Model\n",
    "### 用Qwen构造带有64类分类头的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bc543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen_and_Head(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.qwen = pretrained_model\n",
    "        joint_hidden_size = 3584\n",
    "\n",
    "        # final head now produces 3×64 dims\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(joint_hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 3 * 64),    # 3 timesteps × 64 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, image_grid_thw):\n",
    "        outputs = self.qwen(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_grid_thw=image_grid_thw,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # grab the [CLS] token\n",
    "        last_hidden = outputs.hidden_states[-1]   # (B, L, D)\n",
    "        cls_token   = last_hidden[:, -1, :]        # (B, D)\n",
    "\n",
    "        # project to (B, 3*64) and reshape\n",
    "        logits_flat = self.classifier(cls_token)            # (B, 192)\n",
    "        logits      = logits_flat.view(-1, 3, 64)           # (B, 3, 64)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b7894",
   "metadata": {},
   "source": [
    "### 加载Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de76fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen-vl-utils[decord]==0.0.8\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30343410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 00:34:07,646 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 00:34:07,884 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f5d99a551e4ce988acac4b2e7dcc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 15.49 GB\n"
     ]
    }
   ],
   "source": [
    "# 配置 bfloat16 精度\n",
    "finetune_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,    # 设置模型权重为 bfloat16\n",
    "    device_map=\"cuda\",              # 自动分配设备\n",
    "    trust_remote_code=True,         # 必须开启\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device=device)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cd9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2_5_VLForConditionalGeneration(\n",
      "  (model): Qwen2_5_VLModel(\n",
      "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "      )\n",
      "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "      (blocks): ModuleList(\n",
      "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "          (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (mlp): Qwen2_5_VLMLP(\n",
      "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (merger): Qwen2_5_VLPatchMerger(\n",
      "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (language_model): Qwen2_5_VLTextModel(\n",
      "      (embed_tokens): Embedding(152064, 3584)\n",
      "      (layers): ModuleList(\n",
      "        (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "          (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "            (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "            (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "            (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "            (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e842dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/home/dzr/anaconda3/envs/mllm/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#  冻结原模型所有参数\n",
    "for param in finetune_model.parameters():\n",
    "    param.requires_grad = False\n",
    "#  配置 LoRA Adapter\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                         # LoRA rank\n",
    "    lora_alpha=32,               # LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "#  注入 LoRA\n",
    "qwen_lora = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e69a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 0.00 GB\n",
      "Qwen_and_Head(\n",
      "  (qwen): PeftModelForCausalLM(\n",
      "    (base_model): LoraModel(\n",
      "      (model): Qwen2_5_VLForConditionalGeneration(\n",
      "        (model): Qwen2_5_VLModel(\n",
      "          (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "            (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "              (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "            )\n",
      "            (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "            (blocks): ModuleList(\n",
      "              (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "                (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "                (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "                (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "                  (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "                  (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                )\n",
      "                (mlp): Qwen2_5_VLMLP(\n",
      "                  (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                  (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                  (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (merger): Qwen2_5_VLPatchMerger(\n",
      "              (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "              (mlp): Sequential(\n",
      "                (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "                (1): GELU(approximate='none')\n",
      "                (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (language_model): Qwen2_5_VLTextModel(\n",
      "            (embed_tokens): Embedding(152064, 3584)\n",
      "            (layers): ModuleList(\n",
      "              (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "                (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "                  (q_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                  (v_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "                  (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): Qwen2MLP(\n",
      "                  (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                  (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                  (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "                (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              )\n",
      "            )\n",
      "            (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "          )\n",
      "        )\n",
      "        (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=192, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "finetuner = Qwen_and_Head(pretrained_model=qwen_lora).to(device)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(finetuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f8aeb",
   "metadata": {},
   "source": [
    "### 检查输入样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fc16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'time:t-4gps:longitude:0.000087,dimension:0.000158time:t-3gps:longitude:0.000084,dimension:0.000159time:t-2gps:longitude:0.000081,dimension:0.000159time:t-1gps:longitude:0.000078,dimension:0.000159time:Current time (t)gps:longitude:0.000075,dimension:0.000159', 'image_paths': ['/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_976_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_123.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_977_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_124.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_978_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_125.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_979_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_126.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_resized/image_BS1_980_02_12_22.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_127.png'], 'target_mmwave': tensor([[0.0778, 0.1865, 0.1825, 0.1027, 0.0815, 0.0884, 0.1438, 0.2466, 0.3730,\n",
      "         0.4503, 0.5026, 0.5143, 0.4780, 0.4624, 0.3400, 0.2270, 0.1590, 0.0933,\n",
      "         0.0871, 0.0929, 0.1002, 0.0913, 0.1182, 0.1346, 0.1863, 0.1726, 0.1324,\n",
      "         0.0892, 0.0785, 0.0791, 0.0952, 0.0975, 0.0934, 0.1030, 0.1114, 0.0994,\n",
      "         0.0887, 0.0862, 0.0865, 0.0838, 0.0737, 0.0849, 0.0736, 0.0811, 0.0834,\n",
      "         0.0932, 0.0893, 0.0802, 0.0787, 0.0791, 0.0946, 0.1072, 0.1015, 0.0791,\n",
      "         0.0751, 0.0793, 0.0795, 0.0822, 0.0977, 0.0914, 0.0857, 0.0874, 0.0756,\n",
      "         0.0614],\n",
      "        [0.1002, 0.1837, 0.2138, 0.1128, 0.0987, 0.0933, 0.0952, 0.1936, 0.2908,\n",
      "         0.4070, 0.4543, 0.4580, 0.4948, 0.4694, 0.3498, 0.2551, 0.2063, 0.1130,\n",
      "         0.0881, 0.0809, 0.0876, 0.0780, 0.1017, 0.1337, 0.1671, 0.1830, 0.1427,\n",
      "         0.1074, 0.0838, 0.0787, 0.0839, 0.0986, 0.0971, 0.1164, 0.1056, 0.1063,\n",
      "         0.0890, 0.0791, 0.0801, 0.0885, 0.0769, 0.0822, 0.0709, 0.0798, 0.0789,\n",
      "         0.0904, 0.1004, 0.0845, 0.0882, 0.0758, 0.0913, 0.0941, 0.1040, 0.0861,\n",
      "         0.0782, 0.0748, 0.0761, 0.0727, 0.0978, 0.0940, 0.0966, 0.0926, 0.0765,\n",
      "         0.0607],\n",
      "        [0.0667, 0.1895, 0.2176, 0.1707, 0.1318, 0.1359, 0.0890, 0.1056, 0.1989,\n",
      "         0.3248, 0.3945, 0.4246, 0.4703, 0.4939, 0.3897, 0.3163, 0.2616, 0.1739,\n",
      "         0.1020, 0.0747, 0.0825, 0.0759, 0.0901, 0.1179, 0.1620, 0.1808, 0.1672,\n",
      "         0.1444, 0.1161, 0.0904, 0.0875, 0.0955, 0.1032, 0.1408, 0.1110, 0.1191,\n",
      "         0.0936, 0.0787, 0.0805, 0.0821, 0.0740, 0.0809, 0.0697, 0.0764, 0.0765,\n",
      "         0.0853, 0.0898, 0.1068, 0.0938, 0.0799, 0.0835, 0.0888, 0.0945, 0.1001,\n",
      "         0.0973, 0.0784, 0.0703, 0.0721, 0.0846, 0.0923, 0.1015, 0.0991, 0.0856,\n",
      "         0.0633]])}\n",
      "({'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653,\n",
      "         151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653,\n",
      "         151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653,   1678,  60777,     12,\n",
      "             19,  74511,     25,  25446,     25,     15,     13,     15,     15,\n",
      "             15,     15,     23,     22,  11991,  18161,     25,     15,     13,\n",
      "             15,     15,     15,     16,     20,     23,   1678,  60777,     12,\n",
      "             18,  74511,     25,  25446,     25,     15,     13,     15,     15,\n",
      "             15,     15,     23,     19,  11991,  18161,     25,     15,     13,\n",
      "             15,     15,     15,     16,     20,     24,   1678,  60777,     12,\n",
      "             17,  74511,     25,  25446,     25,     15,     13,     15,     15,\n",
      "             15,     15,     23,     16,  11991,  18161,     25,     15,     13,\n",
      "             15,     15,     15,     16,     20,     24,   1678,  60777,     12,\n",
      "             16,  74511,     25,  25446,     25,     15,     13,     15,     15,\n",
      "             15,     15,     22,     23,  11991,  18161,     25,     15,     13,\n",
      "             15,     15,     15,     16,     20,     24,   1678,     25,   5405,\n",
      "            882,    320,     83,      8,  74511,     25,  25446,     25,     15,\n",
      "             13,     15,     15,     15,     15,     22,     20,  11991,  18161,\n",
      "             25,     15,     13,     15,     15,     15,     16,     20,     24,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0'), 'pixel_values': tensor([[-1.5295, -1.3105, -1.1937,  ..., -1.1105, -1.0252, -1.0963],\n",
      "        [-1.2959, -1.2813, -1.3689,  ..., -1.3096, -1.4376, -1.4802],\n",
      "        [-1.4565, -1.1645, -0.4346,  ..., -1.3238, -1.3238, -1.3380],\n",
      "        ...,\n",
      "        [-0.7558, -0.7558, -0.7558,  ..., -0.0724, -0.0724, -0.0724],\n",
      "        [-0.7558, -0.7558, -0.7558,  ..., -0.0582, -0.0582, -0.0724],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 10, 18],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 18],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 18],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 18],\n",
      "        [ 1,  8,  8],\n",
      "        [ 1, 10, 18],\n",
      "        [ 1,  8,  8]], device='cuda:0')}, tensor([[0.0778, 0.1865, 0.1825, 0.1027, 0.0815, 0.0884, 0.1438, 0.2466, 0.3730,\n",
      "         0.4503, 0.5026, 0.5143, 0.4780, 0.4624, 0.3400, 0.2270, 0.1590, 0.0933,\n",
      "         0.0871, 0.0929, 0.1002, 0.0913, 0.1182, 0.1346, 0.1863, 0.1726, 0.1324,\n",
      "         0.0892, 0.0785, 0.0791, 0.0952, 0.0975, 0.0934, 0.1030, 0.1114, 0.0994,\n",
      "         0.0887, 0.0862, 0.0865, 0.0838, 0.0737, 0.0849, 0.0736, 0.0811, 0.0834,\n",
      "         0.0932, 0.0893, 0.0802, 0.0787, 0.0791, 0.0946, 0.1072, 0.1015, 0.0791,\n",
      "         0.0751, 0.0793, 0.0795, 0.0822, 0.0977, 0.0914, 0.0857, 0.0874, 0.0756,\n",
      "         0.0614],\n",
      "        [0.1002, 0.1837, 0.2138, 0.1128, 0.0987, 0.0933, 0.0952, 0.1936, 0.2908,\n",
      "         0.4070, 0.4543, 0.4580, 0.4948, 0.4694, 0.3498, 0.2551, 0.2063, 0.1130,\n",
      "         0.0881, 0.0809, 0.0876, 0.0780, 0.1017, 0.1337, 0.1671, 0.1830, 0.1427,\n",
      "         0.1074, 0.0838, 0.0787, 0.0839, 0.0986, 0.0971, 0.1164, 0.1056, 0.1063,\n",
      "         0.0890, 0.0791, 0.0801, 0.0885, 0.0769, 0.0822, 0.0709, 0.0798, 0.0789,\n",
      "         0.0904, 0.1004, 0.0845, 0.0882, 0.0758, 0.0913, 0.0941, 0.1040, 0.0861,\n",
      "         0.0782, 0.0748, 0.0761, 0.0727, 0.0978, 0.0940, 0.0966, 0.0926, 0.0765,\n",
      "         0.0607],\n",
      "        [0.0667, 0.1895, 0.2176, 0.1707, 0.1318, 0.1359, 0.0890, 0.1056, 0.1989,\n",
      "         0.3248, 0.3945, 0.4246, 0.4703, 0.4939, 0.3897, 0.3163, 0.2616, 0.1739,\n",
      "         0.1020, 0.0747, 0.0825, 0.0759, 0.0901, 0.1179, 0.1620, 0.1808, 0.1672,\n",
      "         0.1444, 0.1161, 0.0904, 0.0875, 0.0955, 0.1032, 0.1408, 0.1110, 0.1191,\n",
      "         0.0936, 0.0787, 0.0805, 0.0821, 0.0740, 0.0809, 0.0697, 0.0764, 0.0765,\n",
      "         0.0853, 0.0898, 0.1068, 0.0938, 0.0799, 0.0835, 0.0888, 0.0945, 0.1001,\n",
      "         0.0973, 0.0784, 0.0703, 0.0721, 0.0846, 0.0923, 0.1015, 0.0991, 0.0856,\n",
      "         0.0633]]))\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2324]\n",
    "print(build_prompt_and_inputs(sample))\n",
    "print(process_sample(sample,processor=processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e7e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本 token 数: 482\n",
      "图像张数: 10\n",
      "  第 1 张图 → 180 个 patch token\n",
      "  第 2 张图 → 64 个 patch token\n",
      "  第 3 张图 → 180 个 patch token\n",
      "  第 4 张图 → 64 个 patch token\n",
      "  第 5 张图 → 180 个 patch token\n",
      "  第 6 张图 → 64 个 patch token\n",
      "  第 7 张图 → 180 个 patch token\n",
      "  第 8 张图 → 64 个 patch token\n",
      "  第 9 张图 → 180 个 patch token\n",
      "  第 10 张图 → 64 个 patch token\n",
      "图像总 patch token 数: 1220\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经有 process_sample 函数和 processor\n",
    "sample = train_dataset[0]     # 或者任何一个样本\n",
    "\n",
    "# 1) 只做一次前处理（在 CPU 上）\n",
    "inputs, _ = process_sample(sample, processor)\n",
    "\n",
    "# 2) 文本 token 数\n",
    "#    inputs[\"input_ids\"] 的形状是 (1, seq_len)\n",
    "text_token_count = inputs[\"input_ids\"].shape[1]\n",
    "print(f\"文本 token 数: {text_token_count}\")\n",
    "\n",
    "# 3) 图像 token 数\n",
    "#    inputs[\"image_grid_thw\"] 的形状是 (n_images, 3)\n",
    "#      每行 = [T, H, W]，对于静态图片 T=1，token = H*W\n",
    "grid = inputs[\"image_grid_thw\"].cpu().long()  # (n_images, 3)\n",
    "T, H, W = grid.unbind(dim=1)                 # 拆成三个向量\n",
    "tokens_per_image = (H * W).tolist()           # list 长度 = n_images\n",
    "total_image_tokens = sum(tokens_per_image)\n",
    "\n",
    "print(f\"图像张数: {grid.shape[0]}\")\n",
    "for i, nt in enumerate(tokens_per_image):\n",
    "    print(f\"  第 {i+1} 张图 → {nt} 个 patch token\")\n",
    "print(f\"图像总 patch token 数: {total_image_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, processor, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct_1 = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "        batch_labels = []\n",
    "\n",
    "        for sample in batch:\n",
    "            inputs, target = process_sample(sample, processor)  # 变量名改为target\n",
    "            batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "            batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "            batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "            batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "            batch_labels.append(target)  # 接收target_mmwave数据\n",
    "\n",
    "        # 修改维度处理\n",
    "        batch_inputs = {\n",
    "            k: torch.cat(v, dim=0).to(device)\n",
    "            for k, v in batch_inputs.items()\n",
    "        }\n",
    "        # 改为stack处理三维目标数据 (batch_size, seq_len, num_classes)\n",
    "        batch_labels = torch.stack(batch_labels).to(device)  # [B, T, C]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits = model(**batch_inputs)  # [B, T, C]\n",
    "            # 展平时间步维度计算损失\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), \n",
    "                            batch_labels.view(-1, batch_labels.size(-1)))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        total_correct_1 += calculate_accuracy(logits, batch_labels, k=1)\n",
    "    train_acc1 = total_correct_1 / len(train_loader)\n",
    "    return total_loss / len(train_loader) , train_acc1\n",
    "\n",
    "def evaluate(model, processor, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = defaultdict(int)  # 存储不同k值的正确数\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"evaluating\"):\n",
    "            batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [], \"image_grid_thw\": []}\n",
    "            batch_labels = []\n",
    "\n",
    "            for sample in batch:\n",
    "                inputs, target = process_sample(sample, processor)  # 变量名改为target\n",
    "                batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "                batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "                batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "                batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "                batch_labels.append(target)\n",
    "\n",
    "            batch_inputs = {k: torch.cat(v, dim=0).to(device) for k, v in batch_inputs.items()}\n",
    "            batch_labels = torch.stack(batch_labels).to(device)  # [B, T, C]\n",
    "\n",
    "            with autocast():\n",
    "                logits = model(**batch_inputs)  # [B, T, C]\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)),\n",
    "                                batch_labels.view(-1, batch_labels.size(-1)))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "            # 修改准确率计算逻辑\n",
    "            for k in [1, 3, 5]:\n",
    "                _, preds = logits.topk(k, dim=-1)  # [B, T, k]\n",
    "                # 将target转换为类别索引（假设target是one-hot编码）\n",
    "                targets = torch.argmax(batch_labels, dim=-1)  # [B, T]\n",
    "                correct = preds.eq(targets.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "                total_correct[k] += correct.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracies = {k: total_correct[k]/(total_samples * batch_labels.size(1)) for k in [1,3,5]}\n",
    "    \n",
    "    return avg_loss, accuracies[1], accuracies[3], accuracies[5]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e7f54",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaeb6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-5\n",
    "patience  = 5\n",
    "checkpoint_dir = \"/data2/dzr/finetune/finetunning_1_checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8d85a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897710/1980472877.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "model = finetuner.to(device)\n",
    "model.qwen.gradient_checkpointing_enable()\n",
    "scaler    = GradScaler()\n",
    "criterion = NMSELoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9cb3c",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217077e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test ,train_test_acc = train_epoch(model,processor,train_loader,criterion,optimizer,scaler,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f0ee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897710/67753029.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/4 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_loss, train_acc1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[1;32m     22\u001b[0m val_loss ,acc_1 ,acc_3 ,acc_5 \u001b[38;5;241m=\u001b[39m evaluate(model,processor,val_loader,criterion,device)\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, processor, train_loader, criterion, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, T, C]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# 展平时间步维度计算损失\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \n\u001b[1;32m     32\u001b[0m                     batch_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mQwen_and_Head.forward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, pixel_values, image_grid_thw):\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqwen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# grab the [CLS] token\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/peft/peft_model.py:1757\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1756\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1908\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1903\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1904\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1905\u001b[0m )\n\u001b[1;32m   1906\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1908\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1927\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1661\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts)\u001b[0m\n\u001b[1;32m   1659\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1661\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m     n_image_tokens \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1663\u001b[0m     n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1614\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.get_image_features\u001b[0;34m(self, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03mEncodes images into continuous embeddings that can be forwarded to the language model.\u001b[39;00m\n\u001b[1;32m   1606\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;124;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1614\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_embeds\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:526\u001b[0m, in \u001b[0;36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw)\u001b[0m\n\u001b[1;32m    524\u001b[0m     cu_seqlens_now \u001b[38;5;241m=\u001b[39m cu_window_seqlens\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 526\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m blk(hidden_states, cu_seqlens\u001b[38;5;241m=\u001b[39mcu_seqlens_now, position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:488\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         )\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    491\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    492\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:263\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    260\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 263\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:341\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 341\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:304\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionSdpaAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    302\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, seq_length, seq_length], device\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cu_seqlens)):\n\u001b[0;32m--> 304\u001b[0m     attention_mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, cu_seqlens[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] : cu_seqlens[i], cu_seqlens[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] : cu_seqlens[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    305\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    306\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def format_time(seconds):\n",
    "    mins, sec = divmod(seconds, 60)\n",
    "    hrs, mins = divmod(mins, 60)\n",
    "    return f\"{int(hrs)}h {int(mins)}m {int(sec)}s\"\n",
    "\n",
    "num_epochs = epochs\n",
    "best_val_loss = float('inf') # 初始化为“正无限大”（infinity）\n",
    "\n",
    "# 确保保存模型的目录存在\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 记录训练开始时间\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # 训练\n",
    "    train_loss, train_acc1 = train_epoch(model,processor,train_loader,criterion,optimizer,scaler,device)\n",
    "\n",
    "    # 验证\n",
    "    val_loss ,acc_1 ,acc_3 ,acc_5 = evaluate(model,processor,val_loader,criterion,device)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # 计算剩余时间\n",
    "    elapsed_time = epoch_end_time - training_start_time\n",
    "    avg_epoch_time = elapsed_time / (epoch + 1)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * remaining_epochs\n",
    "\n",
    "    # 转换为更易读的格式\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f},Train Accuracy@1:{train_acc1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f},Val Accuracy@1: {acc_1.item():.4f},Val Accuracy@3: {acc_3.item():.4f},Val Accuracy@5: {acc_5.item():.4f}\")\n",
    "    print(f\"Epoch Duration: {format_time(epoch_duration)}, Estimated Remaining Time: {format_time(remaining_time)}\")\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1} to {best_model_path}\")\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "    else:\n",
    "        early_stop_counter += 1  # 增加计数器\n",
    "\n",
    "    # 如果验证损失连续多个 epoch 没有改善，则停止训练\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break  # 提前停止训练\n",
    "\n",
    "    # 每隔若干个 epoch 保存模型\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'multimodal_encoder_decoder_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model at epoch {epoch+1} to {checkpoint_path}\")\n",
    "\n",
    "# 7. 测试评估\n",
    "\n",
    "# 加载最佳模型\n",
    "best_model_path = os.path.join(checkpoint_dir, 'multimodal_encoder_decoder_best.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Loaded best model for testing.\")\n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}. Skipping test evaluation.\")\n",
    "\n",
    "# 定义测试评估函数（可以与验证相同）\n",
    "\n",
    "\n",
    "test_loss ,test_acc1 ,test_acc3 ,test_acc5 = evaluate(model,processor,test_loader,criterion,device)\n",
    "print(f\"Test Loss : {test_loss:.4f};Test Accuracy@3 : {test_acc3:.4f}\")\n",
    "print(f\"Test Accuracy@1 : {test_acc1:.4f};Test Accuracy@5 : {test_acc5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
