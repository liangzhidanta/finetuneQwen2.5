{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0429336f-3500-4614-b166-c0173f2cf73a",
   "metadata": {},
   "source": [
    "# 微调Qwen2.5-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74de980a-545b-4045-ba6d-474e48ed4bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:07:45,926 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:07:46,218 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "2025-05-23 23:07:47,498 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:07:47,836 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = \"/data2/dzr/.cache\" \n",
    "from collections import OrderedDict, defaultdict\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm  # 引入 tqdm 库\n",
    "import time  # 引入 time 模块\n",
    "import argparse  # 引入 argparse 模块\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from typing import Dict, List\n",
    "from modelscope import AutoTokenizer, AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from modelscope import AutoModel\n",
    "\n",
    "model_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "processor = AutoProcessor.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e50d6",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d8b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "\n",
    "class QwenVisionDataset(Dataset):\n",
    "    def __init__(self, data_csv_paths, modal='mmwave_gps', input_length=8, output_length=3):\n",
    "        self.data_csv_paths = data_csv_paths\n",
    "        self.modal = modal\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "\n",
    "        # 特征列映射\n",
    "        self.features_column = {\n",
    "            'rgbs': 'unit1_rgb',\n",
    "            'u1_loc': 'unit1_loc',\n",
    "            'u2_loc': 'unit2_loc',\n",
    "            'mmwave': 'unit1_pwr_60ghz',\n",
    "            'heatmap': 'unit1_mmwave_heatmap'  # 新增热力图列\n",
    "        }\n",
    "        \n",
    "        # 初始化滑动窗口\n",
    "        self.window_samples = []\n",
    "        for seq_idx, data_csv_path in enumerate(self.data_csv_paths):\n",
    "            data_csv = pd.read_csv(data_csv_path)\n",
    "            for seq_id in data_csv['seq_index'].unique():\n",
    "                seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "                if len(seq_data) >= self.input_length:\n",
    "                    for start_idx in range(len(seq_data) - self.input_length + 1):\n",
    "                        self.window_samples.append((seq_idx, seq_id, start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, seq_id, start_idx = self.window_samples[idx]\n",
    "        base_path = os.path.dirname(self.data_csv_paths[seq_idx])\n",
    "        data_csv = pd.read_csv(self.data_csv_paths[seq_idx])\n",
    "        seq_data = data_csv[data_csv['seq_index'] == seq_id]\n",
    "\n",
    "        # 获取原始路径数据\n",
    "        window_data = {\n",
    "            'video_paths': seq_data[self.features_column['rgbs']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist(),\n",
    "            'heatmap_paths': seq_data[self.features_column['heatmap']]\n",
    "            .iloc[start_idx:start_idx+self.input_length] \n",
    "            .tolist()\n",
    "        }\n",
    "\n",
    "        # 处理GPS数据\n",
    "        gps = []\n",
    "        for i in range(self.input_length):\n",
    "            u1_loc = os.path.join(base_path, seq_data[self.features_column['u1_loc']].iloc[start_idx+i])\n",
    "            u2_loc = os.path.join(base_path, seq_data[self.features_column['u2_loc']].iloc[start_idx+i])\n",
    "            \n",
    "            with open(u1_loc, 'r') as f:\n",
    "                lat1, lon1 = map(float, f.read().strip().split())\n",
    "            with open(u2_loc, 'r') as f:\n",
    "                lat2, lon2 = map(float, f.read().strip().split())\n",
    "                \n",
    "            gps.append(torch.tensor([lat2-lat1, lon2-lon1], dtype=torch.float32))\n",
    "        gps = torch.stack(gps)\n",
    "\n",
    "        # 处理mmWave数据\n",
    "        mmwave = []\n",
    "        for i in range(self.input_length):\n",
    "            mmwave_path = os.path.join(base_path, \n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                mmwave.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())), \n",
    "                    dtype=torch.float32))\n",
    "        mmwave = torch.stack(mmwave)\n",
    "\n",
    "        # 目标数据（最后output_length个时间步）\n",
    "        target = []\n",
    "        for i in range(self.input_length-self.output_length, self.input_length):\n",
    "            mmwave_path = os.path.join(base_path,\n",
    "                seq_data[self.features_column['mmwave']].iloc[start_idx+i])\n",
    "            with open(mmwave_path, 'r') as f:\n",
    "                target.append(torch.tensor(\n",
    "                    list(map(float, f.read().strip().split())),\n",
    "                    dtype=torch.float32))\n",
    "        target = torch.stack(target)\n",
    "\n",
    "        return {\n",
    "            'video_paths': [os.path.join(base_path, p) for p in window_data['video_paths']],\n",
    "            'heatmap_paths': [os.path.join(base_path, p) for p in window_data['heatmap_paths']],\n",
    "            'gps': gps,\n",
    "            'mmwave': mmwave,\n",
    "            'target_mmwave': target\n",
    "        }\n",
    "\n",
    "def qwen_collate_fn(batch):\n",
    "    collated = {\n",
    "        'video_paths': [item['video_paths'] for item in batch],\n",
    "        'heatmap_paths': [item['heatmap_paths'] for item in batch],\n",
    "        'gps': pad_sequence([item['gps'] for item in batch], batch_first=True),\n",
    "        'mmwave': pad_sequence([item['mmwave'] for item in batch], batch_first=True),\n",
    "        'target_mmwave': pad_sequence([item['target_mmwave'] for item in batch], batch_first=True)\n",
    "    }\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05a91d2-2fd5-49b4-bd05-d1acc3a2cbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files for training.\n"
     ]
    }
   ],
   "source": [
    "dataset_start_idx = 1\n",
    "dataset_end_idx = 9\n",
    "# 定义数据集路径\n",
    "dataset_path = [f'/data2/wzj/Datasets/DeepSense/scenario{i}/' for i in range(dataset_start_idx, dataset_end_idx)]  # scenario1 ~ scenario8\n",
    "\n",
    "data_csv_paths = []\n",
    "for path in dataset_path:\n",
    "    data_csv_paths.extend(glob.glob(os.path.join(path, '*.csv')))\n",
    "\n",
    "print(f\"Found {len(data_csv_paths)} CSV files for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ecad2",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405ca765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = QwenVisionDataset(\n",
    "    data_csv_paths,\n",
    "    input_length=8,\n",
    "    output_length=3\n",
    ")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98d0a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5376_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5377_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5378_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5379_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5380_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5381_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5382_00_52_36.jpg',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/camera_data/image_BS1_5383_00_52_36.jpg'],\n",
       " 'heatmap_paths': ['/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1075.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1076.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1077.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1078.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1079.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1080.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1081.png',\n",
       "  '/data2/wzj/Datasets/DeepSense/scenario1/./unit1/mmWave_heatmap/mmWave_power_1082.png'],\n",
       " 'gps': tensor([[2.3318e-05, 2.2738e-04],\n",
       "         [2.7988e-05, 2.2698e-04],\n",
       "         [3.2698e-05, 2.2658e-04],\n",
       "         [3.7458e-05, 2.2618e-04],\n",
       "         [4.2258e-05, 2.2588e-04],\n",
       "         [4.7098e-05, 2.2558e-04],\n",
       "         [5.1988e-05, 2.2518e-04],\n",
       "         [5.6918e-05, 2.2488e-04]]),\n",
       " 'mmwave': tensor([[0.0169, 0.0203, 0.0211, 0.0198, 0.0211, 0.0201, 0.0187, 0.0196, 0.0210,\n",
       "          0.0223, 0.0211, 0.0221, 0.0206, 0.0206, 0.0195, 0.0222, 0.0212, 0.0206,\n",
       "          0.0195, 0.0212, 0.0226, 0.0238, 0.0233, 0.0206, 0.0186, 0.0193, 0.0241,\n",
       "          0.0338, 0.0400, 0.0487, 0.0505, 0.0473, 0.0453, 0.0347, 0.0272, 0.0239,\n",
       "          0.0217, 0.0186, 0.0190, 0.0197, 0.0215, 0.0233, 0.0239, 0.0260, 0.0256,\n",
       "          0.0212, 0.0199, 0.0196, 0.0191, 0.0191, 0.0199, 0.0193, 0.0192, 0.0186,\n",
       "          0.0192, 0.0192, 0.0183, 0.0183, 0.0184, 0.0190, 0.0184, 0.0182, 0.0171,\n",
       "          0.0159],\n",
       "         [0.0174, 0.0222, 0.0234, 0.0222, 0.0207, 0.0207, 0.0210, 0.0219, 0.0258,\n",
       "          0.0254, 0.0245, 0.0235, 0.0219, 0.0225, 0.0219, 0.0272, 0.0271, 0.0217,\n",
       "          0.0239, 0.0263, 0.0375, 0.0365, 0.0293, 0.0235, 0.0219, 0.0240, 0.0423,\n",
       "          0.0815, 0.1052, 0.1008, 0.1143, 0.1066, 0.0822, 0.0552, 0.0324, 0.0234,\n",
       "          0.0199, 0.0204, 0.0236, 0.0273, 0.0257, 0.0286, 0.0346, 0.0332, 0.0322,\n",
       "          0.0234, 0.0208, 0.0206, 0.0223, 0.0239, 0.0257, 0.0225, 0.0222, 0.0208,\n",
       "          0.0217, 0.0227, 0.0217, 0.0199, 0.0191, 0.0224, 0.0216, 0.0199, 0.0185,\n",
       "          0.0163],\n",
       "         [0.0186, 0.0225, 0.0244, 0.0212, 0.0214, 0.0213, 0.0245, 0.0251, 0.0272,\n",
       "          0.0253, 0.0239, 0.0214, 0.0184, 0.0213, 0.0226, 0.0284, 0.0267, 0.0227,\n",
       "          0.0232, 0.0318, 0.0365, 0.0320, 0.0259, 0.0198, 0.0204, 0.0359, 0.0623,\n",
       "          0.1012, 0.1065, 0.0929, 0.0903, 0.0835, 0.0531, 0.0334, 0.0219, 0.0197,\n",
       "          0.0209, 0.0220, 0.0237, 0.0263, 0.0290, 0.0303, 0.0308, 0.0288, 0.0268,\n",
       "          0.0207, 0.0185, 0.0188, 0.0205, 0.0240, 0.0241, 0.0213, 0.0209, 0.0204,\n",
       "          0.0192, 0.0207, 0.0202, 0.0202, 0.0206, 0.0221, 0.0209, 0.0190, 0.0187,\n",
       "          0.0172],\n",
       "         [0.0184, 0.0218, 0.0214, 0.0197, 0.0225, 0.0263, 0.0285, 0.0309, 0.0290,\n",
       "          0.0241, 0.0215, 0.0195, 0.0185, 0.0244, 0.0266, 0.0298, 0.0326, 0.0228,\n",
       "          0.0275, 0.0320, 0.0305, 0.0257, 0.0222, 0.0200, 0.0270, 0.0518, 0.0799,\n",
       "          0.1019, 0.0933, 0.0828, 0.0704, 0.0567, 0.0351, 0.0226, 0.0194, 0.0188,\n",
       "          0.0228, 0.0268, 0.0269, 0.0296, 0.0321, 0.0330, 0.0290, 0.0249, 0.0204,\n",
       "          0.0191, 0.0193, 0.0190, 0.0207, 0.0223, 0.0220, 0.0203, 0.0211, 0.0201,\n",
       "          0.0192, 0.0189, 0.0181, 0.0193, 0.0196, 0.0205, 0.0204, 0.0193, 0.0178,\n",
       "          0.0169],\n",
       "         [0.0199, 0.0212, 0.0240, 0.0281, 0.0319, 0.0373, 0.0405, 0.0387, 0.0312,\n",
       "          0.0220, 0.0185, 0.0214, 0.0259, 0.0320, 0.0423, 0.0427, 0.0340, 0.0379,\n",
       "          0.0461, 0.0604, 0.0454, 0.0259, 0.0245, 0.0357, 0.0627, 0.1006, 0.1222,\n",
       "          0.1290, 0.1155, 0.0817, 0.0676, 0.0495, 0.0306, 0.0206, 0.0206, 0.0200,\n",
       "          0.0239, 0.0297, 0.0308, 0.0351, 0.0396, 0.0350, 0.0280, 0.0236, 0.0206,\n",
       "          0.0197, 0.0223, 0.0219, 0.0210, 0.0241, 0.0223, 0.0218, 0.0224, 0.0220,\n",
       "          0.0200, 0.0186, 0.0187, 0.0198, 0.0198, 0.0208, 0.0207, 0.0198, 0.0183,\n",
       "          0.0171],\n",
       "         [0.0203, 0.0224, 0.0240, 0.0318, 0.0294, 0.0274, 0.0281, 0.0237, 0.0215,\n",
       "          0.0191, 0.0185, 0.0190, 0.0210, 0.0222, 0.0276, 0.0288, 0.0263, 0.0256,\n",
       "          0.0293, 0.0349, 0.0236, 0.0219, 0.0314, 0.0448, 0.0755, 0.0939, 0.1082,\n",
       "          0.1029, 0.0792, 0.0563, 0.0406, 0.0329, 0.0243, 0.0190, 0.0209, 0.0220,\n",
       "          0.0253, 0.0331, 0.0358, 0.0373, 0.0343, 0.0313, 0.0238, 0.0202, 0.0203,\n",
       "          0.0202, 0.0231, 0.0212, 0.0206, 0.0227, 0.0209, 0.0205, 0.0216, 0.0221,\n",
       "          0.0206, 0.0191, 0.0191, 0.0193, 0.0194, 0.0188, 0.0192, 0.0220, 0.0199,\n",
       "          0.0172],\n",
       "         [0.0214, 0.0263, 0.0292, 0.0378, 0.0287, 0.0271, 0.0238, 0.0210, 0.0196,\n",
       "          0.0192, 0.0205, 0.0200, 0.0232, 0.0256, 0.0295, 0.0349, 0.0354, 0.0302,\n",
       "          0.0321, 0.0257, 0.0225, 0.0270, 0.0390, 0.0473, 0.0642, 0.0705, 0.0747,\n",
       "          0.0773, 0.0669, 0.0450, 0.0292, 0.0242, 0.0235, 0.0217, 0.0219, 0.0223,\n",
       "          0.0302, 0.0410, 0.0395, 0.0360, 0.0320, 0.0267, 0.0228, 0.0229, 0.0246,\n",
       "          0.0234, 0.0234, 0.0247, 0.0253, 0.0240, 0.0201, 0.0203, 0.0215, 0.0244,\n",
       "          0.0257, 0.0229, 0.0207, 0.0203, 0.0202, 0.0198, 0.0202, 0.0272, 0.0261,\n",
       "          0.0179],\n",
       "         [0.0187, 0.0213, 0.0252, 0.0245, 0.0210, 0.0208, 0.0181, 0.0176, 0.0189,\n",
       "          0.0221, 0.0206, 0.0197, 0.0229, 0.0271, 0.0302, 0.0290, 0.0282, 0.0239,\n",
       "          0.0192, 0.0184, 0.0286, 0.0496, 0.0654, 0.0738, 0.0865, 0.0931, 0.0866,\n",
       "          0.0636, 0.0463, 0.0302, 0.0224, 0.0211, 0.0243, 0.0270, 0.0373, 0.0356,\n",
       "          0.0529, 0.0608, 0.0455, 0.0315, 0.0258, 0.0227, 0.0208, 0.0240, 0.0285,\n",
       "          0.0224, 0.0211, 0.0230, 0.0223, 0.0202, 0.0187, 0.0184, 0.0189, 0.0205,\n",
       "          0.0219, 0.0215, 0.0191, 0.0192, 0.0198, 0.0219, 0.0216, 0.0232, 0.0223,\n",
       "          0.0169]]),\n",
       " 'target_mmwave': tensor([[0.0203, 0.0224, 0.0240, 0.0318, 0.0294, 0.0274, 0.0281, 0.0237, 0.0215,\n",
       "          0.0191, 0.0185, 0.0190, 0.0210, 0.0222, 0.0276, 0.0288, 0.0263, 0.0256,\n",
       "          0.0293, 0.0349, 0.0236, 0.0219, 0.0314, 0.0448, 0.0755, 0.0939, 0.1082,\n",
       "          0.1029, 0.0792, 0.0563, 0.0406, 0.0329, 0.0243, 0.0190, 0.0209, 0.0220,\n",
       "          0.0253, 0.0331, 0.0358, 0.0373, 0.0343, 0.0313, 0.0238, 0.0202, 0.0203,\n",
       "          0.0202, 0.0231, 0.0212, 0.0206, 0.0227, 0.0209, 0.0205, 0.0216, 0.0221,\n",
       "          0.0206, 0.0191, 0.0191, 0.0193, 0.0194, 0.0188, 0.0192, 0.0220, 0.0199,\n",
       "          0.0172],\n",
       "         [0.0214, 0.0263, 0.0292, 0.0378, 0.0287, 0.0271, 0.0238, 0.0210, 0.0196,\n",
       "          0.0192, 0.0205, 0.0200, 0.0232, 0.0256, 0.0295, 0.0349, 0.0354, 0.0302,\n",
       "          0.0321, 0.0257, 0.0225, 0.0270, 0.0390, 0.0473, 0.0642, 0.0705, 0.0747,\n",
       "          0.0773, 0.0669, 0.0450, 0.0292, 0.0242, 0.0235, 0.0217, 0.0219, 0.0223,\n",
       "          0.0302, 0.0410, 0.0395, 0.0360, 0.0320, 0.0267, 0.0228, 0.0229, 0.0246,\n",
       "          0.0234, 0.0234, 0.0247, 0.0253, 0.0240, 0.0201, 0.0203, 0.0215, 0.0244,\n",
       "          0.0257, 0.0229, 0.0207, 0.0203, 0.0202, 0.0198, 0.0202, 0.0272, 0.0261,\n",
       "          0.0179],\n",
       "         [0.0187, 0.0213, 0.0252, 0.0245, 0.0210, 0.0208, 0.0181, 0.0176, 0.0189,\n",
       "          0.0221, 0.0206, 0.0197, 0.0229, 0.0271, 0.0302, 0.0290, 0.0282, 0.0239,\n",
       "          0.0192, 0.0184, 0.0286, 0.0496, 0.0654, 0.0738, 0.0865, 0.0931, 0.0866,\n",
       "          0.0636, 0.0463, 0.0302, 0.0224, 0.0211, 0.0243, 0.0270, 0.0373, 0.0356,\n",
       "          0.0529, 0.0608, 0.0455, 0.0315, 0.0258, 0.0227, 0.0208, 0.0240, 0.0285,\n",
       "          0.0224, 0.0211, 0.0230, 0.0223, 0.0202, 0.0187, 0.0184, 0.0189, 0.0205,\n",
       "          0.0219, 0.0215, 0.0191, 0.0192, 0.0198, 0.0219, 0.0216, 0.0232, 0.0223,\n",
       "          0.0169]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[998]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e5744-9218-4e1a-8759-f290b84b2856",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5b276c-237d-457f-956c-7c9ba8ca2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples: 11400\n",
      "Total Validation samples: 1425\n",
      "Total Testing samples: 1425\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Total Training samples: {len(train_dataset)}\")\n",
    "print(f\"Total Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Total Testing samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77164e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    # 直接返回样本列表，不进行合并\n",
    "    return batch\n",
    "\n",
    "# 创建 DataLoader 时指定 collate_fn\n",
    "train_loader = DataLoader(dataset, batch_size=1, collate_fn=custom_collate)\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "#batch_size = 64\n",
    "\n",
    "\n",
    "#train_loader = DataLoader(\n",
    "#   train_dataset,\n",
    " #   batch_size=batch_size,\n",
    " #   shuffle=True,  # 打乱训练集\n",
    " #   num_workers=4,\n",
    " #   pin_memory=True if torch.cuda.is_available() else False,\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2773a",
   "metadata": {},
   "source": [
    "## 定义度量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca8a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#---------新增topkloss---------\n",
    "class TopkLoss(nn.Module):\n",
    "    def __init__(self, k=1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output : [B, T, C] 模型输出的logits（未归一化）\n",
    "            target : [B, T, C] one-hot编码 或 [B, T] 类别索引\n",
    "            B = Batch Size        批量大小（数据加载时设置的batch_size）\n",
    "            T = Sequence Length   输出序列的时间步数（output_length=3）\n",
    "            C = Num Classes       类别数量（64个离散目标类别）\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        output_flat = output.view(B*T, C)  # [B*T, C]\n",
    "        target_flat = target.contiguous().view(-1)  # [B*T]\n",
    "        \n",
    "        # 计算Top-k正确性\n",
    "        _, topk_indices = torch.topk(output_flat, self.k, dim=1)  # [B*T, k]\n",
    "        correct = topk_indices.eq(target_flat.unsqueeze(1)).any(dim=1)  # [B*T]\n",
    "        \n",
    "        # 计算损失（仅惩罚Top-k错误的样本）\n",
    "        loss = F.cross_entropy(output_flat, target_flat, reduction='none')  # [B*T]，表示每个样本的预测是否在 Top-K 中命中真实标签\n",
    "        masked_loss = loss * ~correct  # 仅保留错误样本的损失值，正确样本的损失被置零\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return masked_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return masked_loss.sum()\n",
    "        return masked_loss\n",
    "\n",
    "#------------新增HybridLoss--------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, k=3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # 混合权重\n",
    "        self.k = k\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        output : [B, T, C]\n",
    "        target : [B, T]\n",
    "        \"\"\"\n",
    "        # 转换target为类别索引\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        \n",
    "        B, T, C = output.shape\n",
    "        \n",
    "        # 常规交叉熵损失（保持生成特性）\n",
    "        ce_loss = self.ce(output.view(-1, C), target.view(-1))  # [B*T]\n",
    "        \n",
    "        # Top-K增强损失\n",
    "        _, topk = output.topk(self.k, dim=-1)  # [B, T, k]\n",
    "        correct = topk.eq(target.unsqueeze(-1)).any(-1)  # [B, T]\n",
    "        topk_loss = (1 - correct.float()).mean()  # 错误率\n",
    "        \n",
    "        # 时间依赖惩罚项\n",
    "        seq_penalty = self._sequence_consistency(output, target)  # [1]\n",
    "        \n",
    "        return self.alpha*ce_loss.mean() + (1-self.alpha)*topk_loss + seq_penalty\n",
    "        \n",
    "    def _sequence_consistency(self, output, target):\n",
    "        \"\"\"\n",
    "        惩罚相邻时间步预测不一致的情况\n",
    "        \"\"\"\n",
    "        preds = output.argmax(-1)  # [B, T]\n",
    "        diff = (preds[:, 1:] != preds[:, :-1]).float().mean()\n",
    "        return diff * 0.2  # 可调节系数\n",
    "    \n",
    "#-------------新增CrossEntropyloss-----------------\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')  # 始终返回非归约结果\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # 处理one-hot编码目标\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "\n",
    "        # 重塑维度\n",
    "        output = output.view(-1, output.size(-1))  # [B*T, C]\n",
    "        target = target.view(-1)                   # [B*T]\n",
    "\n",
    "        # 计算基础损失\n",
    "        ce_loss = self.ce(output, target)\n",
    "        \n",
    "        # 自定义归约方式\n",
    "        if self.reduction == 'mean':\n",
    "            return ce_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return ce_loss.sum()\n",
    "        return ce_loss  # 'none'模式返回原始形状\n",
    "\n",
    "def calculate_accuracy(output, target, k=3):\n",
    "        if target.dim() == 3:\n",
    "            target = torch.argmax(target, dim=-1)  # [B, T]\n",
    "        with torch.no_grad():\n",
    "            _, pred = output.topk(k, dim=-1)  # [B, T, k]\n",
    "            correct = pred.eq(target.unsqueeze(-1)).any(dim=-1)\n",
    "            return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8110b6-ed26-453a-b994-c6f923e2bd2d",
   "metadata": {},
   "source": [
    "## 加载Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0964da6-7251-4124-9e4a-52f704e938b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (NVIDIA A100-SXM4-80GB)\n"
     ]
    }
   ],
   "source": [
    "# It's highly recommanded to use `[decord]` feature for faster video loading.\n",
    "#!pip install qwen-vl-utils[decord]==0.0.8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using device: {device} ({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "    print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd634f22-ebf9-45c5-979c-5d7d81e295f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:07:50,247 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data2/dzr/.cache/models/Qwen/Qwen2.5-VL-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:07:50,539 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0799d09c550499cac23c2e9b94d28dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  31774 MiB |  31774 MiB |  31774 MiB |      0 B   |\n",
      "|       from large pool |  31771 MiB |  31771 MiB |  31771 MiB |      0 B   |\n",
      "|       from small pool |      3 MiB |      3 MiB |      3 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  31774 MiB |  31774 MiB |  31774 MiB |      0 B   |\n",
      "|       from large pool |  31771 MiB |  31771 MiB |  31771 MiB |      0 B   |\n",
      "|       from small pool |      3 MiB |      3 MiB |      3 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  31632 MiB |  31632 MiB |  31632 MiB |      0 B   |\n",
      "|       from large pool |  31628 MiB |  31628 MiB |  31628 MiB |      0 B   |\n",
      "|       from small pool |      3 MiB |      3 MiB |      3 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  32122 MiB |  32122 MiB |  32122 MiB |      0 B   |\n",
      "|       from large pool |  32118 MiB |  32118 MiB |  32118 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 355444 KiB | 355473 KiB | 700525 KiB | 345081 KiB |\n",
      "|       from large pool | 354688 KiB | 354688 KiB | 696448 KiB | 341760 KiB |\n",
      "|       from small pool |    756 KiB |   2047 KiB |   4077 KiB |   3321 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     759    |     759    |     759    |       0    |\n",
      "|       from large pool |     361    |     361    |     361    |       0    |\n",
      "|       from small pool |     398    |     398    |     398    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     759    |     759    |     759    |       0    |\n",
      "|       from large pool |     361    |     361    |     361    |       0    |\n",
      "|       from small pool |     398    |     398    |     398    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     313    |     313    |     313    |       0    |\n",
      "|       from large pool |     311    |     311    |     311    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     166    |     166    |     169    |       3    |\n",
      "|       from large pool |     165    |     165    |     167    |       2    |\n",
      "|       from small pool |       1    |       2    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 正确加载模型的方式（使用AutoModelForCausalLM）\n",
    "finetune_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_ckpt,\n",
    "    trust_remote_code=True,  # 必须开启此选项\n",
    "    device_map=\"cuda\",\n",
    "    return_dict=True,\n",
    ").to(device)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bece5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2_5_VLForConditionalGeneration(\n",
      "  (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "    (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "        (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "        (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): Qwen2_5_VLMLP(\n",
      "          (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "          (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): Qwen2_5_VLPatchMerger(\n",
      "      (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2_5_VLModel(\n",
      "    (embed_tokens): Embedding(152064, 3584)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "        (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed8504b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 冻结原模型所有参数\n",
    "for param in finetune_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33353e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 配置 LoRA Adapter\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                         # LoRA rank\n",
    "    lora_alpha=32,               # LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# 5) 注入 LoRA\n",
    "qwen_lora = get_peft_model(finetune_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32e85f",
   "metadata": {},
   "source": [
    "## 用Qwen构造带有64类分类头的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc728b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen_and_Head(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.qwen = pretrained_model\n",
    "        joint_hidden_size = 3584\n",
    "\n",
    "        # final head now produces 3×64 dims\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(joint_hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 3 * 64),    # 3 timesteps × 64 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, image_grid_thw):\n",
    "        outputs = self.qwen(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_grid_thw=image_grid_thw,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # grab the [CLS] token\n",
    "        last_hidden = outputs.hidden_states[-1]   # (B, L, D)\n",
    "        cls_token   = last_hidden[:, 0, :]        # (B, D)\n",
    "\n",
    "        # project to (B, 3*64) and reshape\n",
    "        logits_flat = self.classifier(cls_token)            # (B, 192)\n",
    "        logits      = logits_flat.view(-1, 3, 64)           # (B, 3, 64)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e26c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  31791 MiB |  31791 MiB |  31791 MiB |      0 B   |\n",
      "|       from large pool |  31778 MiB |  31778 MiB |  31778 MiB |      0 B   |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  31791 MiB |  31791 MiB |  31791 MiB |      0 B   |\n",
      "|       from large pool |  31778 MiB |  31778 MiB |  31778 MiB |      0 B   |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  31649 MiB |  31649 MiB |  31649 MiB |      0 B   |\n",
      "|       from large pool |  31635 MiB |  31635 MiB |  31635 MiB |      0 B   |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  32132 MiB |  32132 MiB |  32132 MiB |      0 B   |\n",
      "|       from large pool |  32118 MiB |  32118 MiB |  32118 MiB |      0 B   |\n",
      "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 348273 KiB | 356724 KiB | 710301 KiB | 362028 KiB |\n",
      "|       from large pool | 347520 KiB | 354688 KiB | 696448 KiB | 348928 KiB |\n",
      "|       from small pool |    753 KiB |   2047 KiB |  13853 KiB |  13100 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     875    |     875    |     875    |       0    |\n",
      "|       from large pool |     362    |     362    |     362    |       0    |\n",
      "|       from small pool |     513    |     513    |     513    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     875    |     875    |     875    |       0    |\n",
      "|       from large pool |     362    |     362    |     362    |       0    |\n",
      "|       from small pool |     513    |     513    |     513    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     318    |     318    |     318    |       0    |\n",
      "|       from large pool |     311    |     311    |     311    |       0    |\n",
      "|       from small pool |       7    |       7    |       7    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     167    |     168    |     174    |       7    |\n",
      "|       from large pool |     165    |     165    |     167    |       2    |\n",
      "|       from small pool |       2    |       3    |       7    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Qwen_and_Head(\n",
      "  (qwen): PeftModelForCausalLM(\n",
      "    (base_model): LoraModel(\n",
      "      (model): Qwen2_5_VLForConditionalGeneration(\n",
      "        (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
      "          (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
      "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "          )\n",
      "          (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
      "          (blocks): ModuleList(\n",
      "            (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
      "              (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "              (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "              (attn): Qwen2_5_VLVisionSdpaAttention(\n",
      "                (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "                (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              )\n",
      "              (mlp): Qwen2_5_VLMLP(\n",
      "                (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
      "                (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (merger): Qwen2_5_VLPatchMerger(\n",
      "            (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (model): Qwen2_5_VLModel(\n",
      "          (embed_tokens): Embedding(152064, 3584)\n",
      "          (layers): ModuleList(\n",
      "            (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
      "              (self_attn): Qwen2_5_VLSdpaAttention(\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "                (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "              )\n",
      "              (mlp): Qwen2MLP(\n",
      "                (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
      "        )\n",
      "        (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=192, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "finetuner = Qwen_and_Head(pretrained_model=qwen_lora).to(device)\n",
    "print(torch.cuda.memory_summary())\n",
    "print(finetuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887fcfa",
   "metadata": {},
   "source": [
    "## 构建多模态提示词并提取视觉输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2684fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'You are a wireless communication expert. Please analyze spatiotemporal multimodal data comprehensively and predict future mmWave beam indices.This is the observation data for t-4:\\n- Monitoring photos:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_976_02_12_21.jpg]\\n- mmWave heatmap:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_123.png]\\n- The relative position between the car and the base station:longitude:0.000087, dimension:0.000158\\nThis is the observation data for t-3:\\n- Monitoring photos:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_977_02_12_21.jpg]\\n- mmWave heatmap:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_124.png]\\n- The relative position between the car and the base station:longitude:0.000084, dimension:0.000159\\nThis is the observation data for t-2:\\n- Monitoring photos:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_978_02_12_21.jpg]\\n- mmWave heatmap:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_125.png]\\n- The relative position between the car and the base station:longitude:0.000081, dimension:0.000159\\nThis is the observation data for t-1:\\n- Monitoring photos:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_979_02_12_21.jpg]\\n- mmWave heatmap:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_126.png]\\n- The relative position between the car and the base station:longitude:0.000078, dimension:0.000159\\nThis is the observation data for Current time (t):\\n- Monitoring photos:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_980_02_12_22.jpg]\\n- mmWave heatmap:[IMG:/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_127.png]\\n- The relative position between the car and the base station:longitude:0.000075, dimension:0.000159\\nPlease predict the beam indices for the next three time steps (t+1, t+2, t+3) based on the above time-series observation data, and provide numerical results directly.', 'image_paths': ['/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_976_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_123.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_977_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_124.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_978_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_125.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_979_02_12_21.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_126.png', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/camera_data/image_BS1_980_02_12_22.jpg', '/data2/wzj/Datasets/DeepSense/scenario2/unit1/mmWave_heatmap/mmWave_power_127.png'], 'labels': [11, 12, 13]}\n",
      "({'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[-1.5003, -1.3251, -1.4711,  ..., -0.7834, -0.8261, -0.7692],\n",
      "        [-1.1937, -1.3397, -1.3689,  ..., -0.9683, -0.8972, -0.9399],\n",
      "        [-0.7120, -0.8142, -0.7266,  ..., -1.0394, -0.9256, -0.8545],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}, [11, 12, 13])\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  31791 MiB |  31855 MiB |  31855 MiB |  65620 KiB |\n",
      "|       from large pool |  31778 MiB |  31842 MiB |  31842 MiB |  65536 KiB |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |     84 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  31791 MiB |  31855 MiB |  31855 MiB |  65620 KiB |\n",
      "|       from large pool |  31778 MiB |  31842 MiB |  31842 MiB |  65536 KiB |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |     84 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  31649 MiB |  31712 MiB |  31712 MiB |  65300 KiB |\n",
      "|       from large pool |  31635 MiB |  31699 MiB |  31699 MiB |  65231 KiB |\n",
      "|       from small pool |     13 MiB |     13 MiB |     13 MiB |     69 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  32196 MiB |  32196 MiB |  32196 MiB |      0 B   |\n",
      "|       from large pool |  32182 MiB |  32182 MiB |  32182 MiB |      0 B   |\n",
      "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 348273 KiB | 356724 KiB | 710385 KiB | 362112 KiB |\n",
      "|       from large pool | 347520 KiB | 354688 KiB | 696448 KiB | 348928 KiB |\n",
      "|       from small pool |    753 KiB |   2047 KiB |  13937 KiB |  13184 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     875    |     886    |     911    |      36    |\n",
      "|       from large pool |     362    |     363    |     363    |       1    |\n",
      "|       from small pool |     513    |     523    |     548    |      35    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     875    |     886    |     911    |      36    |\n",
      "|       from large pool |     362    |     363    |     363    |       1    |\n",
      "|       from small pool |     513    |     523    |     548    |      35    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     319    |     319    |     319    |       0    |\n",
      "|       from large pool |     312    |     312    |     312    |       0    |\n",
      "|       from small pool |       7    |       7    |       7    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     167    |     169    |     188    |      21    |\n",
      "|       from large pool |     165    |     165    |     167    |       2    |\n",
      "|       from small pool |       2    |       4    |      21    |      19    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_prompt_and_inputs(sample: Dict, hist_steps: int = 5) -> Dict:\n",
    "    \"\"\"构建多模态提示词并提取视觉输入\n",
    "    Args:\n",
    "        sample: 包含多模态数据的样本\n",
    "        hist_steps: 使用历史时间步数（默认为5）\n",
    "    Returns:\n",
    "        包含处理后的提示词和视觉输入的字典\n",
    "    \"\"\"\n",
    "    # 提取并规范化路径\n",
    "    def normalize_paths(path_list: List[str]) -> List[str]:\n",
    "        return [os.path.normpath(p) for p in path_list]\n",
    "    # 处理所有路径\n",
    "    video_paths = normalize_paths(sample['video_paths'][:hist_steps])\n",
    "    heatmap_paths = normalize_paths(sample['heatmap_paths'][:hist_steps])\n",
    "    gps_data = sample['gps'][:hist_steps].tolist()\n",
    "    \n",
    "    # 构建时间序列提示词\n",
    "    prompt_parts = []\n",
    "    for step in range(hist_steps):\n",
    "        time_label = f\"t-{hist_steps-1-step}\" if step < hist_steps-1 else \"Current time (t)\"\n",
    "        \n",
    "        # GPS数据格式化（假设张量存储的是经度、纬度）\n",
    "        lon, lat = gps_data[step]\n",
    "        gps_str = f\"longitude:{lon:.6f}, dimension:{lat:.6f}\"\n",
    "        \n",
    "        # 添加多模态信息块\n",
    "        prompt_part = (\n",
    "            f\"This is the observation data for {time_label}:\\n\"\n",
    "            f\"- Monitoring photos:[IMG:{video_paths[step]}]\\n\"\n",
    "            f\"- mmWave heatmap:[IMG:{heatmap_paths[step]}]\\n\"\n",
    "            f\"- The relative position between the car and the base station:{gps_str}\\n\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    # 组合完整提示词\n",
    "    system_prompt = \"You are a wireless communication expert. Please analyze spatiotemporal multimodal data comprehensively and predict future mmWave beam indices.\"\n",
    "    full_prompt = (\n",
    "        f\"{system_prompt}\" \n",
    "        + \"\".join(prompt_parts) +\n",
    "        \"Please predict the beam indices for the next three time steps (t+1, t+2, t+3) based on the above time-series observation data, and provide numerical results directly.\"\n",
    "    )\n",
    "    \n",
    "    # 提取所有视觉路径（RGB + 热力图）\n",
    "    all_image_paths = [p for pair in zip(video_paths, heatmap_paths) for p in pair]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"image_paths\": all_image_paths,\n",
    "        \"labels\": sample['target_mmwave'].argmax(dim=-1).tolist()  # 假设索引是最大值位置\n",
    "    }\n",
    "\n",
    "# 示例使用 ---------------------------------------------------\n",
    "def process_sample(sample, processor):  # 添加processor参数\n",
    "    # Step 1: 构建提示词和获取图像路径\n",
    "    processed = build_prompt_and_inputs(sample)\n",
    "    \n",
    "    # Step 2: 构建messages结构\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\", \"image\": path} for path in processed[\"image_paths\"]] + \n",
    "                  [{\"type\": \"text\", \"text\": processed[\"prompt\"]}]\n",
    "    }]\n",
    "    \n",
    "    # Step 3: 使用传入的processor处理输入\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return inputs, processed[\"labels\"]\n",
    "\n",
    "sample = dataset[2324]\n",
    "print(build_prompt_and_inputs(sample))\n",
    "print(process_sample(sample,processor=processor))\n",
    "print(torch.cuda.memory_summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "680d29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_epoch(model, processor, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # 1) run process_sample on every raw sample\n",
    "        batch_inputs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": [],\"image_grid_thw\": []}\n",
    "        batch_labels = []\n",
    "\n",
    "        for sample in batch:\n",
    "            inputs, label = process_sample(sample, processor)\n",
    "            batch_inputs[\"input_ids\"].append(inputs[\"input_ids\"])\n",
    "            batch_inputs[\"attention_mask\"].append(inputs[\"attention_mask\"])\n",
    "            batch_inputs[\"pixel_values\"].append(inputs[\"pixel_values\"])\n",
    "            batch_inputs[\"image_grid_thw\"].append(inputs[\"image_grid_thw\"])\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        # 2) stack/cat into real batched tensors\n",
    "        batch_inputs = {\n",
    "            k: torch.cat(v, dim=0).to(device)\n",
    "            for k, v in batch_inputs.items()\n",
    "        }\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long, device=device)\n",
    "        print(batch_inputs)\n",
    "        # 3) forward + loss + backward\n",
    "        optimizer.zero_grad()\n",
    "        # 1) forward in mixed precision\n",
    "        with autocast():\n",
    "            logits = model(**batch_inputs)\n",
    "            loss   = criterion(logits, batch_labels)\n",
    "\n",
    "        # 2) scale, backward, unscale, step, update\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model,  processor,evaluate_loader, criterion , optimizer, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    criterion = criterion\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # 多模态数据处理\n",
    "        inputs_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        # 处理每个样本\n",
    "        for sample in batch:\n",
    "            # 使用之前定义的process_sample函数\n",
    "            inputs, labels = process_sample(sample)\n",
    "            inputs_list.append(inputs)\n",
    "            labels_list.extend(labels)\n",
    "        \n",
    "        # 自定义批处理函数\n",
    "        batch_inputs = (inputs_list)\n",
    "        batch_labels = torch.tensor(labels_list, dtype=torch.long, device=device)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 模型前向传播\n",
    "        outputs = model(\n",
    "            input_ids=batch_inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=batch_inputs[\"attention_mask\"].to(device),\n",
    "            pixel_values=batch_inputs[\"pixel_values\"].to(device),\n",
    "            image_grid_thw=batch_inputs[\"image_grid_thw\"].to(device)\n",
    "        )\n",
    "        \n",
    "        # 输出层适配\n",
    "        # 输出logits的形状为 (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "        #计算topk准确率\n",
    "        topk_acc = calculate_accuracy(outputs, batch_labels, k=3)\n",
    "\n",
    "        avg_loss = total_loss / len(evaluate_loader)\n",
    "    return avg_loss ,topk_acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f9b1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/3830852601.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Training:   0%|          | 0/14250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2661,  0.1785,  0.1493,  ..., -0.8545, -0.9683, -0.8972],\n",
      "        [ 0.0325, -0.3178, -0.1280,  ..., -0.5701, -0.3142, -0.3142],\n",
      "        [-0.9602, -0.9748, -0.7120,  ...,  0.3968,  0.5390,  0.4253],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Training:   0%|          | 1/14250 [00:08<32:11:53,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2953,  0.2661,  0.2661,  ..., -0.8403, -1.0110, -0.8688],\n",
      "        [ 0.0033, -0.2010, -0.0259,  ..., -0.5559, -0.2573, -0.3711],\n",
      "        [-0.9310, -0.9456, -0.7558,  ...,  0.4253,  0.5675,  0.3115],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 2/14250 [00:14<28:59:15,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2661,  0.1931,  0.1639,  ..., -0.8545, -0.8972, -0.8830],\n",
      "        [-0.0405, -0.2594, -0.1718,  ..., -0.5275, -0.2857, -0.2431],\n",
      "        [-0.9164, -0.9602, -0.8142,  ...,  0.3257,  0.4110,  0.3257],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 3/14250 [00:22<28:53:24,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2953,  0.2369,  0.1639,  ..., -0.8261, -0.9967, -0.9683],\n",
      "        [ 0.0179, -0.3616, -0.2010,  ..., -0.6412, -0.3711, -0.3568],\n",
      "        [-0.9602, -0.9748, -0.7120,  ...,  0.3684,  0.4253,  0.3399],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 4/14250 [00:28<28:02:27,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2807,  0.2223,  0.2369,  ..., -0.7977, -0.9399, -0.9114],\n",
      "        [ 0.0763, -0.3178, -0.2740,  ..., -0.5417, -0.2857, -0.3000],\n",
      "        [-0.9310, -0.9602, -0.7412,  ...,  0.3968,  0.4964,  0.3115],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 5/14250 [00:35<27:35:41,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2953,  0.2369,  0.2077,  ..., -0.8830, -1.0110, -0.8261],\n",
      "        [ 0.0471, -0.2886, -0.1426,  ..., -0.5701, -0.2573, -0.2857],\n",
      "        [-0.8726, -1.0039, -0.7558,  ...,  0.3826,  0.6386,  0.3684],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 6/14250 [00:42<27:21:55,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.3099,  0.2223,  0.1785,  ..., -0.8545, -1.0110, -0.9114],\n",
      "        [-0.0113, -0.2594, -0.1134,  ..., -0.5701, -0.2431, -0.3568],\n",
      "        [-0.9164, -0.9310, -0.7558,  ...,  0.4395,  0.6528,  0.4110],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 7/14250 [00:49<27:15:11,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.3099,  0.2223,  0.1785,  ..., -0.9541, -0.9541, -0.9114],\n",
      "        [ 0.0179, -0.2740, -0.0550,  ..., -0.5417, -0.3711, -0.3000],\n",
      "        [-0.8872, -1.0039, -0.7704,  ...,  0.3542,  0.5390,  0.3826],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 8/14250 [00:56<27:14:22,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.3391,  0.2515,  0.2515,  ..., -0.8403, -0.9683, -0.9256],\n",
      "        [ 0.0763, -0.2886, -0.0696,  ..., -0.5701, -0.3000, -0.3000],\n",
      "        [-0.9018, -1.0039, -0.6828,  ...,  0.3968,  0.4964,  0.2973],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 9/14250 [01:03<27:20:24,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[ 0.2515,  0.1785,  0.1347,  ..., -0.7977, -0.9256, -0.8545],\n",
      "        [-0.0113, -0.4054, -0.1134,  ..., -0.5844, -0.3142, -0.3142],\n",
      "        [-0.9310, -0.9456, -0.7412,  ...,  0.2973,  0.4110,  0.2688],\n",
      "        ...,\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857],\n",
      "        [-0.7996, -0.7996, -0.7996,  ..., -0.2857, -0.2857, -0.2857]],\n",
      "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16],\n",
      "        [ 1, 38, 68],\n",
      "        [ 1, 16, 16]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4079545/355242496.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/dzr/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 9/14250 [01:04<28:15:41,  7.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 5) pass scaler into your training loop\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#    (you’ll need to adjust train_epoch to accept it)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# NEW argument\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_test)\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, processor, train_loader, criterion, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 1) forward in mixed precision\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 31\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(logits, batch_labels)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 2) scale, backward, unscale, step, update\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mQwen_and_Head.forward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, pixel_values, image_grid_thw):\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqwen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# grab the [CLS] token\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n"
     ]
    }],
      "source": [
        "code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (mllm)",
      "language": "python",
      "name": "mllm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}